[
  {
    "objectID": "man/MockJurors.html",
    "href": "man/MockJurors.html",
    "title": "betareg",
    "section": "",
    "text": "Data with responses of naive mock jurors to the conventional conventional two-option verdict (guilt vs. acquittal) versus a three-option verdict setup (the third option was the Scottish ‘not proven’ alternative), in the presence/absence of conflicting testimonial evidence.\n\ndata(\"MockJurors\", package = \"betareg\")\n\nA data frame containing 104 observations on 3 variables.\n\n\nverdict\n\n\nfactor indicating whether a two-option or three-option verdict is requested. (A sum contrast rather than treatment contrast is employed.)\n\n\nconflict\n\n\nfactor. Is there conflicting testimonial evidence? (A sum contrast rather than treatment contrast is employed.)\n\n\nconfidence\n\n\njurors degree of confidence in his/her verdict, scaled to the open unit interval (see below).\n\n\nThe data were collected by Daily (2004) among first-year psychology students at Australian National University. Smithson and Verkuilen (2006) employed the data scaling the original confidence (on a scale 0–100) to the open unit interval: ((original_confidence/100) * 103 - 0.5) / 104.\nThe original coding of conflict in the data provided from Smithson’s homepage is -1/1 which Smithson and Verkuilen (2006) describe to mean no/yes. However, all their results (sample statistics, histograms, etc.) suggest that it actually means yes/no which was employed in MockJurors.\n\nExample 1 from Smithson and Verkuilen (2006) supplements.\n\nDeady S (2004). The Psychological Third Verdict: ‘Not Proven’ or ‘Not Willing to Make a Decision’? Unpublished honors thesis, The Australian National University, Canberra.\nSmithson M, Verkuilen J (2006). A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables. Psychological Methods, 11(7), 54–71.\n\nbetareg, ReadingSkills, StressAnxiety\n\n\nlibrary(\"betareg\")\n\ndata(\"MockJurors\", package = \"betareg\")\nlibrary(\"lmtest\")\n\n## Smithson & Verkuilen (2006, Table 1)\n## variable dispersion model\n## (NOTE: numerical rather than analytical Hessian is used for replication,\n##  Smithson & Verkuilen erroneously compute one-sided p-values)\nmj_vd &lt;- betareg(confidence ~ verdict * conflict | verdict * conflict,\n  data = MockJurors, hessian = TRUE)\nsummary(mj_vd)\n\n\nCall:\nbetareg(formula = confidence ~ verdict * conflict | verdict * conflict, \n    data = MockJurors, hessian = TRUE)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.4764 -0.6653 -0.0989  0.6000  2.6436 \n\nCoefficients (mean model with logit link):\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      0.912404   0.103979   8.775  &lt; 2e-16 ***\nverdict          0.005035   0.103979   0.048  0.96138    \nconflict         0.168573   0.103979   1.621  0.10497    \nverdict:conflict 0.280010   0.103979   2.693  0.00708 ** \n\nPhi coefficients (precision model with log link):\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.1733     0.1278   9.180  &lt; 2e-16 ***\nverdict           -0.3299     0.1278  -2.581  0.00985 ** \nconflict           0.2196     0.1278   1.718  0.08576 .  \nverdict:conflict   0.3163     0.1278   2.475  0.01334 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 40.12 on 8 Df\nPseudo R-squared: 0.03885\nNumber of iterations in BFGS optimization: 19 \n\n## model selection for beta regression: null model, fixed dispersion model (p. 61)\nmj_null &lt;- betareg(confidence ~ 1 | 1, data = MockJurors)\nmj_fd &lt;-   betareg(confidence ~ verdict * conflict | 1, data = MockJurors)\nlrtest(mj_null, mj_fd)\n\nLikelihood ratio test\n\nModel 1: confidence ~ 1 | 1\nModel 2: confidence ~ verdict * conflict | 1\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)\n1   2 28.226                     \n2   5 30.580  3 4.7086     0.1944\n\nlrtest(mj_null, mj_vd)\n\nLikelihood ratio test\n\nModel 1: confidence ~ 1 | 1\nModel 2: confidence ~ verdict * conflict | verdict * conflict\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 28.226                         \n2   8 40.117  6 23.782  0.0005728 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## McFadden's pseudo-R-squared\n1 - as.vector(logLik(mj_null)/logLik(mj_vd))\n\n[1] 0.296407\n\n## visualization\nif(require(\"lattice\")) {\n  histogram(~ confidence | conflict + verdict, data = MockJurors,\n    col = \"lightgray\", breaks = 0:10/10, type = \"density\")\n}\n\n\n\n\n\n\n## see demo(\"SmithsonVerkuilen2006\", package = \"betareg\") for more details",
    "crumbs": [
      "Data sets",
      "MockJurors"
    ]
  },
  {
    "objectID": "man/MockJurors.html#confidence-of-mock-jurors-in-their-verdicts",
    "href": "man/MockJurors.html#confidence-of-mock-jurors-in-their-verdicts",
    "title": "betareg",
    "section": "",
    "text": "Data with responses of naive mock jurors to the conventional conventional two-option verdict (guilt vs. acquittal) versus a three-option verdict setup (the third option was the Scottish ‘not proven’ alternative), in the presence/absence of conflicting testimonial evidence.\n\ndata(\"MockJurors\", package = \"betareg\")\n\nA data frame containing 104 observations on 3 variables.\n\n\nverdict\n\n\nfactor indicating whether a two-option or three-option verdict is requested. (A sum contrast rather than treatment contrast is employed.)\n\n\nconflict\n\n\nfactor. Is there conflicting testimonial evidence? (A sum contrast rather than treatment contrast is employed.)\n\n\nconfidence\n\n\njurors degree of confidence in his/her verdict, scaled to the open unit interval (see below).\n\n\nThe data were collected by Daily (2004) among first-year psychology students at Australian National University. Smithson and Verkuilen (2006) employed the data scaling the original confidence (on a scale 0–100) to the open unit interval: ((original_confidence/100) * 103 - 0.5) / 104.\nThe original coding of conflict in the data provided from Smithson’s homepage is -1/1 which Smithson and Verkuilen (2006) describe to mean no/yes. However, all their results (sample statistics, histograms, etc.) suggest that it actually means yes/no which was employed in MockJurors.\n\nExample 1 from Smithson and Verkuilen (2006) supplements.\n\nDeady S (2004). The Psychological Third Verdict: ‘Not Proven’ or ‘Not Willing to Make a Decision’? Unpublished honors thesis, The Australian National University, Canberra.\nSmithson M, Verkuilen J (2006). A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables. Psychological Methods, 11(7), 54–71.\n\nbetareg, ReadingSkills, StressAnxiety\n\n\nlibrary(\"betareg\")\n\ndata(\"MockJurors\", package = \"betareg\")\nlibrary(\"lmtest\")\n\n## Smithson & Verkuilen (2006, Table 1)\n## variable dispersion model\n## (NOTE: numerical rather than analytical Hessian is used for replication,\n##  Smithson & Verkuilen erroneously compute one-sided p-values)\nmj_vd &lt;- betareg(confidence ~ verdict * conflict | verdict * conflict,\n  data = MockJurors, hessian = TRUE)\nsummary(mj_vd)\n\n\nCall:\nbetareg(formula = confidence ~ verdict * conflict | verdict * conflict, \n    data = MockJurors, hessian = TRUE)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.4764 -0.6653 -0.0989  0.6000  2.6436 \n\nCoefficients (mean model with logit link):\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      0.912404   0.103979   8.775  &lt; 2e-16 ***\nverdict          0.005035   0.103979   0.048  0.96138    \nconflict         0.168573   0.103979   1.621  0.10497    \nverdict:conflict 0.280010   0.103979   2.693  0.00708 ** \n\nPhi coefficients (precision model with log link):\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.1733     0.1278   9.180  &lt; 2e-16 ***\nverdict           -0.3299     0.1278  -2.581  0.00985 ** \nconflict           0.2196     0.1278   1.718  0.08576 .  \nverdict:conflict   0.3163     0.1278   2.475  0.01334 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 40.12 on 8 Df\nPseudo R-squared: 0.03885\nNumber of iterations in BFGS optimization: 19 \n\n## model selection for beta regression: null model, fixed dispersion model (p. 61)\nmj_null &lt;- betareg(confidence ~ 1 | 1, data = MockJurors)\nmj_fd &lt;-   betareg(confidence ~ verdict * conflict | 1, data = MockJurors)\nlrtest(mj_null, mj_fd)\n\nLikelihood ratio test\n\nModel 1: confidence ~ 1 | 1\nModel 2: confidence ~ verdict * conflict | 1\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)\n1   2 28.226                     \n2   5 30.580  3 4.7086     0.1944\n\nlrtest(mj_null, mj_vd)\n\nLikelihood ratio test\n\nModel 1: confidence ~ 1 | 1\nModel 2: confidence ~ verdict * conflict | verdict * conflict\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 28.226                         \n2   8 40.117  6 23.782  0.0005728 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## McFadden's pseudo-R-squared\n1 - as.vector(logLik(mj_null)/logLik(mj_vd))\n\n[1] 0.296407\n\n## visualization\nif(require(\"lattice\")) {\n  histogram(~ confidence | conflict + verdict, data = MockJurors,\n    col = \"lightgray\", breaks = 0:10/10, type = \"density\")\n}\n\n\n\n\n\n\n## see demo(\"SmithsonVerkuilen2006\", package = \"betareg\") for more details",
    "crumbs": [
      "Data sets",
      "MockJurors"
    ]
  },
  {
    "objectID": "man/XBeta.html",
    "href": "man/XBeta.html",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for extended-support beta distributions using the workflow from the distributions3 package.\n\nXBeta(mu, phi, nu = 0)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Exceedence parameter for the support of the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\nIn order to obtain an extended-support beta distribution on [0, 1] an additional exceedence parameter nu is introduced. If nu &gt; 0, this scales the underlying beta distribution to the interval [-nu, 1 + nu] where the tails are subsequently censored to the unit interval [0, 1] with point masses on the boundaries 0 and 1. Thus, nu controls how likely boundary observations are and for nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\nA XBeta distribution object.\n\ndxbeta, BetaR\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- XBeta(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  nu = c(0, 0.1, 0.2)\n)\n\nX\n\n[1] \"XBeta(mu = 0.25, phi = 1, nu = 0.0)\" \"XBeta(mu = 0.50, phi = 1, nu = 0.1)\"\n[3] \"XBeta(mu = 0.75, phi = 2, nu = 0.2)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.2500000 0.5000000 0.7886591\n\nvariance(X)\n\n[1] 0.09375000 0.15331441 0.08617379\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## it is only continuous when there are no point masses on the boundary\nis_continuous(X)\n\n[1]  TRUE FALSE FALSE\n\ncdf(X, 0)\n\n[1] 0.0000000 0.1864295 0.0239812\n\ncdf(X, 1, lower.tail = FALSE)\n\n[1] 0.0000000 0.1864295 0.4695222\n\n## simulate random variables\nrandom(X, 5)\n\n           r_1       r_2        r_3        r_4       r_5\n[1,] 0.7497152 0.8385523 0.03196796 0.91882879 0.5454367\n[2,] 0.1263742 1.0000000 0.00000000 0.07151503 0.0000000\n[3,] 1.0000000 1.0000000 0.86031184 1.00000000 0.9764434\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.5305165 0.6607051\n\npdf(X, x, log = TRUE)\n\n[1] -0.3796622 -0.6339043 -0.4144477\n\nlog_pdf(X, x)\n\n[1] -0.3796622 -0.6339043 -0.4144477\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.3189318\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 0.97152842\n\n## cdf() and quantile() are inverses (except at censoring points)\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n           q_0.05      q_0.5    q_0.95\n[1,] 9.512588e-06 0.09331223 0.9118445\n[2,] 0.000000e+00 0.50000000 1.0000000\n[3,] 1.199277e-01 0.97152842 1.0000000\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 1.000000e+00\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 1.000000e+00\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]   0.2500000 0.2464581\n[2,]   0.5000000 0.4949177\n[3,]   0.7886591 0.7955785",
    "crumbs": [
      "distributions3 objects",
      "XBeta"
    ]
  },
  {
    "objectID": "man/XBeta.html#create-an-extended-support-beta-distribution",
    "href": "man/XBeta.html#create-an-extended-support-beta-distribution",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for extended-support beta distributions using the workflow from the distributions3 package.\n\nXBeta(mu, phi, nu = 0)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Exceedence parameter for the support of the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\nIn order to obtain an extended-support beta distribution on [0, 1] an additional exceedence parameter nu is introduced. If nu &gt; 0, this scales the underlying beta distribution to the interval [-nu, 1 + nu] where the tails are subsequently censored to the unit interval [0, 1] with point masses on the boundaries 0 and 1. Thus, nu controls how likely boundary observations are and for nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\nA XBeta distribution object.\n\ndxbeta, BetaR\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- XBeta(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  nu = c(0, 0.1, 0.2)\n)\n\nX\n\n[1] \"XBeta(mu = 0.25, phi = 1, nu = 0.0)\" \"XBeta(mu = 0.50, phi = 1, nu = 0.1)\"\n[3] \"XBeta(mu = 0.75, phi = 2, nu = 0.2)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.2500000 0.5000000 0.7886591\n\nvariance(X)\n\n[1] 0.09375000 0.15331441 0.08617379\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## it is only continuous when there are no point masses on the boundary\nis_continuous(X)\n\n[1]  TRUE FALSE FALSE\n\ncdf(X, 0)\n\n[1] 0.0000000 0.1864295 0.0239812\n\ncdf(X, 1, lower.tail = FALSE)\n\n[1] 0.0000000 0.1864295 0.4695222\n\n## simulate random variables\nrandom(X, 5)\n\n           r_1       r_2        r_3        r_4       r_5\n[1,] 0.7497152 0.8385523 0.03196796 0.91882879 0.5454367\n[2,] 0.1263742 1.0000000 0.00000000 0.07151503 0.0000000\n[3,] 1.0000000 1.0000000 0.86031184 1.00000000 0.9764434\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.5305165 0.6607051\n\npdf(X, x, log = TRUE)\n\n[1] -0.3796622 -0.6339043 -0.4144477\n\nlog_pdf(X, x)\n\n[1] -0.3796622 -0.6339043 -0.4144477\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.3189318\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 0.97152842\n\n## cdf() and quantile() are inverses (except at censoring points)\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n           q_0.05      q_0.5    q_0.95\n[1,] 9.512588e-06 0.09331223 0.9118445\n[2,] 0.000000e+00 0.50000000 1.0000000\n[3,] 1.199277e-01 0.97152842 1.0000000\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 1.000000e+00\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 1.000000e+00\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]   0.2500000 0.2464581\n[2,]   0.5000000 0.4949177\n[3,]   0.7886591 0.7955785",
    "crumbs": [
      "distributions3 objects",
      "XBeta"
    ]
  },
  {
    "objectID": "man/dbeta4.html",
    "href": "man/dbeta4.html",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the 4-parameter beta distribution in regression parameterization.\n\n\n\ndbeta4(x, mu, phi, theta1 = 0, theta2 = 1 - theta1, log = FALSE)\n\npbeta4(q, mu, phi, theta1 = 0, theta2 = 1 - theta1, lower.tail = TRUE, log.p = FALSE)\n\nqbeta4(p, mu, phi, theta1 = 0, theta2 = 1 - theta1, lower.tail = TRUE, log.p = FALSE)\n\nrbeta4(n, mu, phi, theta1 = 0, theta2 = 1 - theta1)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\ntheta1, theta2\n\n\nnumeric. The minimum and maximum, respectively, of the 4-parameter beta distribution. By default a symmetric support is chosen by theta2 = 1 - theta1 which reduces to the classic beta distribution because of the default theta1 = 0.\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nThe distribution is obtained by a linear transformation of a beta-distributed random variable with intercept theta1 and slope theta2 - theta1.\n\n\n\ndbeta4 gives the density, pbeta4 gives the distribution function, qbeta4 gives the quantile function, and rbeta4 generates random deviates.\n\n\n\ndbetar, Beta4",
    "crumbs": [
      "Distributions",
      "dbeta4"
    ]
  },
  {
    "objectID": "man/dbeta4.html#the-4-parameter-beta-distribution-in-regression-parameterization",
    "href": "man/dbeta4.html#the-4-parameter-beta-distribution-in-regression-parameterization",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the 4-parameter beta distribution in regression parameterization.\n\n\n\ndbeta4(x, mu, phi, theta1 = 0, theta2 = 1 - theta1, log = FALSE)\n\npbeta4(q, mu, phi, theta1 = 0, theta2 = 1 - theta1, lower.tail = TRUE, log.p = FALSE)\n\nqbeta4(p, mu, phi, theta1 = 0, theta2 = 1 - theta1, lower.tail = TRUE, log.p = FALSE)\n\nrbeta4(n, mu, phi, theta1 = 0, theta2 = 1 - theta1)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\ntheta1, theta2\n\n\nnumeric. The minimum and maximum, respectively, of the 4-parameter beta distribution. By default a symmetric support is chosen by theta2 = 1 - theta1 which reduces to the classic beta distribution because of the default theta1 = 0.\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nThe distribution is obtained by a linear transformation of a beta-distributed random variable with intercept theta1 and slope theta2 - theta1.\n\n\n\ndbeta4 gives the density, pbeta4 gives the distribution function, qbeta4 gives the quantile function, and rbeta4 generates random deviates.\n\n\n\ndbetar, Beta4",
    "crumbs": [
      "Distributions",
      "dbeta4"
    ]
  },
  {
    "objectID": "man/ReadingSkills.html",
    "href": "man/ReadingSkills.html",
    "title": "betareg",
    "section": "",
    "text": "Data for assessing the contribution of non-verbal IQ to children’s reading skills in dyslexic and non-dyslexic children.\n\ndata(\"ReadingSkills\", package = \"betareg\")\n\nA data frame containing 44 observations on 3 variables.\n\n\naccuracy\n\n\nnumeric. Reading score with maximum restricted to be 0.99 rather than 1 (see below).\n\n\ndyslexia\n\n\nfactor. Is the child dyslexic? (A sum contrast rather than treatment contrast is employed.)\n\n\niq\n\n\nnumeric. Non-verbal intelligence quotient transformed to z-scores.\n\n\naccuracy1\n\n\nnumeric. Unrestricted reading score with a maximum of 1 (see below).\n\n\nThe data were collected by Pammer and Kevan (2004) and employed by Smithson and Verkuilen (2006). The original reading accuracy score was transformed by Smithson and Verkuilen (2006) so that accuracy is in the open unit interval (0, 1) and beta regression can be employed. First, the original accuracy was scaled using the minimal and maximal score (a and b, respectively) that can be obtained in the test: accuracy1 = (original_accuracy - a) / (b - a) (a and b are not provided). Subsequently, accuracy was obtained from accuracy1 by replacing all observations with a value of 1 with 0.99.\nKosmidis and Zeileis (2024) propose to investigate the original unrestricted accuracy1 variable using their extended-support beta mixture regression.\n\nExample 3 from Smithson and Verkuilen (2006) supplements.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\nPammer K, Kevan A (2004). The Contribution of Visual Sensitivity, Phonological Processing and Non-Verbal IQ to Children’s Reading. Unpublished manuscript, The Australian National University, Canberra.\nSmithson M, Verkuilen J (2006). A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables. Psychological Methods, 11(7), 54–71.\n\nbetareg, MockJurors, StressAnxiety\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\ndata(\"ReadingSkills\", package = \"betareg\")\n\n## Smithson & Verkuilen (2006, Table 5)\n## OLS regression\n## (Note: typo in iq coefficient: 0.3954 instead of 0.3594)\nrs_ols &lt;- lm(qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)\nsummary(rs_ols)\n\n\nCall:\nlm(formula = qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6640 -0.3797  0.0369  0.4089  2.5035 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.601      0.226    7.09  1.4e-08 ***\ndyslexia      -1.206      0.226   -5.34  4.0e-06 ***\niq             0.359      0.225    1.59    0.119    \ndyslexia:iq   -0.423      0.225   -1.88    0.068 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.2 on 40 degrees of freedom\nMultiple R-squared:  0.615, Adjusted R-squared:  0.586 \nF-statistic: 21.3 on 3 and 40 DF,  p-value: 2.08e-08\n\n## Beta regression (with numerical rather than analytic standard errors)\n## (Note: Smithson & Verkuilen erroneously compute one-sided p-values)\nrs_beta &lt;- betareg(accuracy ~ dyslexia * iq | dyslexia + iq,\n  data = ReadingSkills, hessian = TRUE)\nsummary(rs_beta)\n\n\nCall:\nbetareg(formula = accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills, \n    hessian = TRUE)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.362 -0.587  0.303  0.942  1.587 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.123      0.151    7.44  9.8e-14 ***\ndyslexia      -0.742      0.151   -4.90  9.7e-07 ***\niq             0.486      0.167    2.91  0.00360 ** \ndyslexia:iq   -0.581      0.173   -3.37  0.00076 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    3.304      0.227   14.59  &lt; 2e-16 ***\ndyslexia       1.747      0.294    5.94  2.8e-09 ***\niq             1.229      0.460    2.67   0.0075 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 65.9 on 7 Df\nPseudo R-squared: 0.576\nNumber of iterations in BFGS optimization: 25 \n\n## Extended-support beta mixture regression (Kosmidis & Zeileis 2024)\nrs_xbx &lt;- betareg(accuracy1 ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\nsummary(rs_xbx)\n\n\nCall:\nbetareg(formula = accuracy1 ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\n\nRandomized quantile residuals:\n   Min     1Q Median     3Q    Max \n-2.418 -0.598 -0.083  0.775  1.874 \n\nCoefficients (mu model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.903      0.217    4.17  3.1e-05 ***\ndyslexia      -0.606      0.182   -3.34  0.00084 ***\niq             0.329      0.188    1.75  0.07989 .  \ndyslexia:iq   -0.388      0.199   -1.94  0.05205 .  \n\nPhi coefficients (phi model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    3.499      0.530    6.60  4.1e-11 ***\ndyslexia       1.736      0.449    3.86  0.00011 ***\niq             0.697      0.571    1.22  0.22243    \n\nExceedence parameter (extended-support xbetax model):\n        Estimate Std. Error z value Pr(&gt;|z|)  \nLog(nu)   -1.790      0.854    -2.1    0.036 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nExceedence parameter nu: 0.167\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 18.5 on 8 Df\nNumber of iterations in BFGS optimization: 29 \n\n## Coefficients in XBX are typically somewhat shrunken compared to beta\ncbind(XBX = coef(rs_xbx), Beta = c(coef(rs_beta), NA))\n\n                      XBX    Beta\n(Intercept)        0.9030  1.1232\ndyslexia          -0.6065 -0.7416\niq                 0.3288  0.4864\ndyslexia:iq       -0.3875 -0.5813\n(phi)_(Intercept)  3.4990  3.3044\n(phi)_dyslexia     1.7360  1.7466\n(phi)_iq           0.6965  1.2291\nLog(nu)           -1.7905      NA\n\n## Visualization\nplot(accuracy1 ~ iq, data = ReadingSkills, col = c(4, 2)[dyslexia], pch = 19)\nnd &lt;- data.frame(dyslexia = \"no\", iq = -30:30/10)\nlines(nd$iq, predict(rs_xbx, nd), col = 4)\nlines(nd$iq, predict(rs_beta, nd), col = 4, lty = 5)\nlines(nd$iq, plogis(predict(rs_ols, nd)), col = 4, lty = 3)\nnd &lt;- data.frame(dyslexia = \"yes\", iq = -30:30/10)\nlines(nd$iq, predict(rs_xbx, nd), col = 2)\nlines(nd$iq, predict(rs_beta, nd), col = 2, lty = 5)\nlines(nd$iq, plogis(predict(rs_ols, nd)), col = 2, lty = 3)\nlegend(\"topleft\", c(\"Dyslexia: no\", \"Dyslexia: yes\", \"OLS\", \"XBX\", \"Beta\"),\n  lty = c(0, 0, 3, 1, 5), pch = c(19, 19, NA, NA, NA), col = c(4, 2, 1, 1, 1), bty = \"n\")\n\n\n\n\n\n\n## see demo(\"SmithsonVerkuilen2006\", package = \"betareg\") for further details",
    "crumbs": [
      "Data sets",
      "ReadingSkills"
    ]
  },
  {
    "objectID": "man/ReadingSkills.html#dyslexia-and-iq-predicting-reading-accuracy",
    "href": "man/ReadingSkills.html#dyslexia-and-iq-predicting-reading-accuracy",
    "title": "betareg",
    "section": "",
    "text": "Data for assessing the contribution of non-verbal IQ to children’s reading skills in dyslexic and non-dyslexic children.\n\ndata(\"ReadingSkills\", package = \"betareg\")\n\nA data frame containing 44 observations on 3 variables.\n\n\naccuracy\n\n\nnumeric. Reading score with maximum restricted to be 0.99 rather than 1 (see below).\n\n\ndyslexia\n\n\nfactor. Is the child dyslexic? (A sum contrast rather than treatment contrast is employed.)\n\n\niq\n\n\nnumeric. Non-verbal intelligence quotient transformed to z-scores.\n\n\naccuracy1\n\n\nnumeric. Unrestricted reading score with a maximum of 1 (see below).\n\n\nThe data were collected by Pammer and Kevan (2004) and employed by Smithson and Verkuilen (2006). The original reading accuracy score was transformed by Smithson and Verkuilen (2006) so that accuracy is in the open unit interval (0, 1) and beta regression can be employed. First, the original accuracy was scaled using the minimal and maximal score (a and b, respectively) that can be obtained in the test: accuracy1 = (original_accuracy - a) / (b - a) (a and b are not provided). Subsequently, accuracy was obtained from accuracy1 by replacing all observations with a value of 1 with 0.99.\nKosmidis and Zeileis (2024) propose to investigate the original unrestricted accuracy1 variable using their extended-support beta mixture regression.\n\nExample 3 from Smithson and Verkuilen (2006) supplements.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\nPammer K, Kevan A (2004). The Contribution of Visual Sensitivity, Phonological Processing and Non-Verbal IQ to Children’s Reading. Unpublished manuscript, The Australian National University, Canberra.\nSmithson M, Verkuilen J (2006). A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables. Psychological Methods, 11(7), 54–71.\n\nbetareg, MockJurors, StressAnxiety\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\ndata(\"ReadingSkills\", package = \"betareg\")\n\n## Smithson & Verkuilen (2006, Table 5)\n## OLS regression\n## (Note: typo in iq coefficient: 0.3954 instead of 0.3594)\nrs_ols &lt;- lm(qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)\nsummary(rs_ols)\n\n\nCall:\nlm(formula = qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6640 -0.3797  0.0369  0.4089  2.5035 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.601      0.226    7.09  1.4e-08 ***\ndyslexia      -1.206      0.226   -5.34  4.0e-06 ***\niq             0.359      0.225    1.59    0.119    \ndyslexia:iq   -0.423      0.225   -1.88    0.068 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.2 on 40 degrees of freedom\nMultiple R-squared:  0.615, Adjusted R-squared:  0.586 \nF-statistic: 21.3 on 3 and 40 DF,  p-value: 2.08e-08\n\n## Beta regression (with numerical rather than analytic standard errors)\n## (Note: Smithson & Verkuilen erroneously compute one-sided p-values)\nrs_beta &lt;- betareg(accuracy ~ dyslexia * iq | dyslexia + iq,\n  data = ReadingSkills, hessian = TRUE)\nsummary(rs_beta)\n\n\nCall:\nbetareg(formula = accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills, \n    hessian = TRUE)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.362 -0.587  0.303  0.942  1.587 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.123      0.151    7.44  9.8e-14 ***\ndyslexia      -0.742      0.151   -4.90  9.7e-07 ***\niq             0.486      0.167    2.91  0.00360 ** \ndyslexia:iq   -0.581      0.173   -3.37  0.00076 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    3.304      0.227   14.59  &lt; 2e-16 ***\ndyslexia       1.747      0.294    5.94  2.8e-09 ***\niq             1.229      0.460    2.67   0.0075 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 65.9 on 7 Df\nPseudo R-squared: 0.576\nNumber of iterations in BFGS optimization: 25 \n\n## Extended-support beta mixture regression (Kosmidis & Zeileis 2024)\nrs_xbx &lt;- betareg(accuracy1 ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\nsummary(rs_xbx)\n\n\nCall:\nbetareg(formula = accuracy1 ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\n\nRandomized quantile residuals:\n   Min     1Q Median     3Q    Max \n-2.418 -0.598 -0.083  0.775  1.874 \n\nCoefficients (mu model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.903      0.217    4.17  3.1e-05 ***\ndyslexia      -0.606      0.182   -3.34  0.00084 ***\niq             0.329      0.188    1.75  0.07989 .  \ndyslexia:iq   -0.388      0.199   -1.94  0.05205 .  \n\nPhi coefficients (phi model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    3.499      0.530    6.60  4.1e-11 ***\ndyslexia       1.736      0.449    3.86  0.00011 ***\niq             0.697      0.571    1.22  0.22243    \n\nExceedence parameter (extended-support xbetax model):\n        Estimate Std. Error z value Pr(&gt;|z|)  \nLog(nu)   -1.790      0.854    -2.1    0.036 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nExceedence parameter nu: 0.167\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 18.5 on 8 Df\nNumber of iterations in BFGS optimization: 29 \n\n## Coefficients in XBX are typically somewhat shrunken compared to beta\ncbind(XBX = coef(rs_xbx), Beta = c(coef(rs_beta), NA))\n\n                      XBX    Beta\n(Intercept)        0.9030  1.1232\ndyslexia          -0.6065 -0.7416\niq                 0.3288  0.4864\ndyslexia:iq       -0.3875 -0.5813\n(phi)_(Intercept)  3.4990  3.3044\n(phi)_dyslexia     1.7360  1.7466\n(phi)_iq           0.6965  1.2291\nLog(nu)           -1.7905      NA\n\n## Visualization\nplot(accuracy1 ~ iq, data = ReadingSkills, col = c(4, 2)[dyslexia], pch = 19)\nnd &lt;- data.frame(dyslexia = \"no\", iq = -30:30/10)\nlines(nd$iq, predict(rs_xbx, nd), col = 4)\nlines(nd$iq, predict(rs_beta, nd), col = 4, lty = 5)\nlines(nd$iq, plogis(predict(rs_ols, nd)), col = 4, lty = 3)\nnd &lt;- data.frame(dyslexia = \"yes\", iq = -30:30/10)\nlines(nd$iq, predict(rs_xbx, nd), col = 2)\nlines(nd$iq, predict(rs_beta, nd), col = 2, lty = 5)\nlines(nd$iq, plogis(predict(rs_ols, nd)), col = 2, lty = 3)\nlegend(\"topleft\", c(\"Dyslexia: no\", \"Dyslexia: yes\", \"OLS\", \"XBX\", \"Beta\"),\n  lty = c(0, 0, 3, 1, 5), pch = c(19, 19, NA, NA, NA), col = c(4, 2, 1, 1, 1), bty = \"n\")\n\n\n\n\n\n\n## see demo(\"SmithsonVerkuilen2006\", package = \"betareg\") for further details",
    "crumbs": [
      "Data sets",
      "ReadingSkills"
    ]
  },
  {
    "objectID": "man/Beta4.html",
    "href": "man/Beta4.html",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for 4-parameter beta distributions in regression specification using the workflow from the distributions3 package.\n\nBeta4(mu, phi, theta1 = 0, theta2 = 1 - theta1)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\ntheta1, theta2\n\n\nnumeric. The minimum and maximum, respectively, of the 4-parameter beta distribution. By default a symmetric support is chosen by theta2 = 1 - theta1 which reduces to the classic beta distribution because of the default theta1 = 0.\n\n\n\nThe distribution is obtained by a linear transformation of a beta-distributed random variable with intercept theta1 and slope theta2 - theta1.\n\nA Beta4 distribution object.\n\ndbeta4, BetaR\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- Beta4(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  theta1 = c(0, -0.1, -0.1),\n  theta2 = c(1, 1.1, 1.5)\n)\n\nX\n\n[1] \"Beta4(mu = 0.25, phi = 1, theta1 =  0.0, theta2 = 1.0)\"\n[2] \"Beta4(mu = 0.50, phi = 1, theta1 = -0.1, theta2 = 1.1)\"\n[3] \"Beta4(mu = 0.75, phi = 2, theta1 = -0.1, theta2 = 1.5)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.25 0.50 1.10\n\nvariance(X)\n\n[1] 0.09375 0.18000 0.16000\n\n## support interval (minimum and maximum)\nsupport(X)\n\n      min max\n[1,]  0.0 1.0\n[2,] -0.1 1.1\n[3,] -0.1 1.5\n\n## simulate random variables\nrandom(X, 5)\n\n           r_1       r_2         r_3        r_4         r_5\n[1,] 0.7497152 0.8385523  0.03196796 0.91882879  0.54543668\n[2,] 0.1263742 1.0978772 -0.09430773 0.07151503 -0.00499443\n[3,] 1.4311350 1.4850582  1.11178496 1.36985916  1.24450679\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.5305165 0.4235834\n\npdf(X, x, log = TRUE)\n\n[1] -0.3796622 -0.6339043 -0.8590048\n\nlog_pdf(X, x)\n\n[1] -0.3796622 -0.6339043 -0.8590048\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.2022198\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 1.23888962\n\n## cdf() and quantile() are inverses\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n            q_0.05      q_0.5    q_0.95\n[1,]  9.512588e-06 0.09331223 0.9118445\n[2,] -9.261300e-02 0.50000000 1.0926130\n[3,]  2.656317e-01 1.23888962 1.4975313\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 1.497531e+00\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 1.497531e+00\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]        0.25 0.2464581\n[2,]        0.50 0.4930360\n[3,]        1.10 1.1068752",
    "crumbs": [
      "distributions3 objects",
      "Beta4"
    ]
  },
  {
    "objectID": "man/Beta4.html#create-a-4-parameter-beta-distribution",
    "href": "man/Beta4.html#create-a-4-parameter-beta-distribution",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for 4-parameter beta distributions in regression specification using the workflow from the distributions3 package.\n\nBeta4(mu, phi, theta1 = 0, theta2 = 1 - theta1)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution that is extended to support [theta1, theta2].\n\n\n\n\ntheta1, theta2\n\n\nnumeric. The minimum and maximum, respectively, of the 4-parameter beta distribution. By default a symmetric support is chosen by theta2 = 1 - theta1 which reduces to the classic beta distribution because of the default theta1 = 0.\n\n\n\nThe distribution is obtained by a linear transformation of a beta-distributed random variable with intercept theta1 and slope theta2 - theta1.\n\nA Beta4 distribution object.\n\ndbeta4, BetaR\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- Beta4(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  theta1 = c(0, -0.1, -0.1),\n  theta2 = c(1, 1.1, 1.5)\n)\n\nX\n\n[1] \"Beta4(mu = 0.25, phi = 1, theta1 =  0.0, theta2 = 1.0)\"\n[2] \"Beta4(mu = 0.50, phi = 1, theta1 = -0.1, theta2 = 1.1)\"\n[3] \"Beta4(mu = 0.75, phi = 2, theta1 = -0.1, theta2 = 1.5)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.25 0.50 1.10\n\nvariance(X)\n\n[1] 0.09375 0.18000 0.16000\n\n## support interval (minimum and maximum)\nsupport(X)\n\n      min max\n[1,]  0.0 1.0\n[2,] -0.1 1.1\n[3,] -0.1 1.5\n\n## simulate random variables\nrandom(X, 5)\n\n           r_1       r_2         r_3        r_4         r_5\n[1,] 0.7497152 0.8385523  0.03196796 0.91882879  0.54543668\n[2,] 0.1263742 1.0978772 -0.09430773 0.07151503 -0.00499443\n[3,] 1.4311350 1.4850582  1.11178496 1.36985916  1.24450679\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.5305165 0.4235834\n\npdf(X, x, log = TRUE)\n\n[1] -0.3796622 -0.6339043 -0.8590048\n\nlog_pdf(X, x)\n\n[1] -0.3796622 -0.6339043 -0.8590048\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.2022198\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 1.23888962\n\n## cdf() and quantile() are inverses\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n            q_0.05      q_0.5    q_0.95\n[1,]  9.512588e-06 0.09331223 0.9118445\n[2,] -9.261300e-02 0.50000000 1.0926130\n[3,]  2.656317e-01 1.23888962 1.4975313\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 1.497531e+00\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 1.497531e+00\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]        0.25 0.2464581\n[2,]        0.50 0.4930360\n[3,]        1.10 1.1068752",
    "crumbs": [
      "distributions3 objects",
      "Beta4"
    ]
  },
  {
    "objectID": "man/plot.betareg.html",
    "href": "man/plot.betareg.html",
    "title": "betareg",
    "section": "",
    "text": "Various types of standard diagnostic plots can be produced, involving various types of residuals, influence measures etc.\n\n## S3 method for class 'betareg'\nplot(x, which = 1:4,\n  caption = c(\"Residuals vs indices of obs.\", \"Cook's distance plot\",\n    \"Generalized leverage vs predicted values\", \"Residuals vs linear predictor\", \n    \"Half-normal plot of residuals\", \"Predicted vs observed values\"),\n    sub.caption = paste(deparse(x\\$call), collapse = \"\\n\"), main = \"\", \n    ask = prod(par(\"mfcol\")) &lt; length(which) && dev.interactive(), \n    ..., type = \"quantile\", nsim = 100, level = 0.9)\n\n\n\n\n\nx\n\n\nfitted model object of class “betareg”.\n\n\n\n\nwhich\n\n\nnumeric. If a subset of the plots is required, specify a subset of the numbers 1:6.\n\n\n\n\ncaption\n\n\ncharacter. Captions to appear above the plots.\n\n\n\n\nsub.caption\n\n\ncharacter. Common title-above figures if there are multiple.\n\n\n\n\nmain\n\n\ncharacter. Title to each plot in addition to the above caption.\n\n\n\n\nask\n\n\nlogical. If TRUE, the user is asked before each plot.\n\n\n\n\n…\n\n\nother parameters to be passed through to plotting functions.\n\n\n\n\ntype\n\n\ncharacter indicating type of residual to be used, see residuals.betareg.\n\n\n\n\nnsim\n\n\nnumeric. Number of simulations in half-normal plots.\n\n\n\n\nlevel\n\n\nnumeric. Confidence level in half-normal plots.\n\n\n\nThe plot method for betareg objects produces various types of diagnostic plots. Most of these are standard for regression models and involve various types of residuals, influence measures etc. See Ferrari and Cribari-Neto (2004) for a discussion of some of these displays.\nThe which argument can be used to select a subset of currently six supported types of displays. The corresponding element of caption contains a brief description. In some more detail, the displays are: Residuals (as selected by type) vs indices of observations (which = 1). Cook’s distances vs indices of observations (which = 2). Generalized leverage vs predicted values (which = 3). Residuals vs linear predictor (which = 4). Half-normal plot of residuals (which = 5), which is obtained using a simulation approach. Predicted vs observed values (which = 6).\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy &lt;- betareg(yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\npar(mfrow = c(3, 2))\nplot(gy, which = 1:6)\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Beta regression",
      "plot.betareg"
    ]
  },
  {
    "objectID": "man/plot.betareg.html#diagnostic-plots-for-betareg-objects",
    "href": "man/plot.betareg.html#diagnostic-plots-for-betareg-objects",
    "title": "betareg",
    "section": "",
    "text": "Various types of standard diagnostic plots can be produced, involving various types of residuals, influence measures etc.\n\n## S3 method for class 'betareg'\nplot(x, which = 1:4,\n  caption = c(\"Residuals vs indices of obs.\", \"Cook's distance plot\",\n    \"Generalized leverage vs predicted values\", \"Residuals vs linear predictor\", \n    \"Half-normal plot of residuals\", \"Predicted vs observed values\"),\n    sub.caption = paste(deparse(x\\$call), collapse = \"\\n\"), main = \"\", \n    ask = prod(par(\"mfcol\")) &lt; length(which) && dev.interactive(), \n    ..., type = \"quantile\", nsim = 100, level = 0.9)\n\n\n\n\n\nx\n\n\nfitted model object of class “betareg”.\n\n\n\n\nwhich\n\n\nnumeric. If a subset of the plots is required, specify a subset of the numbers 1:6.\n\n\n\n\ncaption\n\n\ncharacter. Captions to appear above the plots.\n\n\n\n\nsub.caption\n\n\ncharacter. Common title-above figures if there are multiple.\n\n\n\n\nmain\n\n\ncharacter. Title to each plot in addition to the above caption.\n\n\n\n\nask\n\n\nlogical. If TRUE, the user is asked before each plot.\n\n\n\n\n…\n\n\nother parameters to be passed through to plotting functions.\n\n\n\n\ntype\n\n\ncharacter indicating type of residual to be used, see residuals.betareg.\n\n\n\n\nnsim\n\n\nnumeric. Number of simulations in half-normal plots.\n\n\n\n\nlevel\n\n\nnumeric. Confidence level in half-normal plots.\n\n\n\nThe plot method for betareg objects produces various types of diagnostic plots. Most of these are standard for regression models and involve various types of residuals, influence measures etc. See Ferrari and Cribari-Neto (2004) for a discussion of some of these displays.\nThe which argument can be used to select a subset of currently six supported types of displays. The corresponding element of caption contains a brief description. In some more detail, the displays are: Residuals (as selected by type) vs indices of observations (which = 1). Cook’s distances vs indices of observations (which = 2). Generalized leverage vs predicted values (which = 3). Residuals vs linear predictor (which = 4). Half-normal plot of residuals (which = 5), which is obtained using a simulation approach. Predicted vs observed values (which = 6).\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy &lt;- betareg(yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\npar(mfrow = c(3, 2))\nplot(gy, which = 1:6)\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Beta regression",
      "plot.betareg"
    ]
  },
  {
    "objectID": "man/dbetar.html",
    "href": "man/dbetar.html",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the beta distribution in regression parameterization.\n\n\n\ndbetar(x, mu, phi, log = FALSE)\n\npbetar(q, mu, phi, lower.tail = TRUE, log.p = FALSE)\n\nqbetar(p, mu, phi, lower.tail = TRUE, log.p = FALSE)\n\nrbetar(n, mu, phi)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution.\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nThis is the reparameterization of the beta distribution with mean mu and precision phi, as employed in beta regression. The classic parameterization of the beta distribution is obtained by setting shape1 = mu * phi and shape2 = (1 - mu) * phi, respectively.\n\n\n\ndbetar gives the density, pbetar gives the distribution function, qbetar gives the quantile function, and rbetar generates random deviates.\n\n\n\ndbeta, BetaR",
    "crumbs": [
      "Distributions",
      "dbetar"
    ]
  },
  {
    "objectID": "man/dbetar.html#the-beta-distribution-in-regression-parameterization",
    "href": "man/dbetar.html#the-beta-distribution-in-regression-parameterization",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the beta distribution in regression parameterization.\n\n\n\ndbetar(x, mu, phi, log = FALSE)\n\npbetar(q, mu, phi, lower.tail = TRUE, log.p = FALSE)\n\nqbetar(p, mu, phi, lower.tail = TRUE, log.p = FALSE)\n\nrbetar(n, mu, phi)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution.\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nThis is the reparameterization of the beta distribution with mean mu and precision phi, as employed in beta regression. The classic parameterization of the beta distribution is obtained by setting shape1 = mu * phi and shape2 = (1 - mu) * phi, respectively.\n\n\n\ndbetar gives the density, pbetar gives the distribution function, qbetar gives the quantile function, and rbetar generates random deviates.\n\n\n\ndbeta, BetaR",
    "crumbs": [
      "Distributions",
      "dbetar"
    ]
  },
  {
    "objectID": "man/summary.betareg.html",
    "href": "man/summary.betareg.html",
    "title": "betareg",
    "section": "",
    "text": "Methods for extracting information from fitted beta regression model objects of class “betareg”.\n\n## S3 method for class 'betareg'\nsummary(object, phi = NULL, type = \"quantile\", ...)\n\n## S3 method for class 'betareg'\ncoef(object, model = c(\"full\", \"mean\", \"precision\"), phi = NULL, ...)\n## S3 method for class 'betareg'\nvcov(object, model = c(\"full\", \"mean\", \"precision\"), phi = NULL, ...)\n## S3 method for class 'betareg'\nbread(x, phi = NULL, ...)\n## S3 method for class 'betareg'\nestfun(x, phi = NULL, ...)\n\n\n\n\n\nobject, x\n\n\nfitted model object of class “betareg”.\n\n\n\n\nphi\n\n\nlogical indicating whether the parameters in the precision model (for phi) should be reported as full model parameters (TRUE) or nuisance parameters (FALSE). The default is taken from object$phi.\n\n\n\n\ntype\n\n\ncharacter specifying type of residuals to be included in the summary output, see residuals.betareg.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model coefficients/covariance should be extracted. (Only used if phi is NULL.)\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “betareg”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix. As usual, the summary method returns an object of class “summary.betareg” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nSimas AB, Barreto-Souza W, Rocha AV (2010). Improved Estimators for a General Class of Beta Regression Models. Computational Statistics & Data Analysis, 54(2), 348–366.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp | temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.104 -0.585 -0.143  0.690  2.520 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.923236   0.183526  -32.27  &lt; 2e-16 ***\nbatch1       1.601988   0.063856   25.09  &lt; 2e-16 ***\nbatch2       1.297266   0.099100   13.09  &lt; 2e-16 ***\nbatch3       1.565338   0.099739   15.69  &lt; 2e-16 ***\nbatch4       1.030072   0.063288   16.28  &lt; 2e-16 ***\nbatch5       1.154163   0.065643   17.58  &lt; 2e-16 ***\nbatch6       1.019445   0.066351   15.36  &lt; 2e-16 ***\nbatch7       0.622259   0.065632    9.48  &lt; 2e-16 ***\nbatch8       0.564583   0.060185    9.38  &lt; 2e-16 ***\nbatch9       0.359439   0.067141    5.35  8.6e-08 ***\ntemp         0.010359   0.000436   23.75  &lt; 2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.36409    1.22578    1.11     0.27    \ntemp         0.01457    0.00362    4.03  5.7e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   87 on 13 Df\nPseudo R-squared: 0.952\nNumber of iterations: 33 (BFGS) + 28 (Fisher scoring) \n\ncoef(gy2)\n\n      (Intercept)            batch1            batch2            batch3 \n         -5.92324           1.60199           1.29727           1.56534 \n           batch4            batch5            batch6            batch7 \n          1.03007           1.15416           1.01944           0.62226 \n           batch8            batch9              temp (phi)_(Intercept) \n          0.56458           0.35944           0.01036           1.36409 \n       (phi)_temp \n          0.01457 \n\nvcov(gy2)\n\n                  (Intercept)     batch1     batch2     batch3     batch4\n(Intercept)         3.368e-02 -4.124e-03 -8.216e-03 -8.839e-03 -3.672e-03\nbatch1             -4.124e-03  4.078e-03  2.483e-03  2.524e-03  2.189e-03\nbatch2             -8.216e-03  2.483e-03  9.821e-03  3.401e-03  2.395e-03\nbatch3             -8.839e-03  2.524e-03  3.401e-03  9.948e-03  2.427e-03\nbatch4             -3.672e-03  2.189e-03  2.395e-03  2.427e-03  4.005e-03\nbatch5             -4.461e-03  2.240e-03  2.548e-03  2.594e-03  2.206e-03\nbatch6             -3.902e-03  2.204e-03  2.439e-03  2.475e-03  2.178e-03\nbatch7             -3.007e-03  2.146e-03  2.267e-03  2.285e-03  2.133e-03\nbatch8             -6.259e-04  1.993e-03  1.804e-03  1.775e-03  2.013e-03\nbatch9             -1.801e-03  2.068e-03  2.031e-03  2.026e-03  2.072e-03\ntemp               -7.753e-05  4.999e-06  1.504e-05  1.657e-05  3.891e-06\n(phi)_(Intercept)  -1.860e-02  1.682e-04  9.769e-04  1.420e-03  1.409e-04\n(phi)_temp          4.618e-05  2.069e-07 -1.937e-06 -2.948e-06  6.530e-08\n                      batch5     batch6     batch7     batch8     batch9\n(Intercept)       -4.461e-03 -3.902e-03 -3.007e-03 -6.259e-04 -1.801e-03\nbatch1             2.240e-03  2.204e-03  2.146e-03  1.993e-03  2.068e-03\nbatch2             2.548e-03  2.439e-03  2.267e-03  1.804e-03  2.031e-03\nbatch3             2.594e-03  2.475e-03  2.285e-03  1.775e-03  2.026e-03\nbatch4             2.206e-03  2.178e-03  2.133e-03  2.013e-03  2.072e-03\nbatch5             4.309e-03  2.223e-03  2.156e-03  1.977e-03  2.065e-03\nbatch6             2.223e-03  4.402e-03  2.140e-03  2.003e-03  2.070e-03\nbatch7             2.156e-03  2.140e-03  4.308e-03  2.044e-03  2.078e-03\nbatch8             1.977e-03  2.003e-03  2.044e-03  3.622e-03  2.100e-03\nbatch9             2.065e-03  2.070e-03  2.078e-03  2.100e-03  4.508e-03\ntemp               5.827e-06  4.454e-06  2.259e-06 -3.585e-06 -7.000e-07\n(phi)_(Intercept)  1.011e-03  5.045e-04 -4.523e-04 -1.307e-03 -3.533e-04\n(phi)_temp        -2.185e-06 -8.969e-07  1.470e-06  3.675e-06  1.119e-06\n                        temp (phi)_(Intercept) (phi)_temp\n(Intercept)       -7.753e-05        -1.860e-02  4.618e-05\nbatch1             4.999e-06         1.682e-04  2.069e-07\nbatch2             1.504e-05         9.769e-04 -1.937e-06\nbatch3             1.657e-05         1.420e-03 -2.948e-06\nbatch4             3.891e-06         1.409e-04  6.530e-08\nbatch5             5.827e-06         1.011e-03 -2.185e-06\nbatch6             4.454e-06         5.045e-04 -8.969e-07\nbatch7             2.259e-06        -4.523e-04  1.470e-06\nbatch8            -3.585e-06        -1.307e-03  3.675e-06\nbatch9            -7.000e-07        -3.533e-04  1.119e-06\ntemp               1.902e-07         4.666e-05 -1.175e-07\n(phi)_(Intercept)  4.666e-05         1.503e+00 -4.342e-03\n(phi)_temp        -1.175e-07        -4.342e-03  1.309e-05\n\nlogLik(gy2)\n\n'log Lik.' 86.98 (df=13)\n\nAIC(gy2)\n\n[1] -148\n\ncoef(gy2, model = \"mean\")\n\n(Intercept)      batch1      batch2      batch3      batch4      batch5 \n   -5.92324     1.60199     1.29727     1.56534     1.03007     1.15416 \n     batch6      batch7      batch8      batch9        temp \n    1.01944     0.62226     0.56458     0.35944     0.01036 \n\ncoef(gy2, model = \"precision\")\n\n(Intercept)        temp \n    1.36409     0.01457 \n\nsummary(gy2, phi = FALSE)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp | temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.104 -0.585 -0.143  0.690  2.520 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.923236   0.183526  -32.27  &lt; 2e-16 ***\nbatch1       1.601988   0.063856   25.09  &lt; 2e-16 ***\nbatch2       1.297266   0.099100   13.09  &lt; 2e-16 ***\nbatch3       1.565338   0.099739   15.69  &lt; 2e-16 ***\nbatch4       1.030072   0.063288   16.28  &lt; 2e-16 ***\nbatch5       1.154163   0.065643   17.58  &lt; 2e-16 ***\nbatch6       1.019445   0.066351   15.36  &lt; 2e-16 ***\nbatch7       0.622259   0.065632    9.48  &lt; 2e-16 ***\nbatch8       0.564583   0.060185    9.38  &lt; 2e-16 ***\nbatch9       0.359439   0.067141    5.35  8.6e-08 ***\ntemp         0.010359   0.000436   23.75  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   87 on 13 Df\nPseudo R-squared: 0.952\nNumber of iterations: 33 (BFGS) + 28 (Fisher scoring)",
    "crumbs": [
      "Beta regression",
      "summary.betareg"
    ]
  },
  {
    "objectID": "man/summary.betareg.html#methods-for-betareg-objects",
    "href": "man/summary.betareg.html#methods-for-betareg-objects",
    "title": "betareg",
    "section": "",
    "text": "Methods for extracting information from fitted beta regression model objects of class “betareg”.\n\n## S3 method for class 'betareg'\nsummary(object, phi = NULL, type = \"quantile\", ...)\n\n## S3 method for class 'betareg'\ncoef(object, model = c(\"full\", \"mean\", \"precision\"), phi = NULL, ...)\n## S3 method for class 'betareg'\nvcov(object, model = c(\"full\", \"mean\", \"precision\"), phi = NULL, ...)\n## S3 method for class 'betareg'\nbread(x, phi = NULL, ...)\n## S3 method for class 'betareg'\nestfun(x, phi = NULL, ...)\n\n\n\n\n\nobject, x\n\n\nfitted model object of class “betareg”.\n\n\n\n\nphi\n\n\nlogical indicating whether the parameters in the precision model (for phi) should be reported as full model parameters (TRUE) or nuisance parameters (FALSE). The default is taken from object$phi.\n\n\n\n\ntype\n\n\ncharacter specifying type of residuals to be included in the summary output, see residuals.betareg.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model coefficients/covariance should be extracted. (Only used if phi is NULL.)\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “betareg”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix. As usual, the summary method returns an object of class “summary.betareg” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nSimas AB, Barreto-Souza W, Rocha AV (2010). Improved Estimators for a General Class of Beta Regression Models. Computational Statistics & Data Analysis, 54(2), 348–366.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp | temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.104 -0.585 -0.143  0.690  2.520 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.923236   0.183526  -32.27  &lt; 2e-16 ***\nbatch1       1.601988   0.063856   25.09  &lt; 2e-16 ***\nbatch2       1.297266   0.099100   13.09  &lt; 2e-16 ***\nbatch3       1.565338   0.099739   15.69  &lt; 2e-16 ***\nbatch4       1.030072   0.063288   16.28  &lt; 2e-16 ***\nbatch5       1.154163   0.065643   17.58  &lt; 2e-16 ***\nbatch6       1.019445   0.066351   15.36  &lt; 2e-16 ***\nbatch7       0.622259   0.065632    9.48  &lt; 2e-16 ***\nbatch8       0.564583   0.060185    9.38  &lt; 2e-16 ***\nbatch9       0.359439   0.067141    5.35  8.6e-08 ***\ntemp         0.010359   0.000436   23.75  &lt; 2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.36409    1.22578    1.11     0.27    \ntemp         0.01457    0.00362    4.03  5.7e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   87 on 13 Df\nPseudo R-squared: 0.952\nNumber of iterations: 33 (BFGS) + 28 (Fisher scoring) \n\ncoef(gy2)\n\n      (Intercept)            batch1            batch2            batch3 \n         -5.92324           1.60199           1.29727           1.56534 \n           batch4            batch5            batch6            batch7 \n          1.03007           1.15416           1.01944           0.62226 \n           batch8            batch9              temp (phi)_(Intercept) \n          0.56458           0.35944           0.01036           1.36409 \n       (phi)_temp \n          0.01457 \n\nvcov(gy2)\n\n                  (Intercept)     batch1     batch2     batch3     batch4\n(Intercept)         3.368e-02 -4.124e-03 -8.216e-03 -8.839e-03 -3.672e-03\nbatch1             -4.124e-03  4.078e-03  2.483e-03  2.524e-03  2.189e-03\nbatch2             -8.216e-03  2.483e-03  9.821e-03  3.401e-03  2.395e-03\nbatch3             -8.839e-03  2.524e-03  3.401e-03  9.948e-03  2.427e-03\nbatch4             -3.672e-03  2.189e-03  2.395e-03  2.427e-03  4.005e-03\nbatch5             -4.461e-03  2.240e-03  2.548e-03  2.594e-03  2.206e-03\nbatch6             -3.902e-03  2.204e-03  2.439e-03  2.475e-03  2.178e-03\nbatch7             -3.007e-03  2.146e-03  2.267e-03  2.285e-03  2.133e-03\nbatch8             -6.259e-04  1.993e-03  1.804e-03  1.775e-03  2.013e-03\nbatch9             -1.801e-03  2.068e-03  2.031e-03  2.026e-03  2.072e-03\ntemp               -7.753e-05  4.999e-06  1.504e-05  1.657e-05  3.891e-06\n(phi)_(Intercept)  -1.860e-02  1.682e-04  9.769e-04  1.420e-03  1.409e-04\n(phi)_temp          4.618e-05  2.069e-07 -1.937e-06 -2.948e-06  6.530e-08\n                      batch5     batch6     batch7     batch8     batch9\n(Intercept)       -4.461e-03 -3.902e-03 -3.007e-03 -6.259e-04 -1.801e-03\nbatch1             2.240e-03  2.204e-03  2.146e-03  1.993e-03  2.068e-03\nbatch2             2.548e-03  2.439e-03  2.267e-03  1.804e-03  2.031e-03\nbatch3             2.594e-03  2.475e-03  2.285e-03  1.775e-03  2.026e-03\nbatch4             2.206e-03  2.178e-03  2.133e-03  2.013e-03  2.072e-03\nbatch5             4.309e-03  2.223e-03  2.156e-03  1.977e-03  2.065e-03\nbatch6             2.223e-03  4.402e-03  2.140e-03  2.003e-03  2.070e-03\nbatch7             2.156e-03  2.140e-03  4.308e-03  2.044e-03  2.078e-03\nbatch8             1.977e-03  2.003e-03  2.044e-03  3.622e-03  2.100e-03\nbatch9             2.065e-03  2.070e-03  2.078e-03  2.100e-03  4.508e-03\ntemp               5.827e-06  4.454e-06  2.259e-06 -3.585e-06 -7.000e-07\n(phi)_(Intercept)  1.011e-03  5.045e-04 -4.523e-04 -1.307e-03 -3.533e-04\n(phi)_temp        -2.185e-06 -8.969e-07  1.470e-06  3.675e-06  1.119e-06\n                        temp (phi)_(Intercept) (phi)_temp\n(Intercept)       -7.753e-05        -1.860e-02  4.618e-05\nbatch1             4.999e-06         1.682e-04  2.069e-07\nbatch2             1.504e-05         9.769e-04 -1.937e-06\nbatch3             1.657e-05         1.420e-03 -2.948e-06\nbatch4             3.891e-06         1.409e-04  6.530e-08\nbatch5             5.827e-06         1.011e-03 -2.185e-06\nbatch6             4.454e-06         5.045e-04 -8.969e-07\nbatch7             2.259e-06        -4.523e-04  1.470e-06\nbatch8            -3.585e-06        -1.307e-03  3.675e-06\nbatch9            -7.000e-07        -3.533e-04  1.119e-06\ntemp               1.902e-07         4.666e-05 -1.175e-07\n(phi)_(Intercept)  4.666e-05         1.503e+00 -4.342e-03\n(phi)_temp        -1.175e-07        -4.342e-03  1.309e-05\n\nlogLik(gy2)\n\n'log Lik.' 86.98 (df=13)\n\nAIC(gy2)\n\n[1] -148\n\ncoef(gy2, model = \"mean\")\n\n(Intercept)      batch1      batch2      batch3      batch4      batch5 \n   -5.92324     1.60199     1.29727     1.56534     1.03007     1.15416 \n     batch6      batch7      batch8      batch9        temp \n    1.01944     0.62226     0.56458     0.35944     0.01036 \n\ncoef(gy2, model = \"precision\")\n\n(Intercept)        temp \n    1.36409     0.01457 \n\nsummary(gy2, phi = FALSE)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp | temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.104 -0.585 -0.143  0.690  2.520 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.923236   0.183526  -32.27  &lt; 2e-16 ***\nbatch1       1.601988   0.063856   25.09  &lt; 2e-16 ***\nbatch2       1.297266   0.099100   13.09  &lt; 2e-16 ***\nbatch3       1.565338   0.099739   15.69  &lt; 2e-16 ***\nbatch4       1.030072   0.063288   16.28  &lt; 2e-16 ***\nbatch5       1.154163   0.065643   17.58  &lt; 2e-16 ***\nbatch6       1.019445   0.066351   15.36  &lt; 2e-16 ***\nbatch7       0.622259   0.065632    9.48  &lt; 2e-16 ***\nbatch8       0.564583   0.060185    9.38  &lt; 2e-16 ***\nbatch9       0.359439   0.067141    5.35  8.6e-08 ***\ntemp         0.010359   0.000436   23.75  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   87 on 13 Df\nPseudo R-squared: 0.952\nNumber of iterations: 33 (BFGS) + 28 (Fisher scoring)",
    "crumbs": [
      "Beta regression",
      "summary.betareg"
    ]
  },
  {
    "objectID": "man/betareg.control.html",
    "href": "man/betareg.control.html",
    "title": "betareg",
    "section": "",
    "text": "Various parameters that control fitting of beta regression models using betareg.\n\nbetareg.control(phi = TRUE, method = \"BFGS\", maxit = 5000,\n  gradient = NULL, hessian = FALSE, trace = FALSE, start = NULL,\n  fsmaxit = 200, fstol = 1e-8, quad = 20, ...)\n\n\n\n\n\nphi\n\n\nlogical indicating whether the precision parameter phi should be treated as a full model parameter (TRUE, default) or as a nuisance parameter.\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim. Additionally, method = “nlminb” can be used to employ nlminb, instead.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\ntrace\n\n\nlogical or integer controlling whether tracing information on\nthe progress of the optimization should be produced (passed to optim).\n\n\n\n\ngradient\n\n\nlogical. Should the analytical gradient be used for optimizing the log-likelihood? If set to FALSE a finite-difference approximation is used instead. The default of NULL signals that analytical gradients are only used for the classical “beta” distribution but not for “xbetax” or “xbeta”.\n\n\n\n\nhessian\n\n\nlogical. Should the numerical Hessian matrix from the optim output be used for estimation of the covariance matrix? By default the analytical solution is employed. For details see below.\n\n\n\n\nstart\n\n\nan optional vector with starting values for all parameters (including phi).\n\n\n\n\nfsmaxit\n\n\ninteger specifying maximal number of additional (quasi) Fisher scoring iterations. For details see below.\n\n\n\n\nfstol\n\n\nnumeric tolerance for convergence in (quasi) Fisher scoring. For details see below.\n\n\n\n\nquad\n\n\nnumeric. The number of quadrature points for numeric integration in case of dist = “xbetax” is used in the beta regression.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in betareg are estimated by maximum likelihood using optim with control options set in betareg.control. Most arguments are passed on directly to optim, and start controls how optim is called.\nAfter the optim maximization, an additional (quasi) Fisher scoring can be perfomed to further enhance the result or to perform additional bias reduction. If fsmaxit is greater than zero, this additional optimization is performed and it converges if the threshold fstol is attained for the cross-product of the step size.\nStarting values can be supplied via start or estimated by lm.wfit, using the link-transformed response. Covariances are in general derived analytically. Only if type = “ML” and hessian = TRUE, they are determined numerically using the Hessian matrix returned by optim. In the latter case no Fisher scoring iterations are performed.\nThe main parameters of interest are the coefficients in the linear predictor of the model and the additional precision parameter phi which can either be treated as a full model parameter (default) or as a nuisance parameter. In the latter case the estimation does not change, only the reported information in output from print, summary, or coef (among others) will be different. See also examples.\n\nA list with the arguments specified.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\n## regression with phi as full model parameter\ngy1 &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\ngy1\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nCoefficients (mean model with logit link):\n(Intercept)       batch1       batch2       batch3       batch4       batch5  \n     -6.160        1.728        1.323        1.572        1.060        1.134  \n     batch6       batch7       batch8       batch9         temp  \n      1.040        0.544        0.496        0.386        0.011  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n  440  \n\n## regression with phi as nuisance parameter\ngy2 &lt;- betareg(yield ~ batch + temp, data = GasolineYield, phi = FALSE)\ngy2\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, phi = FALSE)\n\nCoefficients (mean model with logit link):\n(Intercept)       batch1       batch2       batch3       batch4       batch5  \n     -6.160        1.728        1.323        1.572        1.060        1.134  \n     batch6       batch7       batch8       batch9         temp  \n      1.040        0.544        0.496        0.386        0.011  \n\n## compare reported output\ncoef(gy1)\n\n(Intercept)      batch1      batch2      batch3      batch4      batch5 \n   -6.15957     1.72773     1.32260     1.57231     1.05971     1.13375 \n     batch6      batch7      batch8      batch9        temp       (phi) \n    1.04016     0.54369     0.49590     0.38579     0.01097   440.27839 \n\ncoef(gy2)\n\n(Intercept)      batch1      batch2      batch3      batch4      batch5 \n   -6.15957     1.72773     1.32260     1.57231     1.05971     1.13375 \n     batch6      batch7      batch8      batch9        temp \n    1.04016     0.54369     0.49590     0.38579     0.01097 \n\nsummary(gy1)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.140 -0.570  0.120  0.704  1.751 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\nbatch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\nbatch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\nbatch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\nbatch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\nbatch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\nbatch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\nbatch7       0.543692   0.109127    4.98  6.3e-07 ***\nbatch8       0.495901   0.108926    4.55  5.3e-06 ***\nbatch9       0.385793   0.118593    3.25   0.0011 ** \ntemp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)      440        110       4  6.3e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 84.8 on 12 Df\nPseudo R-squared: 0.962\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring) \n\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, phi = FALSE)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.140 -0.570  0.120  0.704  1.751 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\nbatch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\nbatch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\nbatch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\nbatch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\nbatch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\nbatch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\nbatch7       0.543692   0.109127    4.98  6.3e-07 ***\nbatch8       0.495901   0.108926    4.55  5.3e-06 ***\nbatch9       0.385793   0.118593    3.25   0.0011 ** \ntemp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 84.8 on 12 Df\nPseudo R-squared: 0.962\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring)",
    "crumbs": [
      "Beta regression",
      "betareg.control"
    ]
  },
  {
    "objectID": "man/betareg.control.html#control-parameters-for-beta-regression",
    "href": "man/betareg.control.html#control-parameters-for-beta-regression",
    "title": "betareg",
    "section": "",
    "text": "Various parameters that control fitting of beta regression models using betareg.\n\nbetareg.control(phi = TRUE, method = \"BFGS\", maxit = 5000,\n  gradient = NULL, hessian = FALSE, trace = FALSE, start = NULL,\n  fsmaxit = 200, fstol = 1e-8, quad = 20, ...)\n\n\n\n\n\nphi\n\n\nlogical indicating whether the precision parameter phi should be treated as a full model parameter (TRUE, default) or as a nuisance parameter.\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim. Additionally, method = “nlminb” can be used to employ nlminb, instead.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\ntrace\n\n\nlogical or integer controlling whether tracing information on\nthe progress of the optimization should be produced (passed to optim).\n\n\n\n\ngradient\n\n\nlogical. Should the analytical gradient be used for optimizing the log-likelihood? If set to FALSE a finite-difference approximation is used instead. The default of NULL signals that analytical gradients are only used for the classical “beta” distribution but not for “xbetax” or “xbeta”.\n\n\n\n\nhessian\n\n\nlogical. Should the numerical Hessian matrix from the optim output be used for estimation of the covariance matrix? By default the analytical solution is employed. For details see below.\n\n\n\n\nstart\n\n\nan optional vector with starting values for all parameters (including phi).\n\n\n\n\nfsmaxit\n\n\ninteger specifying maximal number of additional (quasi) Fisher scoring iterations. For details see below.\n\n\n\n\nfstol\n\n\nnumeric tolerance for convergence in (quasi) Fisher scoring. For details see below.\n\n\n\n\nquad\n\n\nnumeric. The number of quadrature points for numeric integration in case of dist = “xbetax” is used in the beta regression.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in betareg are estimated by maximum likelihood using optim with control options set in betareg.control. Most arguments are passed on directly to optim, and start controls how optim is called.\nAfter the optim maximization, an additional (quasi) Fisher scoring can be perfomed to further enhance the result or to perform additional bias reduction. If fsmaxit is greater than zero, this additional optimization is performed and it converges if the threshold fstol is attained for the cross-product of the step size.\nStarting values can be supplied via start or estimated by lm.wfit, using the link-transformed response. Covariances are in general derived analytically. Only if type = “ML” and hessian = TRUE, they are determined numerically using the Hessian matrix returned by optim. In the latter case no Fisher scoring iterations are performed.\nThe main parameters of interest are the coefficients in the linear predictor of the model and the additional precision parameter phi which can either be treated as a full model parameter (default) or as a nuisance parameter. In the latter case the estimation does not change, only the reported information in output from print, summary, or coef (among others) will be different. See also examples.\n\nA list with the arguments specified.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\n## regression with phi as full model parameter\ngy1 &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\ngy1\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nCoefficients (mean model with logit link):\n(Intercept)       batch1       batch2       batch3       batch4       batch5  \n     -6.160        1.728        1.323        1.572        1.060        1.134  \n     batch6       batch7       batch8       batch9         temp  \n      1.040        0.544        0.496        0.386        0.011  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n  440  \n\n## regression with phi as nuisance parameter\ngy2 &lt;- betareg(yield ~ batch + temp, data = GasolineYield, phi = FALSE)\ngy2\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, phi = FALSE)\n\nCoefficients (mean model with logit link):\n(Intercept)       batch1       batch2       batch3       batch4       batch5  \n     -6.160        1.728        1.323        1.572        1.060        1.134  \n     batch6       batch7       batch8       batch9         temp  \n      1.040        0.544        0.496        0.386        0.011  \n\n## compare reported output\ncoef(gy1)\n\n(Intercept)      batch1      batch2      batch3      batch4      batch5 \n   -6.15957     1.72773     1.32260     1.57231     1.05971     1.13375 \n     batch6      batch7      batch8      batch9        temp       (phi) \n    1.04016     0.54369     0.49590     0.38579     0.01097   440.27839 \n\ncoef(gy2)\n\n(Intercept)      batch1      batch2      batch3      batch4      batch5 \n   -6.15957     1.72773     1.32260     1.57231     1.05971     1.13375 \n     batch6      batch7      batch8      batch9        temp \n    1.04016     0.54369     0.49590     0.38579     0.01097 \n\nsummary(gy1)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.140 -0.570  0.120  0.704  1.751 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\nbatch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\nbatch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\nbatch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\nbatch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\nbatch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\nbatch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\nbatch7       0.543692   0.109127    4.98  6.3e-07 ***\nbatch8       0.495901   0.108926    4.55  5.3e-06 ***\nbatch9       0.385793   0.118593    3.25   0.0011 ** \ntemp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)      440        110       4  6.3e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 84.8 on 12 Df\nPseudo R-squared: 0.962\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring) \n\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, phi = FALSE)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.140 -0.570  0.120  0.704  1.751 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\nbatch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\nbatch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\nbatch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\nbatch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\nbatch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\nbatch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\nbatch7       0.543692   0.109127    4.98  6.3e-07 ***\nbatch8       0.495901   0.108926    4.55  5.3e-06 ***\nbatch9       0.385793   0.118593    3.25   0.0011 ** \ntemp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 84.8 on 12 Df\nPseudo R-squared: 0.962\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring)",
    "crumbs": [
      "Beta regression",
      "betareg.control"
    ]
  },
  {
    "objectID": "man/CarTask.html",
    "href": "man/CarTask.html",
    "title": "betareg",
    "section": "",
    "text": "In this study participants were asked to judge how likely it is that a customer trades in a coupe or that a customer buys a car form a specific salesperson out of four possible salespersons.\n\ndata(\"CarTask\", package = \"betareg\")\n\nA data frame with 155 observations on the following 3 variables.\n\n\ntask\n\n\na factor with levels Car and Salesperson indicating the condition.\n\n\nprobability\n\n\na numeric vector of the estimated probability.\n\n\nNFCCscale\n\n\na numeric vector of the NFCC scale.\n\n\nAll participants in the study were undergraduate students at The Australian National University, some of whom obtained course credit in first-year Psychology for their participation in the study.\nThe NFCC scale is a combined scale of the Need for Closure and Need for Certainty scales which are strongly correlated.\nFor task the questions were:\n\n\nCar\n\n\nWhat is the probability that a customer trades in a coupe?\n\n\nSalesperson\n\n\nWhat is the probability that a customer buys a car from Carlos?\n\n\nTaken from Smithson et al. (2011) supplements.\n\nSmithson, M., Merkle, E.C., and Verkuilen, J. (2011). Beta Regression Finite Mixture Models of Polarization and Priming. Journal of Educational and Behavioral Statistics, 36(6), 804–831. doi:10.3102/1076998610396893\nSmithson, M., and Segale, C. (2009). Partition Priming in Judgments of Imprecise Probabilities. Journal of Statistical Theory and Practice, 3(1), 169–181.\n\n\nlibrary(\"betareg\")\n\ndata(\"CarTask\", package = \"betareg\")\nlibrary(\"flexmix\")\ncar_betamix &lt;- betamix(probability ~ 1, data = CarTask, k = 3,\n  extra_components = list(extraComponent(type = \"uniform\", coef = 1/2,\n  delta = 0.01), extraComponent(type = \"uniform\", coef = 1/4, delta = 0.01)),\n  FLXconcomitant = FLXPmultinom(~ task))",
    "crumbs": [
      "Data sets",
      "CarTask"
    ]
  },
  {
    "objectID": "man/CarTask.html#partition-primed-probability-judgement-task-for-car-dealership",
    "href": "man/CarTask.html#partition-primed-probability-judgement-task-for-car-dealership",
    "title": "betareg",
    "section": "",
    "text": "In this study participants were asked to judge how likely it is that a customer trades in a coupe or that a customer buys a car form a specific salesperson out of four possible salespersons.\n\ndata(\"CarTask\", package = \"betareg\")\n\nA data frame with 155 observations on the following 3 variables.\n\n\ntask\n\n\na factor with levels Car and Salesperson indicating the condition.\n\n\nprobability\n\n\na numeric vector of the estimated probability.\n\n\nNFCCscale\n\n\na numeric vector of the NFCC scale.\n\n\nAll participants in the study were undergraduate students at The Australian National University, some of whom obtained course credit in first-year Psychology for their participation in the study.\nThe NFCC scale is a combined scale of the Need for Closure and Need for Certainty scales which are strongly correlated.\nFor task the questions were:\n\n\nCar\n\n\nWhat is the probability that a customer trades in a coupe?\n\n\nSalesperson\n\n\nWhat is the probability that a customer buys a car from Carlos?\n\n\nTaken from Smithson et al. (2011) supplements.\n\nSmithson, M., Merkle, E.C., and Verkuilen, J. (2011). Beta Regression Finite Mixture Models of Polarization and Priming. Journal of Educational and Behavioral Statistics, 36(6), 804–831. doi:10.3102/1076998610396893\nSmithson, M., and Segale, C. (2009). Partition Priming in Judgments of Imprecise Probabilities. Journal of Statistical Theory and Practice, 3(1), 169–181.\n\n\nlibrary(\"betareg\")\n\ndata(\"CarTask\", package = \"betareg\")\nlibrary(\"flexmix\")\ncar_betamix &lt;- betamix(probability ~ 1, data = CarTask, k = 3,\n  extra_components = list(extraComponent(type = \"uniform\", coef = 1/2,\n  delta = 0.01), extraComponent(type = \"uniform\", coef = 1/4, delta = 0.01)),\n  FLXconcomitant = FLXPmultinom(~ task))",
    "crumbs": [
      "Data sets",
      "CarTask"
    ]
  },
  {
    "objectID": "man/betamix.html",
    "href": "man/betamix.html",
    "title": "betareg",
    "section": "",
    "text": "Fit finite mixtures of beta regression models for rates and proportions via maximum likelihood with the EM algorithm using a parametrization with mean (depending through a link function on the covariates) and precision parameter (called phi).\n\nbetamix(formula, data, k, subset, na.action, weights, offset,\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\",\n    \"loglog\"), link.phi = \"log\",\n  control = betareg.control(...), cluster = NULL,\n  FLXconcomitant = NULL, FLXcontrol = list(), verbose = FALSE,\n  nstart = if (is.null(cluster)) 3 else 1, which = \"BIC\", \n  ID, fixed, extra_components, ...)\n\nextraComponent(type = c(\"uniform\", \"betareg\"), coef, delta,\n  link = \"logit\", link.phi = \"log\")\n\n\n\n\n\nformula\n\n\nsymbolic description of the model (of type y ~ x or y ~ x | z; for details see betareg).\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of integer case weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor for the mean.\n\n\n\n\nk\n\n\na vector of integers indicating the number of components of the finite mixture; passed in turn to the k argument of stepFlexmix.\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “identity”, “log”, “sqrt” are supported. The default is “log” unless formula is of type y ~ x where the default is “identity” (for backward compatibility). Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via betareg.control.\n\n\n\n\ncluster\n\n\nEither a matrix with k columns of initial cluster membership probabilities for each observation; or a factor or integer vector with the initial cluster assignments of observations at the start of the EM algorithm. Default is random assignment into k clusters.\n\n\n\n\nFLXconcomitant\n\n\nconcomitant variable model; object of class FLXP. Default is the object returned by calling FLXPconstant. The argument FLXconcomitant can be omitted if formula is a three-part formula of type y ~ x | z | w, where w specificies the concomitant variables.\n\n\n\n\nFLXcontrol\n\n\nobject of class “FLXcontrol” or a named list; controls the EM algorithm and passed in turn to the control argument of flexmix.\n\n\n\n\nverbose\n\n\na logical; if TRUE progress information is shown for different starts of the EM algorithm.\n\n\n\n\nnstart\n\n\nfor each value of k run stepFlexmix nstart times and keep only the solution with maximum likelihood.\n\n\n\n\nwhich\n\n\nnumber of model to get if k is a vector of integers longer than one. If character, interpreted as number of components or name of an information criterion.\n\n\n\n\nID\n\n\ngrouping variable indicating if observations are from the same individual, i.e. the component membership is restricted to be the same for these observations.\n\n\n\n\nfixed\n\n\nsymbolic description of the model for the parameters fixed over components (of type ~ x | z).\n\n\n\n\nextra_components\n\n\na list containing objects returned by extraComponent().\n\n\n\n\n…\n\n\narguments passed to betareg.control.\n\n\n\n\ntype\n\n\nspecifies if the component follows a uniform distribution or a beta regression model.\n\n\n\n\ncoef\n\n\na vector with the coefficients to determine the midpoint of the uniform distribution or names list with the coefficients for the mean and precision of the beta regression model.\n\n\n\n\ndelta\n\n\nnumeric; half-length of the interval of the uniform distribution.\n\n\n\nThe arguments and the model specification are similar to betareg. Internally stepFlexmix is called with suitable arguments to fit the finite mixture model with the EM algorithm. See Grün et al. (2012) for more details.\nextra_components is a list where each element corresponds to a component where the parameters are fixed a-priori.\n\nAn object of class “flexmix” containing the best model with respect to the log likelihood or the one selected according to which if k is a vector of integers longer than 1.\n\nBettina Grün and Achim Zeileis\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nGrün B, Leisch F (2008). FlexMix Version 2: Finite Mixtures with Concomitant Variables and Varying and Constant Parameters. Journal of Statistical Software, 28(4), 1–35. doi:10.18637/jss.v028.i04\nLeisch F (2004). FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R. Journal of Statistical Software, 11(8), 1–18. doi:10.18637/jss.v011.i08\n\nbetareg, flexmix, stepFlexmix\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\n## data with two groups of dyslexic and non-dyslexic children\ndata(\"ReadingSkills\", package = \"betareg\")\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(4040)\n## try to capture accuracy ~ iq relationship (without using dyslexia\n## information) using two beta regression components and one additional\n## extra component for a perfect reading score\nrs_mix &lt;- betamix(accuracy ~ iq, data = ReadingSkills, k = 3,\n  nstart = 10, extra_components = extraComponent(type = \"uniform\",\n  coef = 0.99, delta = 0.01))\n\n## visualize result\n## intensities based on posterior probabilities\nprob &lt;- 2 * (posterior(rs_mix)[cbind(1:nrow(ReadingSkills),\n   clusters(rs_mix))] - 0.5)\n## associated HCL colors\ncol0 &lt;- hcl(c(260, 0, 130), 65, 45, fixup = FALSE)\ncol1 &lt;- col0[clusters(rs_mix)]\ncol2 &lt;- hcl(c(260, 0, 130)[clusters(rs_mix)], 65 * abs(prob)^1.5,\n   95 - 50 * abs(prob)^1.5, fixup = FALSE)\n## scatter plot\nplot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19,\n   cex = 1.5, xlim = c(-2, 2))\npoints(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1,\n   col = col1)\n## fitted lines\niq &lt;- -30:30/10\ncf &lt;- rbind(coef(rs_mix, model = \"mean\", component = 1:2),\n   c(qlogis(0.99), 0))\nfor(i in 1:3)\n   lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2,\n         col = col0[i])\n\n\n\n\n\n\n## refit the model including a concomitant variable model using the\n## dyslexia information with some noise to avoid complete separation\n## between concomitant variable and component memberships\nset.seed(4040)\nw &lt;- rnorm(nrow(ReadingSkills), \n           c(-1, 1)[as.integer(ReadingSkills$dyslexia)])\n\n## The argument FLXconcomitant can be omitted when specifying\n## the model via a three part formula given by\n## accuracy ~ iq | 1 | w\n## The posteriors from the previously fitted model are used\n## for initialization.\nlibrary(\"flexmix\")\nrs_mix2 &lt;- betamix(accuracy ~ iq, data = ReadingSkills,\n  extra_components = extraComponent(type = \"uniform\",\n  coef = 0.99, delta = 0.01), cluster = posterior(rs_mix),\n  FLXconcomitant = FLXPmultinom(~w))\ncoef(rs_mix2, which = \"concomitant\")\n\n  (Intercept)       w\n1      0.0000  0.0000\n2      0.8114  1.0778\n3     -0.1195 -0.3488\n\nsummary(rs_mix2, which = \"concomitant\")\n\n$Comp.2\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    0.821      0.882    0.93     0.35  \nw              1.078      0.497    2.17     0.03 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.3\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -0.113      0.812   -0.14     0.89\nw             -0.346      0.452   -0.77     0.44",
    "crumbs": [
      "Beta regression extensions",
      "betamix"
    ]
  },
  {
    "objectID": "man/betamix.html#finite-mixtures-of-beta-regression-for-rates-and-proportions",
    "href": "man/betamix.html#finite-mixtures-of-beta-regression-for-rates-and-proportions",
    "title": "betareg",
    "section": "",
    "text": "Fit finite mixtures of beta regression models for rates and proportions via maximum likelihood with the EM algorithm using a parametrization with mean (depending through a link function on the covariates) and precision parameter (called phi).\n\nbetamix(formula, data, k, subset, na.action, weights, offset,\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\",\n    \"loglog\"), link.phi = \"log\",\n  control = betareg.control(...), cluster = NULL,\n  FLXconcomitant = NULL, FLXcontrol = list(), verbose = FALSE,\n  nstart = if (is.null(cluster)) 3 else 1, which = \"BIC\", \n  ID, fixed, extra_components, ...)\n\nextraComponent(type = c(\"uniform\", \"betareg\"), coef, delta,\n  link = \"logit\", link.phi = \"log\")\n\n\n\n\n\nformula\n\n\nsymbolic description of the model (of type y ~ x or y ~ x | z; for details see betareg).\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of integer case weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor for the mean.\n\n\n\n\nk\n\n\na vector of integers indicating the number of components of the finite mixture; passed in turn to the k argument of stepFlexmix.\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “identity”, “log”, “sqrt” are supported. The default is “log” unless formula is of type y ~ x where the default is “identity” (for backward compatibility). Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via betareg.control.\n\n\n\n\ncluster\n\n\nEither a matrix with k columns of initial cluster membership probabilities for each observation; or a factor or integer vector with the initial cluster assignments of observations at the start of the EM algorithm. Default is random assignment into k clusters.\n\n\n\n\nFLXconcomitant\n\n\nconcomitant variable model; object of class FLXP. Default is the object returned by calling FLXPconstant. The argument FLXconcomitant can be omitted if formula is a three-part formula of type y ~ x | z | w, where w specificies the concomitant variables.\n\n\n\n\nFLXcontrol\n\n\nobject of class “FLXcontrol” or a named list; controls the EM algorithm and passed in turn to the control argument of flexmix.\n\n\n\n\nverbose\n\n\na logical; if TRUE progress information is shown for different starts of the EM algorithm.\n\n\n\n\nnstart\n\n\nfor each value of k run stepFlexmix nstart times and keep only the solution with maximum likelihood.\n\n\n\n\nwhich\n\n\nnumber of model to get if k is a vector of integers longer than one. If character, interpreted as number of components or name of an information criterion.\n\n\n\n\nID\n\n\ngrouping variable indicating if observations are from the same individual, i.e. the component membership is restricted to be the same for these observations.\n\n\n\n\nfixed\n\n\nsymbolic description of the model for the parameters fixed over components (of type ~ x | z).\n\n\n\n\nextra_components\n\n\na list containing objects returned by extraComponent().\n\n\n\n\n…\n\n\narguments passed to betareg.control.\n\n\n\n\ntype\n\n\nspecifies if the component follows a uniform distribution or a beta regression model.\n\n\n\n\ncoef\n\n\na vector with the coefficients to determine the midpoint of the uniform distribution or names list with the coefficients for the mean and precision of the beta regression model.\n\n\n\n\ndelta\n\n\nnumeric; half-length of the interval of the uniform distribution.\n\n\n\nThe arguments and the model specification are similar to betareg. Internally stepFlexmix is called with suitable arguments to fit the finite mixture model with the EM algorithm. See Grün et al. (2012) for more details.\nextra_components is a list where each element corresponds to a component where the parameters are fixed a-priori.\n\nAn object of class “flexmix” containing the best model with respect to the log likelihood or the one selected according to which if k is a vector of integers longer than 1.\n\nBettina Grün and Achim Zeileis\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nGrün B, Leisch F (2008). FlexMix Version 2: Finite Mixtures with Concomitant Variables and Varying and Constant Parameters. Journal of Statistical Software, 28(4), 1–35. doi:10.18637/jss.v028.i04\nLeisch F (2004). FlexMix: A General Framework for Finite Mixture Models and Latent Class Regression in R. Journal of Statistical Software, 11(8), 1–18. doi:10.18637/jss.v011.i08\n\nbetareg, flexmix, stepFlexmix\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\n## data with two groups of dyslexic and non-dyslexic children\ndata(\"ReadingSkills\", package = \"betareg\")\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(4040)\n## try to capture accuracy ~ iq relationship (without using dyslexia\n## information) using two beta regression components and one additional\n## extra component for a perfect reading score\nrs_mix &lt;- betamix(accuracy ~ iq, data = ReadingSkills, k = 3,\n  nstart = 10, extra_components = extraComponent(type = \"uniform\",\n  coef = 0.99, delta = 0.01))\n\n## visualize result\n## intensities based on posterior probabilities\nprob &lt;- 2 * (posterior(rs_mix)[cbind(1:nrow(ReadingSkills),\n   clusters(rs_mix))] - 0.5)\n## associated HCL colors\ncol0 &lt;- hcl(c(260, 0, 130), 65, 45, fixup = FALSE)\ncol1 &lt;- col0[clusters(rs_mix)]\ncol2 &lt;- hcl(c(260, 0, 130)[clusters(rs_mix)], 65 * abs(prob)^1.5,\n   95 - 50 * abs(prob)^1.5, fixup = FALSE)\n## scatter plot\nplot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19,\n   cex = 1.5, xlim = c(-2, 2))\npoints(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1,\n   col = col1)\n## fitted lines\niq &lt;- -30:30/10\ncf &lt;- rbind(coef(rs_mix, model = \"mean\", component = 1:2),\n   c(qlogis(0.99), 0))\nfor(i in 1:3)\n   lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2,\n         col = col0[i])\n\n\n\n\n\n\n## refit the model including a concomitant variable model using the\n## dyslexia information with some noise to avoid complete separation\n## between concomitant variable and component memberships\nset.seed(4040)\nw &lt;- rnorm(nrow(ReadingSkills), \n           c(-1, 1)[as.integer(ReadingSkills$dyslexia)])\n\n## The argument FLXconcomitant can be omitted when specifying\n## the model via a three part formula given by\n## accuracy ~ iq | 1 | w\n## The posteriors from the previously fitted model are used\n## for initialization.\nlibrary(\"flexmix\")\nrs_mix2 &lt;- betamix(accuracy ~ iq, data = ReadingSkills,\n  extra_components = extraComponent(type = \"uniform\",\n  coef = 0.99, delta = 0.01), cluster = posterior(rs_mix),\n  FLXconcomitant = FLXPmultinom(~w))\ncoef(rs_mix2, which = \"concomitant\")\n\n  (Intercept)       w\n1      0.0000  0.0000\n2      0.8114  1.0778\n3     -0.1195 -0.3488\n\nsummary(rs_mix2, which = \"concomitant\")\n\n$Comp.2\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)    0.821      0.882    0.93     0.35  \nw              1.078      0.497    2.17     0.03 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.3\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -0.113      0.812   -0.14     0.89\nw             -0.346      0.452   -0.77     0.44",
    "crumbs": [
      "Beta regression extensions",
      "betamix"
    ]
  },
  {
    "objectID": "man/FoodExpenditure.html",
    "href": "man/FoodExpenditure.html",
    "title": "betareg",
    "section": "",
    "text": "Data on proportion of income spent on food for a random sample of 38 households in a large US city.\n\ndata(\"FoodExpenditure\", package = \"betareg\")\n\nA data frame containing 38 observations on 3 variables.\n\n\nfood\n\n\nhousehold expenditures for food.\n\n\nincome\n\n\nhousehold income.\n\n\npersons\n\n\nnumber of persons living in household.\n\n\nTaken from Griffiths et al. (1993, Table 15.4).\n\nCribari-Neto, F., and Zeileis, A. (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari, S.L.P., and Cribari-Neto, F. (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nGriffiths, W.E., Hill, R.C., and Judge, G.G. (1993). Learning and Practicing Econometrics New York: John Wiley and Sons.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\ndata(\"FoodExpenditure\", package = \"betareg\")\n\n## Ferrari and Cribari-Neto (2004)\n## Section 4\nfe_lin &lt;- lm(I(food/income) ~ income + persons, data = FoodExpenditure)\nlibrary(\"lmtest\")\nbptest(fe_lin)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fe_lin\nBP = 5.9348, df = 2, p-value = 0.05144\n\n## Table 2\nfe_beta &lt;- betareg(I(food/income) ~ income + persons, data = FoodExpenditure)\nsummary(fe_beta)\n\n\nCall:\nbetareg(formula = I(food/income) ~ income + persons, data = FoodExpenditure)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.5328 -0.4599  0.1698  0.6416  1.7733 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.622548   0.223854  -2.781 0.005418 ** \nincome      -0.012299   0.003036  -4.052 5.09e-05 ***\npersons      0.118462   0.035341   3.352 0.000802 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    35.61       8.08   4.407 1.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 45.33 on 4 Df\nPseudo R-squared: 0.3878\nNumber of iterations: 28 (BFGS) + 4 (Fisher scoring)",
    "crumbs": [
      "Data sets",
      "FoodExpenditure"
    ]
  },
  {
    "objectID": "man/FoodExpenditure.html#proportion-of-household-income-spent-on-food",
    "href": "man/FoodExpenditure.html#proportion-of-household-income-spent-on-food",
    "title": "betareg",
    "section": "",
    "text": "Data on proportion of income spent on food for a random sample of 38 households in a large US city.\n\ndata(\"FoodExpenditure\", package = \"betareg\")\n\nA data frame containing 38 observations on 3 variables.\n\n\nfood\n\n\nhousehold expenditures for food.\n\n\nincome\n\n\nhousehold income.\n\n\npersons\n\n\nnumber of persons living in household.\n\n\nTaken from Griffiths et al. (1993, Table 15.4).\n\nCribari-Neto, F., and Zeileis, A. (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari, S.L.P., and Cribari-Neto, F. (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nGriffiths, W.E., Hill, R.C., and Judge, G.G. (1993). Learning and Practicing Econometrics New York: John Wiley and Sons.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\ndata(\"FoodExpenditure\", package = \"betareg\")\n\n## Ferrari and Cribari-Neto (2004)\n## Section 4\nfe_lin &lt;- lm(I(food/income) ~ income + persons, data = FoodExpenditure)\nlibrary(\"lmtest\")\nbptest(fe_lin)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fe_lin\nBP = 5.9348, df = 2, p-value = 0.05144\n\n## Table 2\nfe_beta &lt;- betareg(I(food/income) ~ income + persons, data = FoodExpenditure)\nsummary(fe_beta)\n\n\nCall:\nbetareg(formula = I(food/income) ~ income + persons, data = FoodExpenditure)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.5328 -0.4599  0.1698  0.6416  1.7733 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.622548   0.223854  -2.781 0.005418 ** \nincome      -0.012299   0.003036  -4.052 5.09e-05 ***\npersons      0.118462   0.035341   3.352 0.000802 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    35.61       8.08   4.407 1.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 45.33 on 4 Df\nPseudo R-squared: 0.3878\nNumber of iterations: 28 (BFGS) + 4 (Fisher scoring)",
    "crumbs": [
      "Data sets",
      "FoodExpenditure"
    ]
  },
  {
    "objectID": "man/gleverage.html",
    "href": "man/gleverage.html",
    "title": "betareg",
    "section": "",
    "text": "Compute the generalized leverages values for fitted models.\n\ngleverage(model, ...)\n\n\n\n\n\nmodel\n\n\na model object.\n\n\n\n\n…\n\n\nfurther arguments passed to methods.\n\n\n\ngleverage is a new generic for computing generalized leverage values as suggested by Wei, Hu, and Fung (1998). Currently, there is only a method for betareg models, implementing the formulas from Rocha and Simas (2011) which are consistent with the formulas from Ferrari and Cribari-Neto (2004) for the fixed dispersion case.\nCurrently, the vector of generalized leverages requires computations and storage of order \\(n \\times n\\).\n\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nRocha AV, Simas AB (2011). Influence Diagnostics in a General Class of Beta Regression Models. Test, 20(1), 95–119. doi:10.1007/s11749-010-0189-z\nWei BC, Hu, YQ, Fung WK (1998). Generalized Leverage and Its Applications. Scandinavian Journal of Statistics, 25, 25–37.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\ndata(\"GasolineYield\", package = \"betareg\")\ngy &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\ngleverage(gy)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n0.2167 0.2517 0.3254 0.4542 0.2239 0.3201 0.5271 0.2819 0.3011 0.5066 0.1970 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.2146 0.3054 0.4397 0.2909 0.3514 0.4049 0.2448 0.3570 0.4840 0.2154 0.1835 \n    23     24     25     26     27     28     29     30     31     32 \n0.2899 0.4701 0.2910 0.2982 0.5449 0.3677 0.6603 0.3181 0.2557 0.4569",
    "crumbs": [
      "Beta regression",
      "gleverage"
    ]
  },
  {
    "objectID": "man/gleverage.html#generalized-leverage-values",
    "href": "man/gleverage.html#generalized-leverage-values",
    "title": "betareg",
    "section": "",
    "text": "Compute the generalized leverages values for fitted models.\n\ngleverage(model, ...)\n\n\n\n\n\nmodel\n\n\na model object.\n\n\n\n\n…\n\n\nfurther arguments passed to methods.\n\n\n\ngleverage is a new generic for computing generalized leverage values as suggested by Wei, Hu, and Fung (1998). Currently, there is only a method for betareg models, implementing the formulas from Rocha and Simas (2011) which are consistent with the formulas from Ferrari and Cribari-Neto (2004) for the fixed dispersion case.\nCurrently, the vector of generalized leverages requires computations and storage of order \\(n \\times n\\).\n\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nRocha AV, Simas AB (2011). Influence Diagnostics in a General Class of Beta Regression Models. Test, 20(1), 95–119. doi:10.1007/s11749-010-0189-z\nWei BC, Hu, YQ, Fung WK (1998). Generalized Leverage and Its Applications. Scandinavian Journal of Statistics, 25, 25–37.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\ndata(\"GasolineYield\", package = \"betareg\")\ngy &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\ngleverage(gy)\n\n     1      2      3      4      5      6      7      8      9     10     11 \n0.2167 0.2517 0.3254 0.4542 0.2239 0.3201 0.5271 0.2819 0.3011 0.5066 0.1970 \n    12     13     14     15     16     17     18     19     20     21     22 \n0.2146 0.3054 0.4397 0.2909 0.3514 0.4049 0.2448 0.3570 0.4840 0.2154 0.1835 \n    23     24     25     26     27     28     29     30     31     32 \n0.2899 0.4701 0.2910 0.2982 0.5449 0.3677 0.6603 0.3181 0.2557 0.4569",
    "crumbs": [
      "Beta regression",
      "gleverage"
    ]
  },
  {
    "objectID": "man/dxbetax.html",
    "href": "man/dxbetax.html",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the extended-support beta mixture distribution (in regression parameterization) on [0, 1].\n\n\n\ndxbetax(x, mu, phi, nu = 0, log = FALSE, quad = 20)\n\npxbetax(q, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE, quad = 20)\n\nqxbetax(p, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE, quad = 20,\n  tol = .Machine\\$double.eps^0.7)\n\nrxbetax(n, mu, phi, nu = 0)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Mean of the exponentially-distributed exceedence parameter for the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\nquad\n\n\nnumeric. The number of quadrature points for numeric integration of the continuous mixture. Alternatively, a matrix with nodes and weights for the quadrature points can be specified.\n\n\n\n\ntol\n\n\nnumeric. Accuracy (convergence tolerance) for numerically determining quantiles based on uniroot and pxbetax.\n\n\n\n\n\n\nThe extended-support beta mixture distribution is a continuous mixture of extended-support beta distributions on [0, 1] where the underlying exceedence parameter is exponentially distributed with mean nu. Thus, if nu &gt; 0, the resulting distribution has point masses on the boundaries 0 and 1 with larger values of nu leading to higher boundary probabilities. For nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\n\n\ndxbetax gives the density, pxbetax gives the distribution function, qxbetax gives the quantile function, and rxbetax generates random deviates.\n\n\n\ndxbeta, XBetaX",
    "crumbs": [
      "Distributions",
      "dxbetax"
    ]
  },
  {
    "objectID": "man/dxbetax.html#the-extended-support-beta-mixture-distribution",
    "href": "man/dxbetax.html#the-extended-support-beta-mixture-distribution",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the extended-support beta mixture distribution (in regression parameterization) on [0, 1].\n\n\n\ndxbetax(x, mu, phi, nu = 0, log = FALSE, quad = 20)\n\npxbetax(q, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE, quad = 20)\n\nqxbetax(p, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE, quad = 20,\n  tol = .Machine\\$double.eps^0.7)\n\nrxbetax(n, mu, phi, nu = 0)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Mean of the exponentially-distributed exceedence parameter for the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\nquad\n\n\nnumeric. The number of quadrature points for numeric integration of the continuous mixture. Alternatively, a matrix with nodes and weights for the quadrature points can be specified.\n\n\n\n\ntol\n\n\nnumeric. Accuracy (convergence tolerance) for numerically determining quantiles based on uniroot and pxbetax.\n\n\n\n\n\n\nThe extended-support beta mixture distribution is a continuous mixture of extended-support beta distributions on [0, 1] where the underlying exceedence parameter is exponentially distributed with mean nu. Thus, if nu &gt; 0, the resulting distribution has point masses on the boundaries 0 and 1 with larger values of nu leading to higher boundary probabilities. For nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\n\n\ndxbetax gives the density, pxbetax gives the distribution function, qxbetax gives the quantile function, and rxbetax generates random deviates.\n\n\n\ndxbeta, XBetaX",
    "crumbs": [
      "Distributions",
      "dxbetax"
    ]
  },
  {
    "objectID": "man/BetaR.html",
    "href": "man/BetaR.html",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for beta distributions in regression specification using the workflow from the distributions3 package.\n\nBetaR(mu, phi)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution.\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\nAlternative parameterization of the classic beta distribution in terms of its mean mu and precision parameter phi. Thus, the distribution provided by BetaR is equivalent to the Beta distribution with parameters alpha = mu * phi and beta = (1 - mu) * phi.\n\nA BetaR distribution object.\n\ndbetar, Beta\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- BetaR(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2)\n)\n\nX\n\n[1] \"BetaR(mu = 0.25, phi = 1)\" \"BetaR(mu = 0.50, phi = 1)\"\n[3] \"BetaR(mu = 0.75, phi = 2)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.25 0.50 0.75\n\nvariance(X)\n\n[1] 0.09375 0.12500 0.06250\n\nskewness(X)\n\n[1] -0.1666667 -1.5000000  0.0000000\n\nkurtosis(X)\n\n[1] -0.1666667 -1.5000000  0.0000000\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## simulate random variables\nrandom(X, 5)\n\n           r_1       r_2         r_3       r_4        r_5\n[1,] 0.7497152 0.8385523 0.031967958 0.9188288 0.54543668\n[2,] 0.1886452 0.9982310 0.004743561 0.1429292 0.07917131\n[3,] 0.9569594 0.9906614 0.757365599 0.9186620 0.84031674\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.6366198 1.1026578\n\npdf(X, x, log = TRUE)\n\n[1] -0.37966219 -0.45158271  0.09772344\n\nlog_pdf(X, x)\n\n[1] -0.37966219 -0.45158271  0.09772344\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.3910022\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 0.83680601\n\n## cdf() and quantile() are inverses (except at censoring points)\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n           q_0.05      q_0.5    q_0.95\n[1,] 9.512588e-06 0.09331223 0.9118445\n[2,] 6.155830e-03 0.50000000 0.9938442\n[3,] 2.285198e-01 0.83680601 0.9984571\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 9.984571e-01\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 9.984571e-01\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]        0.25 0.2464581\n[2,]        0.50 0.4941967\n[3,]        0.75 0.7542970",
    "crumbs": [
      "distributions3 objects",
      "BetaR"
    ]
  },
  {
    "objectID": "man/BetaR.html#create-a-beta-regression-distribution",
    "href": "man/BetaR.html#create-a-beta-regression-distribution",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for beta distributions in regression specification using the workflow from the distributions3 package.\n\nBetaR(mu, phi)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution.\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\nAlternative parameterization of the classic beta distribution in terms of its mean mu and precision parameter phi. Thus, the distribution provided by BetaR is equivalent to the Beta distribution with parameters alpha = mu * phi and beta = (1 - mu) * phi.\n\nA BetaR distribution object.\n\ndbetar, Beta\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- BetaR(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2)\n)\n\nX\n\n[1] \"BetaR(mu = 0.25, phi = 1)\" \"BetaR(mu = 0.50, phi = 1)\"\n[3] \"BetaR(mu = 0.75, phi = 2)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.25 0.50 0.75\n\nvariance(X)\n\n[1] 0.09375 0.12500 0.06250\n\nskewness(X)\n\n[1] -0.1666667 -1.5000000  0.0000000\n\nkurtosis(X)\n\n[1] -0.1666667 -1.5000000  0.0000000\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## simulate random variables\nrandom(X, 5)\n\n           r_1       r_2         r_3       r_4        r_5\n[1,] 0.7497152 0.8385523 0.031967958 0.9188288 0.54543668\n[2,] 0.1886452 0.9982310 0.004743561 0.1429292 0.07917131\n[3,] 0.9569594 0.9906614 0.757365599 0.9186620 0.84031674\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.6366198 1.1026578\n\npdf(X, x, log = TRUE)\n\n[1] -0.37966219 -0.45158271  0.09772344\n\nlog_pdf(X, x)\n\n[1] -0.37966219 -0.45158271  0.09772344\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.3910022\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 0.83680601\n\n## cdf() and quantile() are inverses (except at censoring points)\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n           q_0.05      q_0.5    q_0.95\n[1,] 9.512588e-06 0.09331223 0.9118445\n[2,] 6.155830e-03 0.50000000 0.9938442\n[3,] 2.285198e-01 0.83680601 0.9984571\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 9.984571e-01\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 9.984571e-01\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]        0.25 0.2464581\n[2,]        0.50 0.4941967\n[3,]        0.75 0.7542970",
    "crumbs": [
      "distributions3 objects",
      "BetaR"
    ]
  },
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite betareg in publications use:\n\nCribari-Neto F, Zeileis A (2010). “Beta Regression in R.” Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02.\n\nIf you use betatree(), betamix(), or bias correction/reduction in betareg(), please cite:\n\nGrün B, Kosmidis I, Zeileis A (2012). “Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned.” Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11.\n\nIf you use betareg() for response variables with boundary observations at 0 and/or 1, please cite:\n\nKosmidis I, Zeileis A (2024). “Extended-Support Beta Regression for [0, 1] Responses.” arXiv 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233."
  },
  {
    "objectID": "vignettes/betareg-ext.html",
    "href": "vignettes/betareg-ext.html",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "",
    "text": "Bettina Grün\nIoannis Kosmidis\nAchim Zeileis\n\n\nAbstract\nThis introduction to the extended features of the R package betareg is a (slightly) modified version of Grün, Kosmidis, and Zeileis (2012), published in the Journal of Statistical Software.\nBeta regression – an increasingly popular approach for modeling rates and proportions – is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. All three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. Using the analogy of Smithson and Verkuilen (2006), these extensions make beta regression not only “a better lemon squeezer” (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). All three extensions are provided in the R package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. Specifically, the new functions betatree() and betamix() reuse the object-oriented flexible implementation from the R packages partykit and flexmix, respectively."
  },
  {
    "objectID": "vignettes/betareg-ext.html#sec-intro",
    "href": "vignettes/betareg-ext.html#sec-intro",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "\n1 Introduction",
    "text": "1 Introduction\n\n1.1 A brief review of beta regression\nBeta regression is a model for continuous response variables \\(y\\) which assume values in the open unit interval \\((0, 1)\\). Such response variables may stem from rates, proportions, concentrations, etc. A regression model where the mean as well as the precision is modeled through covariates was introduced by Ferrari and Cribari-Neto (2004) along with the extensions by Smithson and Verkuilen (2006) and Simas, Barreto-Souza, and Rocha (2010). This model is also referred to as “double index regression model” because it contains two regression parts: one for the mean and one for the precision. Ferrari and Cribari-Neto (2004) employed an alternative parameterization of the beta distribution characterizing more easily the mean and the variance. In this parameterization the beta distribution has the density\n\\[\nf(y;\\mu,\\phi) =\n\\frac{\\Gamma(\\phi)}{\\Gamma(\\mu\\phi)\\Gamma((1-\\mu)\\phi)}y^{\\mu\\phi-1}(1-y)^{(1-\\mu)\\phi-1}\\,\n\\tag{1}\\]\nwhere \\(0&lt;y&lt;1\\), \\(0&lt;\\mu&lt;1\\), \\(\\phi &gt; 0\\), and \\(\\Gamma(\\cdot)\\) is the gamma function. A beta-distributed variable \\(Y\\) then has mean \\(\\text{E}(Y) = \\mu\\) and variance \\(\\text{Var}(Y) = \\mu(1-\\mu)/(1+\\phi)\\) so that \\(\\phi\\) can be seen as a precision parameter.\nThe double index beta regression model is specified in the following way. Given observations on \\(n\\) independent beta-distributed random variables \\(Y_i\\) (\\(i = 1, \\dots, n\\)), the corresponding parameters \\(\\mu_i\\) and \\(\\phi_i\\) are linked to linear predictors \\(\\eta_i\\) and \\(\\zeta_i\\) as follows\n\\[\n\\begin{align}\n  g_1(\\mu_i) & = \\eta_i =   x_i^\\top \\beta \\, ,  \\\\\n  g_2(\\phi_i) & = \\zeta_i =   z_i^\\top \\gamma\\, ,\n\\end{align}\n\\tag{2}\\]\nwhere \\(x_i\\) and \\(z_i\\) are \\(p\\)- and \\(q\\)-dimensional vectors of covariates observed along with \\(Y_i\\) \\((i = 1, \\ldots, n)\\), and \\(\\beta\n= (\\beta_1, \\ldots, \\beta_p)^\\top\\), \\(\\gamma = (\\gamma_1, \\ldots,\n\\gamma_q)^\\top\\) are the vectors of coefficients associated with the means and the precisions, respectively. The functions \\(g_1(\\cdot)\\) and \\(g_2(\\cdot)\\) are monotonic link functions, preferably with the property of mapping the range of \\(\\mu_i\\) \\((0, 1)\\) and \\(\\phi_i\\) \\((0,\n\\infty)\\), respectively, to the real line. Suitable candidates for \\(g_1(\\cdot)\\) are the logit, probit and generally any inverse of a cumulative distribution function, and for \\(g_2(\\cdot)\\) the log function. Another common choice for \\(g_2(\\cdot)\\) is the identity function which, however, can lead to invalid results when some \\(\\zeta_i &lt; 0\\).\nTypically, the coefficients \\(\\beta\\) and \\(\\gamma\\) are estimated by maximum likelihood (ML) and inference is based on the usual central limit theorem with its associated asymptotic tests, e.g., likelihood ratio, Wald, score/Lagrange multiplier (LM).\n\n1.2 Implementation in R\nThe R package betareg (Cribari-Neto and Zeileis 2010) provides ML estimation of beta regressions in its main model fitting function betareg(). The interface as well as the fitted model objects are designed to be similar to those from glm(). The model specification is via a formula plus data. Because two types of covariates need to be distinguished a two-part formula is allowed based on functionality provided by the Formula package (Zeileis and Croissant 2010). For example, would assign the covariates x1, x2, and x3 to the mean submodel and z1 and z2 to the precision submodel in Equation 2. Function betareg() internally uses function optim() as a general purpose optimizer to maximize the log-likelihood. The fitted model has methods for several extractor functions, e.g., coef(), vcov(), residuals(), logLik(). Base methods for the returned fitted model are summary(), AIC(), confint(). Further methods are available for functions from lmtest (Zeileis and Hothorn 2002) and car (Fox and Weisberg 2019), e.g., lrtest(), waldtest(), coeftest(), and linearHypothesis(). Multiple testing is possible via package multcomp (Hothorn, Bretz, and Westfall 2008) and structural change tests can be performed using package strucchange (Zeileis et al. 2002).\n\n1.3 Extensions\nAlthough the betareg package as published by Cribari-Neto and Zeileis (2010) provides a rather complete beta regression toolbox based on classical ML inference, further techniques may be required in practice. First, it has been shown that ML inference may be severely biased in the context of beta regression (Kosmidis and Firth 2010), possibly leading to overly optimistic inferences in the sense of underestimating the standard errors of the estimators. Second, it is not always easy to capture all heterogeneity in the data through the two linear predictors, especially when there are latent unobserved groups/clusters of observations.\nTo address the first issue of potentially biased inference, the results of Kosmidis and Firth (2010) are extended to the case with mean and precision covariates and the corresponding methods are implemented in the betareg() function starting from version 2.4-0 of betareg. The software optionally allows for bias-corrected or bias-reduced estimation by adopting the unifying iteration developed in Kosmidis and Firth (2010).\nTo address the second issue of heterogeneity between groups/clusters of observations, two generic strategies, model-based recursive partitioning (Zeileis, Hothorn, and Hornik 2008) and finite mixture models , are applied to beta regressions. The idea for both techniques is to capture situations in which the regression relationships vary across groups in the population. If one can identify variables which are related to such groups, one may be able to include them directly in the regression relationships. However, (a) this may lead to rather complex and hard to interpret models, and (b) unnecessary complexity is introduced if the differences are only present in a subset of the combined groups induced by several variables. Model-based recursive partitioning avoids such drawbacks. Furthermore, if groups cannot be directly related to observed variables, the heterogeneity can be accounted for by using finite mixture models. Therefore, extensions of the betareg package are introduced where model heterogeneity is taken into account when covariates that characterize the groups are available, and when the heterogeneity is due to latent variables. The new function betatree() provides model-based recursive partitioning of beta regressions leveraging tools from the partykit package (Hothorn and Zeileis 2015), and the function betamix() provides beta regression mixture models (or latent class beta regression) reusing the generic functionality from the flexmix package (Leisch and Grün 2023)."
  },
  {
    "objectID": "vignettes/betareg-ext.html#sec-bias",
    "href": "vignettes/betareg-ext.html#sec-bias",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "\n2 Bias correction and reduction in beta regressions",
    "text": "2 Bias correction and reduction in beta regressions\n\n2.1 Preamble\nKosmidis and Firth (2010) show that bias correction (BC) or bias reduction (BR) of the ML estimator in parametric models may be achieved via a unifying quasi Fisher scoring algorithm. They illustrate the applicability of their algorithm in a beta regression setting with a common precision parameter \\(\\phi\\) for all subjects, also revealing some errors in previous literature for the reduction of bias in beta regression models – specifically mistakes in Ospina, Cribari-Neto, and Vasconcellos (2006) and Simas, Barreto-Souza, and Rocha (2010) — that led to misleading negative conclusions about the effect of BC/BR on inferences for beta regression models. In Kosmidis and Firth (2010), it is shown that BC/BR for beta regression models can be desirable because the ML estimator of \\(\\phi\\) may demonstrate substantial upward bias, which in turn may lead to underestimation of asymptotic standard errors and hence over-optimistic Wald-type inferences (e.g., confidence intervals with coverage far below the nominal levels).\nThe results in Kosmidis and Firth (2010) are extended here to cover not only the case of constant \\(\\phi\\) but also a regression part for the precision parameters as shown in Equation 2.\n\n2.2 Generic framework\nDenote by \\(0_{k}\\) a vector of \\(k\\) zeros and by \\(S(\\theta)\\) the vector of the log-likelihood derivatives for a parametric model with parameter \\(\\theta\\). Firth (1993) showed that the solution \\(\\tilde\\theta\\) of the equation\n\\[\nS(\\tilde{\\theta}) + A(\\tilde{\\theta}) = 0_{p+q} \\, ,\n\\tag{3}\\]\nhas smaller asymptotic bias than the ML estimator, if the \\(t\\)-th component of the vector \\(A(\\theta)\\) has the form\n\\[\nA_t(\\theta) = \\frac{1}{2}\\text{trace}\\left[\\{F(\\theta)\\}^{-1} \\left\\{\n    P_t(\\theta) + Q_t(\\theta) \\right\\}\\right] \\quad (t = 1, \\ldots,\np + q) \\, ,\n\\]\nwith \\(F(\\theta)\\) the expected information matrix and\n\\[\n\\begin{align}\n  P_t(\\theta) & =  \\text{E}\\{S(\\theta)S^\\top(\\theta)S_t(\\theta)\\}\n  \\quad (t = 1, \\ldots, p + q)\\, , \\\\\n  Q_t(\\theta) & = -\\text{E}\\left\\{I(\\theta)S_t(\\theta) \\right\\} \\quad (t =\n  1, \\ldots, p + q)\\, ,\n\\end{align}\n\\tag{4}\\]\nwhere \\(S_t(\\theta)\\) denotes the \\(t\\)-th component of \\(S(\\theta)\\) \\((t =\n1, \\ldots, p + q)\\) and \\(I(\\theta)\\) is the observed information matrix (minus the matrix of second derivatives of the log-likelihood with respect to \\(\\theta\\)).\nThe quasi Fisher scoring iteration that has been developed in Kosmidis and Firth (2010) attempts to solve Equation 3. Specifically, at the \\(j\\)-th step of the iterative procedure, the current value \\(\\theta^{(j)}\\) of the parameter vector is updated to \\(\\theta^{(j+1)}\\) by\n\\[\n\\theta^{(j+1)} = \\theta^{(j)} + \\left\\{F\\left(\\theta^{(j)}\\right)\\right\\}^{-1}\nS\\left(\\theta^{(j)}\\right) - b\\left(\\theta^{(j)}\\right) \\, ,\n\\tag{5}\\]\nwhere \\(b(\\theta) = - \\{F(\\theta)\\}^{-1} A(\\theta)\\) is the vector of the first term in the expansion of the bias of the ML estimator.\nIf the summand \\(b\\left(\\theta^{(j)}\\right)\\) is ignored, then iteration Equation 5 becomes the usual Fisher scoring iteration that can be used to solve the ML score equations \\(S(\\hat\\theta) = 0_{p+q}\\).\nFurthermore, if the starting value \\(\\theta^{(0)}\\) is the ML estimator \\(\\hat\\theta\\), then \\(\\theta^{(1)}\\) is the bias-corrected estimator \\(\\theta^\\dagger\\) of \\(\\theta\\) defined as\n\\[\n\\theta^\\dagger = \\hat\\theta - b(\\hat\\theta) \\, ,\n\\]\nwhich also has smaller asymptotic bias compared to the ML estimator (Efron 1975).\nHence, the quasi Fisher scoring iteration provides a unified framework for implementing all three types of estimators – ML, BR, and BC – by merely deciding whether the summand \\(b\\left(\\theta^{(j)}\\right)\\) is absent or present in the right hand side of Equation 5, and whether more than one iteration should be allowed in the latter case.\n\n2.3 Bias correction and bias reduction for beta regressions\nDenote the vector of the \\(p + q\\) model parameters in a beta regression model by \\(\\theta = (\\beta^\\top, \\gamma^\\top)^\\top\\), and let \\(X\\) and \\(Z\\) be the \\(n\\times p\\) and \\(n\\times q\\) model matrices with \\(i\\)-th row \\(x_i\\) and \\(z_i\\), respectively \\((i = 1, \\ldots, n)\\). The ingredients required for setting the iteration described in Section 2.2 are closed-form expressions for the vector of log-likelihood derivatives \\(S(\\theta)\\), the expected information matrix \\(F(\\theta)\\) and the two higher-order joint null cumulants of log-likelihood derivatives \\(P_t(\\theta)\\) and \\(Q_t(\\theta)\\) shown in Equation 4. Based on these, all matrix multiplications and inversions can be performed numerically during the iterative procedure.\nThe fact that all the aforementioned quantities depend on \\(X\\) and \\(Z\\), and that \\(S(\\theta)\\) and \\(I(\\theta)\\) depend additionally on the random variables \\(Y_i\\) \\((i = 1, \\ldots, n)\\) has been concealed here merely for notational simplicity. The same convention is used for the derivations below, additionally concealing the dependence on \\(\\theta\\) unless otherwise stated.\nUp to an additive constant the log-likelihood for the beta regression model in Equation 1 is \\(\\ell(\\theta) = \\sum_{i =1}^n \\ell_i(\\theta)\\) with\n\\[\n\\ell_i(\\theta) = \\phi_i\\mu_i\n  (T_i - U_i) + \\phi_i U_i +\n  \\log\\Gamma(\\phi_i) -\n  \\log\\Gamma(\\phi_i\\mu_i) -\\log\\Gamma(\\phi_i(1-\\mu_i))\n\\tag{6}\\]\nwhere \\(\\mu_i\\) and \\(\\phi_i\\) are defined by inverting the functions in Equation 2, and where \\(T_i = \\log Y_i\\) and \\(U_i = \\log(1 - Y_i)\\) are the sufficient statistics for the beta distribution with natural parameters \\(\\phi_i\\mu_i\\) and \\(\\phi_i(1 -\n\\mu_i)\\) \\((i =1, \\ldots, n)\\), respectively.\nDirect differentiation of the log-likelihood function reveals that the vector of log-likelihood derivatives has the form\n\\[\nS(\\theta) = \\nabla_\\theta \\ell(\\theta) = \\left[\n\\begin{array}{c}\n  X^\\top \\Phi D_1 \\left(\\bar{T} - \\bar{U}\\right) \\\\\n  Z^\\top D_2 \\left\\{ M\\left(\\bar{T} - \\bar{U}\\right) + \\bar{U}  \\right\\}\n\\end{array}\n\\right]\\, ,\n\\tag{7}\\]\nwith \\(\\Phi = \\text{diag}\\{\\phi_1, \\ldots, \\phi_n\\}\\), \\(M\n= \\text{diag}\\{\\mu_1, \\ldots, \\mu_n\\}\\), \\(D_1 = \\text{diag}\\{d_{1,1}, \\ldots, d_{1,\n  n}\\}\\), and \\(D_2 = \\text{diag}\\{d_{2,1}, \\ldots, d_{2, n}\\}\\), where \\(d_{1,\n  i} = \\partial{\\mu_i}/\\partial{\\eta_i}\\) and \\(d_{2,i} =\n\\partial{\\phi_i}/\\partial{\\zeta_i}\\). Furthermore, \\(\\bar{T} = (\\bar{T}_1, \\ldots, \\bar{T}_n)^\\top\\) and \\(\\bar{U} =\n(\\bar{U}_1, \\ldots, \\bar{U}_n)^\\top\\) are the vectors of centered sufficient statistics, with\n\\[\n\\begin{align*}\n\\bar{T}_i & = T_i - \\text{E}(T_i) \\, , \\\\\n\\bar{U}_i & = U_i - \\text{E}(U_i) \\, ,\n\\end{align*}\n\\]\nwhere \\(\\text{E}(T_i) = \\psi^{(0)}(\\phi\\mu_i) - \\psi^{(0)}(\\phi_i)\\) and \\(\\text{E}(U_i) = \\psi^{(0)}(\\phi(1-\\mu_i)) + \\psi^{(0)}(\\phi_i)\\), with \\(\\psi^{(r)}(k) = \\partial{^{r+1} \\log\\Gamma(k)}/\\partial{k^{r+1}}\\) the polygamma function of degree \\(r\\) \\((r = 0, 1, \\ldots; i =1, \\ldots,\nn)\\).\nDifferentiating \\(\\ell(\\theta)\\) one more time reveals that the observed information on \\(\\theta\\) is\n\\[\nI(\\theta) = F(\\theta) - \\left[\n\\begin{array}{cc}\n  X^\\top \\Phi D_1' \\text{diag}\\{\\bar{T} - \\bar{U}\\}X & X^\\top D_1\\text{diag}\\{\\bar{T} - \\bar{U}\\}D_2Z \\\\\n  Z^\\top D_2\\text{diag}\\{\\bar{T} - \\bar{U}\\}D_1X & Z^\\top D_2'\\left(M\\text{diag}\\left\\{\\bar{T} -\n      \\bar{U}\\} + \\text{diag}\\{\\bar{U}\\right\\}\\right)Z \\\\\n\\end{array}\n\\right] \\, ,\n\\tag{8}\\]\nwhere\n\\[\nF(\\theta) = \\left[\n\\begin{array}{cc}\n  X^\\top D_1\\Phi K_2 \\Phi D_1 X & X^\\top D_1\\Phi \\left(MK_2 - \\Psi_1\\right)D_2Z \\\\\n  Z^\\top D_2 \\left(MK_2 - \\Psi_1\\right)\\Phi D_1  X &\n  Z^\\top D_2\\left\\{M^2K_2 + (1_n-2M)\\Psi_1 - \\Omega_1\\right\\}D_2Z\n\\end{array}\n\\right]\\, ,\n\\tag{9}\\]\nis the expected information on \\(\\theta\\), because the second summand in the right hand side of Equation 8 depends linearly on the centered sufficient statistics and hence has expectation zero. Here, \\(1_n\\) is the \\(n \\times n\\) identity matrix, \\(D_1' = \\text{diag}\\{d'_{1,1},\n\\ldots, d'_{1,n}\\}\\) with \\(d'_{1,i} = \\partial{^2\\mu_i}/\\partial{\\eta_i^2}\\) and \\(D_2' = \\text{diag}\\{d'_{2,1}, \\ldots, d'_{2,n}\\}\\) with \\(d'_{2,i} =\n\\partial{^2\\phi_i}/\\partial{\\zeta_i^2}\\) \\((i = 1, \\ldots, n)\\). Furthermore, \\(K_2 = \\text{diag}\\{\\kappa_{2,1}, \\ldots, \\kappa_{2,n}\\}\\), where \\(\\kappa_{2,i} = \\text{Var}\\left(\\bar{T}_i -\\bar{U}_i\\right) =\n\\psi^{(1)}(\\phi_i\\mu_i) + \\psi^{(1)}(\\phi_i(1-\\mu_i))\\) for \\(i =1,\n\\ldots, n\\) and\n\\[\n\\begin{align*}\n  \\Psi_r & = \\text{diag}\\left\\{\\psi^{(r)}(\\phi_1(1-\\mu_1)), \\ldots,\n  \\psi^{(r)}(\\phi_n(1-\\mu_n))\\right\\} \\, ,\\\\\n  \\Omega_r & = \\text{diag}\\left\\{\\psi^{(r)}(\\phi_1), \\ldots,\n  \\psi^{(r)}(\\phi_n)\\right\\}\\quad (r = 0, 1, \\ldots)\\, .\n\\end{align*}\n\\]\nSome tedious but straightforward algebra, along with direct use of the results in Kosmidis and Firth (2010) for the joint cumulants of \\(\\bar{T}_i\\) and \\(\\bar{U}_i\\) \\((i = 1, \\ldots, n)\\), gives\n\\[\n  P_t(\\theta) + Q_t(\\theta) = \\left[\n    \\begin{array}{cc}\n      V_{\\beta\\beta, t} & V_{\\beta\\gamma, t} \\\\\n      V_{\\beta\\gamma, t}^\\top & V_{\\gamma\\gamma, t}\n    \\end{array}\n  \\right] \\quad (t = 1, \\ldots, p)\\, ,\n\\tag{10}\\]\n\\[\n  P_{p + s}(\\theta) + Q_{p + s}(\\theta)  = \\left[\n    \\begin{array}{cc}\n      W_{\\beta\\beta, s} & W_{\\beta\\gamma, s} \\\\\n      W_{\\beta\\gamma, s}^\\top & W_{\\gamma\\gamma, s}\n    \\end{array}\n  \\right] \\quad (s = 1, \\ldots, q) \\, ,\n\\tag{11}\\]\nwhere\n\\[\n\\begin{align*}\n  V_{\\beta\\beta, t} & = X^\\top \\Phi^2 D_1 \\left( \\Phi D_1^2 K_3 + D_1'\n    K_2 \\right)X_t^\\text{D} X \\, , \\\\\n  V_{\\beta\\gamma, t} & = X^\\top\\Phi D_1^2 D_2 \\left\\{\\Phi\\left(MK_3 + \\Psi_2\\right) +\n  K_2\\right)X_t^\\text{D} Z \\, , \\\\\n  V_{\\gamma\\gamma, t} & =\n  Z^\\top\\Phi D_1 \\left\\{ D_2^2\\left(M^2 K_3 + 2M\\Psi_2 -\n      \\Psi_2\\right) + D_2'\\left(MK_2 - \\Psi_1\\right)\\right\\}X_t^\\text{D} Z\n\\end{align*}\n\\]\nand\n\\[\n\\begin{align*}\n  W_{\\beta\\beta, s} & =\n  X^\\top \\Phi D_2 \\left\\{\\Phi D_1^2 \\left(M K_3 + \\Psi_2\\right) +\n    D_1'\\left(MK_2 - \\Psi_1\\right) \\right\\}Z_s^\\text{D} X\\, , \\\\\n  W_{\\beta\\gamma, s} & = X^\\top D_1 D_2^2 \\left\\{ \\Phi \\left(M^2 K_3 + 2M\\Psi_2\n      - \\Psi_2\\right) + MK_2 - \\Psi_1 \\right\\}Z_s^\\text{D} Z\\, , \\\\\n  W_{\\gamma\\gamma, s} & =\n  Z^\\top D_2^3 \\left\\{M^3K_3 + \\left( 3M^2 - 3M + 1_n \\right) \\Psi_2 -\n  \\Omega_2 \\right\\} Z_s^\\text{D} Z  \\\\\n& \\qquad + Z^\\top D_2 D_2' \\left\\{M^2K_2 + \\Psi_1 - 2M \\Psi_1 -\n  \\Omega_1 \\right\\}Z_s^\\text{D} Z\\, ,\n\\end{align*}\n\\]\nwhere \\(K_3 = \\text{diag}\\left\\{\\kappa_{3,1}, \\ldots, \\kappa_{3,n} \\right\\}\\), with \\(\\kappa_{3,i} = \\text{E}\\left\\{\\left(\\bar{T}_i -\n    \\bar{U}_i\\right)^3\\right\\} = \\psi^{(2)}(\\phi_i\\mu_i) -\n\\psi^{(2)}(\\phi_i(1 - \\mu_i))\\) \\((i =1, \\ldots, n)\\). Furthermore, \\(C_t^\\text{D}\\) denotes the diagonal matrix with non-zero components the elements of the \\(t\\)-th column of a matrix \\(C\\).\n\n2.4 Implementation in betareg\n\nSupport for both bias correction and bias reduction has been added in the principal model fitting function betareg() starting from betareg 2.4-0. The interface of betareg() is essentially the same as described in Cribari-Neto and Zeileis (2010), with merely the addition of a type argument that specifies the type of estimator that should be used.\n\nbetareg(formula, data, subset, na.action, weights, offset,\n  link = \"logit\", link.phi = NULL, type = c(\"ML\", \"BC\", \"BR\"),\n  control = betareg.control(...), model = TRUE, y = TRUE, x = FALSE, ...)\n\nThe arguments in the first line (formula, data, ) pertain to the data and model specification using a formula that potentially may have two parts pertaining to the mean and the precision submodels, respectively. The arguments link and link.phi specify the link functions \\(g_1(\\cdot)\\) and \\(g_2(\\cdot)\\), respectively. The argument type controls which of the maximum likelihood (type = \"ML\"), bias-corrected (type = \"BC\"), or bias-reduced (type = \"BR\") estimates are computed. Finally, control is a list of control arguments and model, y, and x control whether the respective data components are included in the fitted model object. For more details on all arguments except type see Cribari-Neto and Zeileis (2010).\nWhile the interface of betareg() is almost the same as in previous versions, the internal code has been substantially enhanced. Specifically, the optimization via optim() is now (optionally) enhanced by an additional Fisher scoring iteration. As in previous versions, the initial optimization of the likelihood is carried out via optim(), by default with method = \"BFGS\", using analytic gradients. In recent versions, this is followed by a Fisher scoring iteration with both analytic gradients and expected information that either neglects or includes the summand \\(b\\left(\\theta^{(j)}\\right)\\) in iteration Equation 5. Thus, the iteration is either used to further improve the numerical maximization of the likelihood (for type = \"ML\" or ) or to carry out the bias reduction (for type = \"BR\") as detailed in Section 2.2. To control the details of the (quasi) Fisher scoring, betareg.control() takes two additional arguments fsmaxit = 200 and fstol = 1e-8 controlling the maximal number of iterations and convergence tolerance, respectively. If the number of iterations is set to zero (fsmaxit = 0), no Fisher scoring is carried out (allowed only for type = \"ML\" and \"BC\") and thus results from previous versions of betareg can be exactly replicated."
  },
  {
    "objectID": "vignettes/betareg-ext.html#sec-tree",
    "href": "vignettes/betareg-ext.html#sec-tree",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "\n3 Beta regression trees",
    "text": "3 Beta regression trees\nModel-based recursive partitioning builds on the more widely known method of classification and regression trees . As for CART, the idea is to split the sample recursively with respect to available variables (called “partitioning” variables in what follows) in order to capture differences in the response variable. While CART tries to capture differences in the distribution of the response variable (in particular with respect to location) directly, the aim of model-based recursive partitioning is more broadly to capture differences in parameters describing the distribution of the response. In particular, model-based recursive partitioning allows to incorporate regressor variables in a parametric model for the response variable.\nHere, we adapt the general MOB framework to the model-based partitioning of beta regressions, called “beta regression trees” for short. The aim is to capture differences in the distribution that are not yet adequately described by the regressor variables through a forward search. Basically, the approach proceeds by (a) fitting a beta regression model, (b) assessing whether its parameters are stable across all partitioning variables, (c) splitting the sample along the partitioning variable associated with the highest parameter instability, (d) repeating these steps until some stopping criterion is met. Thus, interactions and nonlinearities can be incorporated by locally maximizing the likelihood of a partitioned model. More precisely and denoting \\(c_{ij}\\) the \\(j\\)-th partitioning variable (\\(j = 1, \\dots, l\\)) for observation \\(i\\), the steps of the MOB algorithm adapted to beta regression are as follows.\n\nFit a beta regression model with parameters \\(\\beta\\) and \\(\\gamma\\) by maximizing the log-likelihood for all observations \\(y_i\\) in the current sample.\nAssess whether the parameters \\(\\beta\\) and \\(\\gamma\\) are stable across each partitioning variable \\(c_{ij}\\).\nIf there is significant parameter instability with respect to at least one of the partitioning variables \\(c_{ij}\\), split the sample along the variable \\(j*\\) with the strongest association: Choose the breakpoint with highest improvement in the fitted log-likelihood.\nRepeat steps 1–3 recursively in the resulting subsamples until there is no significant instability any more or the sample size is too small.\n\nhe MOB framework of Zeileis, Hothorn, and Hornik (2008) is generic in that it requires only the specification of a model with additive objective function for which a central limit theorem holds. Under the usual regularity conditions, the latter requirement is valid for beta regressions. The main building blocks that the MOB algorithm requires are the contributions to the additive objective function (in steps 1 and 3) and to the associated score function (in step 2). For beta regressions, the objective is the log-likelihood \\(\\ell(\\theta)\\) and its contributions \\(\\ell_i(\\theta)\\) are given in Equation 6. By Equation 7 and using the notation in Section 2, the corresponding score (or gradient) contributions have the form\n\\[\n  S_{i}(\\theta) = \\left[\n    \\begin{array}{c}\n      \\mu_i \\phi_i d_{1,i} \\left(\\bar{T}_i -\n  \\bar{U}_i\\right)x_{i1} \\\\\n\\vdots \\\\\n      \\mu_i \\phi_i d_{1,i} \\left(\\bar{T}_i -\n  \\bar{U}_i\\right)x_{ip}  \\\\\nd_{2,i}\\left\\{ \\mu_i\\left(\\bar{T}_i -\n  \\bar{U}_i\\right) + \\bar{U}_i \\right\\}z_{i1} \\\\\n\\vdots \\\\\nd_{2,i}\\left\\{ \\mu_i\\left(\\bar{T}_i -\n  \\bar{U}_i\\right) + \\bar{U}_i \\right\\}z_{iq} \\\\\n    \\end{array}\n    \\right]\\quad (i = 1, \\ldots, n) \\, .\n\\]\nThe above contributions are employed for testing whether there are significant departures from zero across the partitioning variables. More specifically, MOB uses generalized M-fluctuation tests for parameter instability (Zeileis 2006, betareg:Zeileis+Hornik:2007): fluctuations in numeric variables are assessed with a \\(\\sup\\)LM type test (Andrews 1993) and fluctuations in categorical variables are assessed with a \\(\\chi^2\\)-type test (Hjort and Koning 2002). For further details and references, see Zeileis, Hothorn, and Hornik (2008).1\nBeta regression trees are implemented in the betareg package in function betatree() taking the following arguments:\n\nbetatree(formula, partition, data, subset, na.action, weights, offset,\n  link = \"logit\", link.phi = \"log\", control = betareg.control(), ...)\n\nEssentially, almost all arguments work as for the basic betareg() function. The main difference is that a partition formula (without left hand side), such as ~ c1 + c2 + c3 has to be provided to specify the vector of partitioning variables \\(c_i = (c_{i1}, \\dots, c_{il})^\\top\\). As an alternative, partition may be omitted when formula has three parts on the right hand side, such as y ~ x1 + x2 | z1 | c1 + c2 + c3, specifying mean regressors \\(x_i\\), presicion regressors \\(z_i\\), and partitioning variables \\(c_i\\), respectively. The formula y ~ c1 + c2 + c3 is short for y ~ 1 | 1 | c1 + c2 + c3.\nThe betatree() function takes all arguments and carries out all data preprocessing and then calls the function mob() from the partykit package (Hothorn and Zeileis 2015). The latter can perform all steps of the MOB algorithm in an object-oriented manner, provided that a suitable model fitting function (optimizing the log-likelihood) is specified and that extractor functions are available for the optimized log-likelihood Equation 6 and the score function Equation 7 at the estimated parameters. For model fitting betareg.fit() is employed and for extractions the logLik() and estfun() methods are leveraged. To control the details of the MOB algorithm – such as the significance level and the minimal subsample size in step 4 – the ... argument is passed to mob(). (Note that this is somewhat different from betareg() where ... is passed to betareg.control().)"
  },
  {
    "objectID": "vignettes/betareg-ext.html#sec-mix",
    "href": "vignettes/betareg-ext.html#sec-mix",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "\n4 Finite mixtures of beta regressions",
    "text": "4 Finite mixtures of beta regressions\nFinite mixtures are suitable models if the data is assumed to be from different groups, but the group memberships are not observed. If mixture models are fitted one aims at determining the parameters of each group as well as the group sizes. Furthermore, the model can be used to estimate from which group each observation is. In the case of finite mixtures of beta regression models the latent groups can be assumed to differ in their mean and/or in their precision. Furthermore, the group sizes can depend on further covariates.\nThe mixture model with \\(K\\) components which correspond to \\(K\\) groups is given by\n\\[\n\\begin{align}\n  h(y ; x, z, c, \\theta) & = \\sum_{k = 1}^K \\pi(k ; c, \\alpha) f(y ;\n  g_1^{-1}(x^{\\top}\\beta_k), g_2^{-1}(z^{\\top}\\gamma_k)),\n\\end{align}\n\\]\nwhere \\(h(\\cdot ; \\cdot)\\) is the mixture density and \\(f(y ; \\mu, \\phi)\\) is the density of the beta distribution using the mean-precision parameterization shown in Equation 1. Furthermore the component weights \\(\\pi(k ; \\cdot)\\) are nonnegative for all \\(k\\) and sum to one. In what follows the component weights are assumed to be determined from a vector of covariates \\(c\\) by\n\\[\n\\begin{align}\n  \\pi(k ; c, \\alpha) &= \\frac{\\textrm{exp}\\{c^{\\top}\\alpha_k\\}}\n  {\\sum_{u=1}^K\\textrm{exp}\\{c^{\\top}\\alpha_u\\}}\n\\end{align}\n\\]\nwith \\(\\alpha_1 \\equiv 0\\). Without covariates and just a constant (\\(c = 1\\)), this reduces to prior probabilities that are fixed across all observations.\nSmithson and Segale (2009) and Smithson, Merkle, and Verkuilen (2011) consider finite mixtures of beta regression models to analyze priming effects in judgments of imprecise probabilities. Smithson and Segale (2009) fit mixture models where they investigate if priming has an effect on the size of the latent groups, i.e., they include the information on priming as a predictor variable \\(c\\). Smithson, Merkle, and Verkuilen (2011) assume that for at least one component distribution the location parameter is a-priori known due to so-called “anchors”. For example, for partition priming, an anchor would be assumed at location \\(1/K\\) if the respondents are primed to believe that there are \\(K\\) possible events. The component distribution for this anchor can be either assumed to follow a beta distribution with known parameters for the mean and the precision or a uniform distribution with known support.\nPackage flexmix (Leisch 2004; Grün and Leisch 2008) implements a general framework for estimating finite mixture models using the EM algorithm. The EM algorithm is an iterative method for ML estimation in a missing data setting. The missing data for mixture models is the information to which component an observation belongs. The EM algorithm exploits the fact that the complete-data log-likelihood for the data and the missing information is easier to maximize. In general for mixture models the posterior probabilities of an observation to be from each component given the current parameter estimates are determined in the E-step. The M-step then consists of maximizing the complete-data log-likelihood where the missing component memberships are replaced by the current posterior probabilities. This implies that different mixture models only require the implementation of a suitable M-step driver. Function betareg.fit() provides functionality for weighted ML estimation of beta regression models and hence allows the easy implementation of the M-step.\nThe function betamix() allows to fit finite mixtures of beta regression models using the package betareg. It has the following arguments:\n\nbetamix(formula, data, k, fixed, subset, na.action,\n  link = \"logit\", link.phi = \"log\", control = betareg.control(...),\n  FLXconcomitant = NULL, extra_components,\n  verbose = FALSE, ID, nstart = 3, FLXcontrol = list(), cluster = NULL,\n  which = \"BIC\", ...)\n\n\nArguments formula, data, subset, na.action, link, link.phi and control are the same as for betareg().\n\nAdditionally the formula can also consist of three parts on the right hand side when specifying a concomitant variable model (see below for the FLXconcomitant argument).\n\nArguments cluster, FLXconcomitant and FLXcontrol are the same as for function flexmix() (in the latter two cases without prefix FLX).\n\nCurrently functionality to fit a multinomal logit model for the concomitant variable model is provided by FLXPmultinom() with a formula interface for specifying the concomitant variables. To fit a multinomial logit model for the variables c1 and c2 use . Alternatively, yielding equivalent output, the main model formula can be specified via a three-part formula on the right hand side, e.g., y ~ x | 1 | c1 + c2 (if there are no covariates for the precision model).\n\nArgument k, verbose, nstart and which are used to specify the repeated runs of the EM algorithm using function stepFlexmix(), where k is the (vector of) number(s) of mixture components, nstart the number of random starting values used, and which determines which number of components is kept if k is a vector.\nBecause the formula for specifying the beta regression model is already a two-part formula, a potential grouping variable is specified via argument ID as opposed to when using flexmix().\nFurther arguments for the component specific model are fixed and extra_components. The argument fixed can be used to specify the covariates for which parameters are the same over components. This is done via a formula interface. The argument extra_components is a list of \"extraComponent\" objects which specify the distribution of the component that needs to be completely specified (via the type argument). The parameter values of that distribution are specified through coef and delta.\n\nextraComponent(type = c(\"uniform\", \"betareg\"), coef, delta, link = \"logit\", link.phi = \"log\")"
  },
  {
    "objectID": "vignettes/betareg-ext.html#sec-illustr-appl",
    "href": "vignettes/betareg-ext.html#sec-illustr-appl",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "\n5 Illustrative application",
    "text": "5 Illustrative application\nTo illustrate the methods introduced above, we consider the analysis of reading accuracy data for nondyslexic and dyslexic Australian children (Smithson and Verkuilen 2006). The data consists of 44 observations of children with ages between eight years and five months and twelve years and three months. For each child, the variables accuracy (the score on a reading accuracy test), iq (the score on a nonverbal intelligent quotient test, converted to \\(z\\) score), and a binary variable on whether the child is dyslexic were recorded. The 19 dyslexic children have a mean reading accuracy of 0.606 and a mean IQ score of \\(-0.653\\). The 25 nondyslexic children have a mean reading accuracy of 0.900 and a mean IQ score of $ 0.497$.\nSmithson and Verkuilen (2006) investigated whether dyslexic children have a different score on the reading accuracy test when corrected for IQ score. Smithson and Verkuilen (2006) fit a beta regression where the means are linked via the logistic link to main and interaction effects for iq and dyslexic, and where the precision parameters are linked with a log-link to main effects for the same variables. The fitted model and its comparison to the results of an OLS regression using the logit-transformed accuracy as response are given in Cribari-Neto and Zeileis (2010). Figure 1 shows a visualization of the fitted models to briefly highlight the most important findings: In the control group (nondyslexic children), reading skill increases clearly with the IQ score while the variance decreases. In the dyslexic group, reading skills are generally lower and almost unaffected by IQ score.\n\n\n\n\n\n\n\nFigure 1: Reading skills data from Smithson and Verkuilen (2006). Linearly transformed reading accuracy by IQ score and dyslexia status (control, blue vs. dyslexic, red). Fitted curves correspond to beta regression (solid) and OLS regression with logit-transformed dependent variable (dashed).\n\n\n\n\nIn what follows, the data is reanalyzed using the methods from Section 2 to Section 4. Initially, the effect of bias to ML inference is assessed. Subsequently, it is illustrated how the differences with respect to dyslexia could have been discovered in a data-driven way. While in the original study dyslexia has, of course, been of prime interest in the model, the data set is used here to illustrate how (a) the two dyslexia groups are automatically selected by recursive partitioning if dyslexia is just one of many covariables and how (b) mixture modeling recovers the dyslexia groups if that covariable is not available at all.\n\n5.1 Bias correction and reduction\nTo investigate whether the results of Smithson and Verkuilen (2006) may have been affected by severe bias in the ML estimator, all three flavors of estimators are obtained and compared for the model with interactions (both in the mean and precision submodels).\n\ndata(\"ReadingSkills\", package = \"betareg\")\nrs_f &lt;- accuracy ~ dyslexia * iq | dyslexia * iq\nrs_ml &lt;- betareg(rs_f, data = ReadingSkills, type = \"ML\")\nrs_bc &lt;- betareg(rs_f, data = ReadingSkills, type = \"BC\")\nrs_br &lt;- betareg(rs_f, data = ReadingSkills, type = \"BR\")\n\n\n\nTable 1: Comparison of coefficients and standard errors in the interaction model for reading skills. The ML estimator from rs_ml, the BC estimator from rs_bc, and the BR estimator from rs_br all give very similar results for the mean submodel. In the precision submodel, main effects are slightly damped and the interaction effect is slightly amplified when using BC/BR.\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood\nBias correction\nBias reduction\n\n\n\nMean\n(Intercept)\n\\(1.019\\)\n\\(0.990\\)\n\\(0.985\\)\n\n\n\n\n\\(0.145\\)\n\\(0.150\\)\n\\(0.150\\)\n\n\n\ndyslexia\n\\(-0.638\\)\n\\(-0.610\\)\n\\(-0.603\\)\n\n\n\n\n\\(0.145\\)\n\\(0.150\\)\n\\(0.150\\)\n\n\n\niq\n\\(0.690\\)\n\\(0.700\\)\n\\(0.707\\)\n\n\n\n\n\\(0.127\\)\n\\(0.133\\)\n\\(0.133\\)\n\n\n\ndyslexia:iq\n\\(-0.776\\)\n\\(-0.786\\)\n\\(-0.784\\)\n\n\n\n\n\\(0.127\\)\n\\(0.133\\)\n\\(0.133\\)\n\n\nPrecision\n(Intercept)\n\\(3.040\\)\n\\(2.811\\)\n\\(2.721\\)\n\n\n\n\n\\(0.258\\)\n\\(0.257\\)\n\\(0.256\\)\n\n\n\ndyslexia\n\\(1.768\\)\n\\(1.705\\)\n\\(1.634\\)\n\n\n\n\n\\(0.258\\)\n\\(0.257\\)\n\\(0.256\\)\n\n\n\niq\n\\(1.437\\)\n\\(1.370\\)\n\\(1.281\\)\n\n\n\n\n\\(0.257\\)\n\\(0.257\\)\n\\(0.257\\)\n\n\n\ndyslexia:iq\n\\(-0.611\\)\n\\(-0.668\\)\n\\(-0.759\\)\n\n\n\n\n\\(0.257\\)\n\\(0.257\\)\n\\(0.257\\)\n\n\nLog-likelihood\n\n\\(66.734\\)\n\\(66.334\\)\n\\(66.134\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Scatterplots of the logarithm of the estimated precision parameters \\(\\log(\\phi_i)\\) based on the maximum likelihood, bias-corrected and bias-reduced estimates. The dashed black line is the main diagonal, the solid red line is a scatterplot smoother.\n\n\n\n\nThe resulting coefficient estimates, standard errors, and log-likelihoods can be displayed using the summary() method and are reported in Table 1. All three estimators give very similar results for the mean submodel. In the precision submodel, main effects are slightly dampened and the interaction effect is slightly amplified when using BC/BR. Figure 2 shows the scatter plots of the logarithm of the estimated precision parameters based on the ML, BC, and BR estimates. It is apparent that the logarithms of the estimated precision parameters based on the bias-corrected and bias-reduced estimates are mildly shrunk towards zero. This is a similar but much milder effect compared to the one described in Kosmidis and Firth (2010). The reason that the effect is milder in this particular example relates to the fact that bias for the precision parameters is corrected/reduced on the log-scale where the ML estimator has a more symmetric distribution than on the original scale.\nTo emphasize that BC/BR may potentially be crucial for empirical analyses, the appendix replicates the results of Kosmidis and Firth (2010) for a beta regression where substantial upward bias was detected for the ML estimator of the precision parameter, which in turn causes underestimated asymptotic standard errors; note the direct dependence of the expected information matrix on the precision parameters in Equation 9.\nFor the reading accuracy data, the similarity of the results in Table 1 between the three different estimation methods and Figure 2 is reassuring and illustrates that analysis based on the ML estimator would not be influenced by bias-related issues. Furthermore, the effect of BC/BR becomes even smaller when the model without interaction in the precision submodel is considered.\n\n5.2 Beta regression tree\nFor illustrating the use of model-based recursive partitioning methods we assume the following situation: A researcher wants to assess whether the relationship between reading accuracy and nonverbal iq score is different for some subgroups in the data. Covariates potentially describing these subgroups are available but no prior knowledge how exactly the subgroups can be described by these covariates. For investigating the ability of the tree to select suitable variables for partitioning, dyslexia is considered as a partitioning variable along with three additional randomly generated noise variables. One noise variable is drawn from a standard normal distribution, one from a uniform distribution and the third is a categorical variable which takes two different values with equal probability.\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(1071)\nn &lt;- nrow(ReadingSkills)\nReadingSkills$x1 &lt;- rnorm(n)\nReadingSkills$x2 &lt;- runif(n)\nReadingSkills$x3 &lt;- factor(sample(0:1, n, replace = TRUE))\n\nThe model-based tree is fitted using betatree(). The first argument is a formula which specifies the model to be partitioned: We have a beta regression where both the mean and the precision of accuracy depend on iq. The second argument is a formula for the symbolic description of the partitioning variables and both formulas are evaluated using data. Additional control arguments for the recursive partitioning method used in mob_control() can be specified via the argument. In this case the minimum number of observations in a node is given by minsize = 10.\n\nrs_tree &lt;- betatree(accuracy ~ iq | iq, ~ dyslexia + x1 + x2 + x3,\n  data = ReadingSkills, minsize = 10)\n\nAlternatively the model could be specified using a three-part formula where the third part is the symbolic description of the partitioning variables.\n\nrs_tree &lt;- betatree(accuracy ~ iq | iq | dyslexia + x1 + x2 + x3,\n  data = ReadingSkills, minsize = 10)\n\nThe returned object is of class \"betatree\" which inherits from \"modelparty\" and \"party\". All methods for \"modelparty\"/\"party\" objects can be reused, e.g., the print() method and the plot() method (see Figure 3).\n\nplot(rs_tree)\n\n\n\n\n\n\n\n\nFigure 3: Partitioned beta regression model for the ReadingSkills data.\n\n\n\n\nFigure 3 indicates that the data was only split into two subsamples. None of the three noise variables was selected in order to perform a split, but only variable dyslexia. This indicates that the relationship between the IQ score and the reading accuracy does not depend on the noise variables as expected. By contrast, the relationship between these two variables differ for dyslexic and nondyslexic children. The beta regressions fitted to each of the two groups of children are illustrated in the two leaf nodes. Note that the fitted models use the IQ score as predictor for the mean and the precision. Hence the results are equivalent to the ML results from Table 1 (where sum contrasts are employed for dyslexia). Function coef() allows to inspect the parameters of the fitted models, by default in the terminal nodes (nodes 2 and 3).\n\ncoef(rs_tree)\n##   (Intercept)        iq (phi)_(Intercept) (phi)_iq\n## 2     1.65653  1.465708            1.2726  2.04786\n## 3     0.38093 -0.086228            4.8077  0.82603\n\nIf the fitted object is printed the output indicates after the number of the node, which part of the data according to the split is contained (e.g., ) or the weights of the observations in the terminal nodes indicated by stars. In the terminal nodes also the estimated parameters of the beta regression models are provided.\n\nrs_tree\n## Beta regression tree\n## \n## Model formula:\n## accuracy ~ iq + iq | dyslexia + x1 + x2 + x3\n## \n## Fitted party:\n## [1] root\n## |   [2] dyslexia in no: n = 25\n## |             (Intercept)                iq (phi)_(Intercept) \n## |                  1.6565            1.4657            1.2726 \n## |                (phi)_iq \n## |                  2.0479 \n## |   [3] dyslexia in yes: n = 19\n## |             (Intercept)                iq (phi)_(Intercept) \n## |                0.380932         -0.086228          4.807662 \n## |                (phi)_iq \n## |                0.826033 \n## \n## Number of inner nodes:    1\n## Number of terminal nodes: 2\n## Number of parameters per node: 4\n## Objective function (negative log-likelihood): -66.734\n\nThe output above confirms that in the nondyslexic group there is a positive association of both mean accuracy and the precision with IQ score. In the dyslexic group, the mean accuracy is generally lower with almost no dependence on IQ score while precision is higher and slightly decreasing with IQ score. Some further details could be revealed by considering for example summary(rs_tree, node = 3) that provides the usual regression model summary (unadjusted for recursive partitioning) for the model associated with node 3.\nTo gain further insight into the recursive construction of the beta regression tree, we use the results of the parameter instability tests in all three nodes. The test statistics together with the corresponding \\(p\\) values can be obtained using function sctest() (for structural change test). This indicates which partitioning variables in each node exhibited significant instability and the reason for performing no further split, i.e., either because all parameter instability tests were insignificant (see node 2) or because the node size is too small for a further split (see node 3).\n\nlibrary(\"strucchange\")\nsctest(rs_tree)\n## $`1`\n##             dyslexia      x1      x2     x3\n## statistic 2.2687e+01 8.52510 5.56986 3.6273\n## p.value   5.8479e-04 0.90946 0.99871 0.9142\n## \n## $`2`\n##           dyslexia      x1      x2      x3\n## statistic        0 6.41163 4.51702 8.20191\n## p.value         NA 0.84121 0.97516 0.23257\n## \n## $`3`\n## NULL\n\nIn node 1 only dyslexia shows significant instability while the noise variables are all insignificant. In node 2, dyslexia cannot be used for splitting anymore and all other variables are still insignificant and thus the partitioning stops. With only 19 observations, node 3 is considered too small to warrant further splitting given that minsize = 10 requires that each node contains at least 10 observations and hence no tests are carried out.\n\n5.3 Latent class beta regression\nFor illustrating the use of finite mixture models we assume the following situation: A researcher wants to assess whether the relationship between reading accuracy and nonverbal iq score is different for some subgroups in the data without having further covariates potentially describing the groups available. In particular, we assume that the information whether the children are dyslexic or not is not available. Modeling the relationship between reading accuracy and IQ score is now complicated by the fact that latent groups exist in the data where this relationship is different.\nThe group of nondyslexic children is challenging as some of them essentially have a perfect reading accuracy while for others accuracy is strongly increasing with the IQ score. In a model with observed dyslexia, this can be captured by different variances in the two groups. However, issues arise when dyslexia is unobserved and a mixture model is employed to infer the groups. Specifically, the subgroup with perfect reading score will typically be selected as one component of the mixture whose variance converges to zero leading to an unbounded likelihood. To address this issue we fit a finite mixture model with three components, where one component is used to capture those children who have a perfect reading accuracy test score. Following Smithson, Merkle, and Verkuilen (2011) this additional component is assumed to follow a uniform distribution on the interval coef \\(\\pm\\) delta.\n\nrs_mix &lt;- betamix(accuracy ~ iq, data = ReadingSkills, k = 3,\n  extra_components = extraComponent(type = \"uniform\",\n    coef = 0.99, delta = 0.01), nstart = 10)\n\nThe argument nstart is set to 10. This implies that the EM algorithm is run 10 times, with each run being randomly initialized. Then only the best solution according to the log-likelihood is returned. In this way, the chance that the global optimum is detected is increased (the EM algorithm is generally only guaranteed to converge to a local optimum and the convergence behaviour depends on the initialization).\nThe returned fitted model is of class \"betamix\" and has methods for clusters,betamix,ANY-method, coef, coerce,oldClass,S3-method, fitted,betamix-method, initialize,oldClass-method, logLik, posterior,betamix,ANY-method, predict,betamix-method, print, show,oldClass-method, slotsFromS3,oldClass-method and summary. These methods reuse functionality already available for finite mixture models that are directly fitted using flexmix() from package flexmix. The print() method shows the function call and provides information on how many observations are assigned to each of the components based on the values of the posterior probabilities. Furthermore, the convergence status of the EM algorithm is reported, and in the case of convergence, the number of iterations that were performed is shown. % it is indicated % if the EM algorithm converged or not and in the case of convergence % how many iterations were performed.\n\nrs_mix\n## \n## Call:\n## betamix(formula = accuracy ~ iq, data = ReadingSkills, \n##     k = 3, nstart = 10, extra_components = extraComponent(type = \"uniform\", \n##         coef = 0.99, delta = 0.01))\n## \n## Cluster sizes:\n##  1  2  3 \n## 20 10 14 \n## \n## convergence after 20 iterations\n\nThe summary() method provides more information on the estimated coefficients and their estimated standard errors. For the calculation of the latter, function optim() is used for the numerical approximation of the corresponding Hessian matrix.\n\nsummary(rs_mix)\n## $Comp.1\n## $Comp.1$mean\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   0.5025     0.0825    6.09  1.1e-09 ***\n## iq           -0.0484     0.1130   -0.43     0.67    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## $Comp.1$precision\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)    4.251      0.748    5.69  1.3e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## $Comp.2\n## $Comp.2$mean\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)    1.403      0.263    5.33  9.9e-08 ***\n## iq             0.825      0.216    3.81  0.00014 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## $Comp.2$precision\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)    2.685      0.454    5.91  3.4e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nBecause only two components are freely estimated and the parameters for the third component were fixed a-priori, the detailed information on the estimated parameters is only provided for components 1 and 2. The regression part for the mean indicates that in the first component the IQ score does not significantly affect the achieved accuracy, while there is a positive significant effect of the IQ score on accuracy in the second component.\n\n\n\n\n\n\n\nFigure 4: Fitted regression lines for the mixture model with three components and the observations shaded according to their posterior probabilities (left). Fitted regression lines for the partitioned beta regression model with shading according to the observed dyslexic variable where nondyslexic and dyslexic children are in blue and red, respectively (right).\n\n\n\n\nA cross-tabulation of the cluster assignments of the mixture model with the variable dyslexia indicates that no dyslexic children are assigned to the third component. Furthermore, children assigned to the first component have a high probability (80%) of being dyslexic.\n\ntable(clusters(rs_mix), ReadingSkills$dyslexia)\n##    \n##     no yes\n##   1  4  16\n##   2  7   3\n##   3 14   0\n\nThe fitted mean regression lines for each of the three components are provided in Figure 4 (left). The observations are shaded according to the magnitude of the corresponding posterior probabilities. The stronger the shading of an observation is in red, the higher the posterior probability for this observation being from the first component is. Blue shading corresponds to the second component and green to the third. For comparison purposes, the right plot in Figure 4 shows the mean regression lines for the dyslexic and nondyslexic children as obtained by recursive partitioning – or equivalently for the model where an interaction with the variable dyslexic is specified in the regressions for mean and precision.\nThe fitted regression lines for the dyslexic children (red) and the latent group capturing the dyslexic children (component 1) are very similar. In contrast, the group of nondyslexic children is modeled differently. With observed dyslexia, the heterogeneity in the control group is captured by differences in the precision submodel, i.e., in the variance. However, for unobserved dyslexia, it is more natural to capture the increased heterogeneity in the control group using two components, one of which would correspond to perfect reading accuracy irrespective of the IQ score."
  },
  {
    "objectID": "vignettes/betareg-ext.html#sec-conclusions",
    "href": "vignettes/betareg-ext.html#sec-conclusions",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "\n6 Conclusions",
    "text": "6 Conclusions\nThe new extensions of the package betareg allow to move beyond classical ML inference when fitting beta regression models. Bias correction and bias reduction of the ML estimates can be useful alternatives when the ML inferences turn out to be unreliable, and actually their ready availability in the package allows users to check how sensitive inferences (standard errors, confidence intervals and Wald tests, in particular) can be to the bias of the ML estimator. Recursive partitioning methods and finite mixture models enable the user to investigate heterogeneity – both observed and unobserved – in the regression model fitted to the whole sample.\nFor users already familiar with previous versions of the betareg package, obtaining the bias-corrected/reduced estimators is straightforward; the user only needs to appropriately specify the type argument (which defaults to \"ML\").\nFor the implementation of the aforementioned extensions some changes and additions in the fitting function betareg.fit() were necessary. Specifically, the optimization of the likelihood is now followed by a Fisher scoring iteration. Furthermore, if the bias-reduced or bias-corrected estimates are requested, that Fisher scoring iteration is accordingly modified using a bias adjustment.\nTo fit beta regression trees and finite mixtures of beta regressions the new functions betatree() and betamix() are available in package betareg. These functions borrow functionality from the packages partykit and flexmix. The interface of the two new functions has been designed to be as similar as possible to betareg(), in order to facilitate their use by users that are already familiar with the betareg() function.\nFor modeling heterogeneity betareg.fit() is reused to fit the models in the nodes when beta regression trees are constructed, and in the M-step when finite mixture models are fitted. For this task, only a small amount of additional code was necessary to inherit the functionality provided by the partykit and flexmix packages to the package betareg.\nOverall, the increased flexibility of the extended package betareg enables users to conveniently check model suitability and appropriateness of the resultant inferences. With the extended package users can easily compare the results from a beta regression model fitted using ML estimation to those using bias correction/reduction, and draw conclusions incorporating observed or unobserved heterogeneity in their models."
  },
  {
    "objectID": "vignettes/betareg-ext.html#acknowledgments",
    "href": "vignettes/betareg-ext.html#acknowledgments",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nBG gratefully acknowledges financial support from the Austrian Science Fund (FWF): V170-N18."
  },
  {
    "objectID": "vignettes/betareg-ext.html#appendix-bias-correctionreduction-for-gasoline-yield-data",
    "href": "vignettes/betareg-ext.html#appendix-bias-correctionreduction-for-gasoline-yield-data",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "Appendix: Bias correction/reduction for gasoline yield data",
    "text": "Appendix: Bias correction/reduction for gasoline yield data\nTo illustrate how upward bias in the ML estimator of the precision parameter in beta regressions can severely affect inference, results from Kosmidis and Firth (2010) are replicated. All three flavors of estimators (ML, BC, and BR) are computed for the fixed-precision beta regression model considered in Ferrari and Cribari-Neto (2004) (also replicated in ):\n\ndata(\"GasolineYield\", package = \"betareg\")\ngy &lt;- lapply(c(\"ML\", \"BC\", \"BR\"), function(x)\n  betareg(yield ~ batch + temp, data = GasolineYield, type = x))\n\n\n\nTable 2: ML, BC and BR estimates and corresponding estimated standard errors for a logit-linked beta regression model for the gasoline yield data. The precision parameter \\(\\phi\\) is assumed to be equal across the observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood\n\nBias correction\n\nBias reduction\n\n\n\n\n\\(\\beta_{1}\\)\n\\(-6.15957\\)\n\\(0.18232\\)\n\\(-6.14837\\)\n\\(0.23595\\)\n\\(-6.14171\\)\n\\(0.23588\\)\n\n\n\\(\\beta_{2}\\)\n\\(1.72773\\)\n\\(0.10123\\)\n\\(1.72484\\)\n\\(0.13107\\)\n\\(1.72325\\)\n\\(0.13106\\)\n\n\n\\(\\beta_{3}\\)\n\\(1.32260\\)\n\\(0.11790\\)\n\\(1.32009\\)\n\\(0.15260\\)\n\\(1.31860\\)\n\\(0.15257\\)\n\n\n\\(\\beta_{4}\\)\n\\(1.57231\\)\n\\(0.11610\\)\n\\(1.56928\\)\n\\(0.15030\\)\n\\(1.56734\\)\n\\(0.15028\\)\n\n\n\\(\\beta_{5}\\)\n\\(1.05971\\)\n\\(0.10236\\)\n\\(1.05788\\)\n\\(0.13251\\)\n\\(1.05677\\)\n\\(0.13249\\)\n\n\n\\(\\beta_{6}\\)\n\\(1.13375\\)\n\\(0.10352\\)\n\\(1.13165\\)\n\\(0.13404\\)\n\\(1.13024\\)\n\\(0.13403\\)\n\n\n\\(\\beta_{7}\\)\n\\(1.04016\\)\n\\(0.10604\\)\n\\(1.03829\\)\n\\(0.13729\\)\n\\(1.03714\\)\n\\(0.13727\\)\n\n\n\\(\\beta_{8}\\)\n\\(0.54369\\)\n\\(0.10913\\)\n\\(0.54309\\)\n\\(0.14119\\)\n\\(0.54242\\)\n\\(0.14116\\)\n\n\n\\(\\beta_{9}\\)\n\\(0.49590\\)\n\\(0.10893\\)\n\\(0.49518\\)\n\\(0.14099\\)\n\\(0.49446\\)\n\\(0.14096\\)\n\n\n\\(\\beta_{10}\\)\n\\(0.38579\\)\n\\(0.11859\\)\n\\(0.38502\\)\n\\(0.15353\\)\n\\(0.38459\\)\n\\(0.15351\\)\n\n\n\\(\\beta_{11}\\)\n\\(0.01097\\)\n\\(0.00041\\)\n\\(0.01094\\)\n\\(0.00053\\)\n\\(0.01093\\)\n\\(0.00053\\)\n\n\n\\(\\phi\\)\n\\(440.27839\\)\n\\(110.02562\\)\n\\(261.20610\\)\n\\(65.25866\\)\n\\(261.03777\\)\n\\(65.21640\\)\n\n\n\n\n\n\nThe estimate of the precision parameter shrinks considerably when bias correction/reduction is used, indicating a large upward bias for the ML estimator of \\(\\phi\\).\n\nsapply(gy, coef, model = \"precision\")\n##  (phi)  (phi)  (phi) \n## 440.28 261.21 261.04\n\nwhile the log-likelihood does not change much\n\nsapply(gy, logLik)\n## [1] 84.798 82.947 82.945\n\nThis results in much larger standard errors (and hence smaller test statistics and larger \\(p\\) values) for all coefficients in the mean part of the model. Table 2 replicates (Kosmidis and Firth 2010, Table 1). The picture does also not change much when a log-link is used in the precision model, see below and Table 3 replicating (Kosmidis and Firth 2010, Table 3).\n\ndata(\"GasolineYield\", package = \"betareg\")\ngy2 &lt;- lapply(c(\"ML\", \"BC\", \"BR\"), function(x)\n  betareg(yield ~ batch + temp | 1, data = GasolineYield, type = x))\nsapply(gy2, logLik)\n## [1] 84.798 83.797 83.268\n\n\n\nTable 3: ML, BC and BR estimates and corresponding estimated standard errors for a logit-linked beta regression model for the gasoline yield data. Precision is estimated on the log-scale and is assumed to be equal across the observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelihood\n\nBias correction\n\nBias reduction\n\n\n\n\n\\(\\beta_{1}\\)\n\\(-6.15957\\)\n\\(0.18232\\)\n\\(-6.14837\\)\n\\(0.21944\\)\n\\(-6.14259\\)\n\\(0.22998\\)\n\n\n\\(\\beta_{2}\\)\n\\(1.72773\\)\n\\(0.10123\\)\n\\(1.72484\\)\n\\(0.12189\\)\n\\(1.72347\\)\n\\(0.12777\\)\n\n\n\\(\\beta_{3}\\)\n\\(1.32260\\)\n\\(0.11790\\)\n\\(1.32009\\)\n\\(0.14193\\)\n\\(1.31880\\)\n\\(0.14875\\)\n\n\n\\(\\beta_{4}\\)\n\\(1.57231\\)\n\\(0.11610\\)\n\\(1.56928\\)\n\\(0.13978\\)\n\\(1.56758\\)\n\\(0.14651\\)\n\n\n\\(\\beta_{5}\\)\n\\(1.05971\\)\n\\(0.10236\\)\n\\(1.05788\\)\n\\(0.12323\\)\n\\(1.05691\\)\n\\(0.12917\\)\n\n\n\\(\\beta_{6}\\)\n\\(1.13375\\)\n\\(0.10352\\)\n\\(1.13165\\)\n\\(0.12465\\)\n\\(1.13041\\)\n\\(0.13067\\)\n\n\n\\(\\beta_{7}\\)\n\\(1.04016\\)\n\\(0.10604\\)\n\\(1.03829\\)\n\\(0.12767\\)\n\\(1.03729\\)\n\\(0.13383\\)\n\n\n\\(\\beta_{8}\\)\n\\(0.54369\\)\n\\(0.10913\\)\n\\(0.54309\\)\n\\(0.13133\\)\n\\(0.54248\\)\n\\(0.13763\\)\n\n\n\\(\\beta_{9}\\)\n\\(0.49590\\)\n\\(0.10893\\)\n\\(0.49518\\)\n\\(0.13112\\)\n\\(0.49453\\)\n\\(0.13743\\)\n\n\n\\(\\beta_{10}\\)\n\\(0.38579\\)\n\\(0.11859\\)\n\\(0.38502\\)\n\\(0.14278\\)\n\\(0.38465\\)\n\\(0.14966\\)\n\n\n\\(\\beta_{11}\\)\n\\(0.01097\\)\n\\(0.00041\\)\n\\(0.01094\\)\n\\(0.00050\\)\n\\(0.01093\\)\n\\(0.00052\\)\n\n\n\\(\\log\\phi\\)\n\\(6.08741\\)\n\\(0.24990\\)\n\\(5.71191\\)\n\\(0.24986\\)\n\\(5.61608\\)\n\\(0.24984\\)"
  },
  {
    "objectID": "vignettes/betareg-ext.html#footnotes",
    "href": "vignettes/betareg-ext.html#footnotes",
    "title": "Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",
    "section": "Footnotes",
    "text": "Footnotes\n\nAn example of M-fluctuation tests for parameter instability (also known as structural change) in beta regressions is also discussed in Zeileis (2006) and replicated in Cribari-Neto and Zeileis (2010). However, this uses a double-maximum type test statistic, not a \\(\\sup\\)LM or \\(\\chi^2\\) statistic.↩︎"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Beta Regression",
    "section": "Overview",
    "text": "Overview\nThe R package betareg provides:\n\nBeta regression for modeling beta-distributed dependent variables on the open unit interval (0, 1), e.g., rates and proportions, see Cribari-Neto and Zeileis (2010, doi:10.18637/jss.v034.i02).\nExtended-support beta regression models for variables on the closed unit interval [0, 1] with boundary observations at 0 and/or 1, see Kosmidis and Zeileis (2024, doi:10.48550/arXiv.2409.07233).\nAlternative specifications of the classical beta regression model: Bias-corrected and bias-reduced estimation, finite mixture models, and recursive partitioning for (0, 1) beta regression, see Grün, Kosmidis, and Zeileis (2012, doi:10.18637/jss.v048.i11)."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Beta Regression",
    "section": "Installation",
    "text": "Installation\nThe stable version of betareg is available on CRAN:\ninstall.packages(\"betareg\")\nThe latest development version can be installed from R-universe:\ninstall.packages(\"betareg\", repos = \"https://zeileis.R-universe.dev\")"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Beta Regression",
    "section": "License",
    "text": "License\nThe package is available under the General Public License version 3 or version 2"
  },
  {
    "objectID": "index.html#illustration",
    "href": "index.html#illustration",
    "title": "Beta Regression",
    "section": "Illustration",
    "text": "Illustration\nA nice first illustration of beta regression is the analysis of reading accuracy scores from primary school children from Smithson & Verkuilen (2006). Package and data can be loaded via:\n\nlibrary(\"betareg\")\ndata(\"ReadingSkills\", package = \"betareg\")\n\nThe reading accuracy was scaled to be within (0, 1). Its mean is explained by verbal iq score with separate lines by dyslexia (control vs. dyslexic). The precision parameter is explained by main effects of the two explanatory variables. More details are provided in ?ReadingSkills.\n\nbr &lt;- betareg(accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\nsummary(br)\n## \n## Call:\n## betareg(formula = accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\n## \n## Quantile residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.3625 -0.5872  0.3026  0.9425  1.5874 \n## \n## Coefficients (mean model with logit link):\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   1.1232     0.1428   7.864 3.73e-15 ***\n## dyslexia     -0.7416     0.1428  -5.195 2.04e-07 ***\n## iq            0.4864     0.1331   3.653 0.000259 ***\n## dyslexia:iq  -0.5813     0.1327  -4.381 1.18e-05 ***\n## \n## Phi coefficients (precision model with log link):\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   3.3044     0.2227  14.835  &lt; 2e-16 ***\n## dyslexia      1.7466     0.2623   6.658 2.77e-11 ***\n## iq            1.2291     0.2672   4.600 4.23e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n## \n## Type of estimator: ML (maximum likelihood)\n## Log-likelihood:  65.9 on 7 Df\n## Pseudo R-squared: 0.5756\n## Number of iterations: 25 (BFGS) + 1 (Fisher scoring)\n\nThe regression summary shows that accuracy increases with iq for the control group but not for the dyslexic group (even slightly decreases). This can be brought out more clearly graphically. This also highlights that the model employs a logit link so that the fitted curves always remain within (0, 1).\n\npal &lt;- palette.colors()[c(4, 8)]\npch &lt;- c(19, 17)\nplot(accuracy ~ iq, data = ReadingSkills, col = pal[dyslexia], pch = pch[dyslexia])\niq &lt;- -30:30/10\nlines(iq, predict(br, newdata = data.frame(dyslexia = \"no\", iq = iq)), col = pal[1], lwd = 2)\nlines(iq, predict(br, newdata = data.frame(dyslexia = \"yes\", iq = iq)), col = pal[2], lwd = 2)\nlegend(\"topleft\", c(\"Control\", \"Dyslexic\"), pch = pch, col = pal, bty = \"n\")"
  },
  {
    "objectID": "index.html#extended-models",
    "href": "index.html#extended-models",
    "title": "Beta Regression",
    "section": "Extended models",
    "text": "Extended models\nFor going beyond this basic analysis the following extensions can be considered.\nExtended-support beta regression\nTo analyze the original accuracy scores in [0, 1] (without scaling the perfect scores of 1 to 0.99), use the variable accuracy1 (instead of accuracy) in the code above. The betareg() model then automatically estimates an additional exceedence parameter that accounts for the boundary probability of a perfect score. For this data set, most coefficients shrink a bit, rendering some coefficients only weakly significant but the qualitative interpretations still remain similar.\n\nbetareg(accuracy1 ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\n\nSee Kosmidis and Zeileis (2024) and the documentation of betareg() for more details.\nBias reduction\nBias-reduced estimation (instead of the default maximum likelihood estimation) can be used by adding the argument type = \"BR\" in betareg(). This slightly shrinks all coefficient estimates but, on this data, leads to qualitatively identical results.\n\nbetareg(accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills, type = \"BR\")\n\nSee Grün et al. (2012) and the documentation of betareg() for more details.\nBeta regression trees\nTo find subgroups in a beta regression by recursively splitting subsamples, beta regression trees can be used. Here, this strategy can be used to figure out the different iq effects by dyslexia rather than fixing the variables’ interaction in advance.\n\nbetatree(accuracy ~ iq | iq, ~ dyslexia + ..., data = ReadingSkills, minsize = 10)\n\nSee Grün et al. (2012) and the documentation of betatree() for more details.\nFinite mixtures of beta regressions\nTo find clusters in a beta regression finite mixtures of beta regressions can be used. Here, this technique can be employed to find the different iq effects in the data without even requiring the dyslexia information.\n\nbetamix(accuracy ~ iq, data = ReadingSkills, k = 3, ...)\n\nSee Grün et al. (2012) and the documentation of betamix() for more details."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "To report bugs please send a simple e-mail to the package maintainer:\nAchim.Zeileis at R-project dot org\nFor inquiries you can also reach out on social media:\n@zeileis@fosstodon.org (Mastodon)\n@AchimZeileis (X/Twitter)\nFor discussions we also try to follow these channels:\n\nStackOverflow with betareg tag\nCrossValidated with beta-regression tag\nR-help mailing list\n\n\n\n\n\nAchim Zeileis  \nFrancisco Cribari-Neto  \nBettina Grün  \nIoannis Kosmidis"
  },
  {
    "objectID": "contact.html#reporting-bugs",
    "href": "contact.html#reporting-bugs",
    "title": "Contact",
    "section": "",
    "text": "To report bugs please send a simple e-mail to the package maintainer:\nAchim.Zeileis at R-project dot org\nFor inquiries you can also reach out on social media:\n@zeileis@fosstodon.org (Mastodon)\n@AchimZeileis (X/Twitter)\nFor discussions we also try to follow these channels:\n\nStackOverflow with betareg tag\nCrossValidated with beta-regression tag\nR-help mailing list"
  },
  {
    "objectID": "contact.html#authors",
    "href": "contact.html#authors",
    "title": "Contact",
    "section": "",
    "text": "Achim Zeileis  \nFrancisco Cribari-Neto  \nBettina Grün  \nIoannis Kosmidis"
  },
  {
    "objectID": "vignettes/betareg.html",
    "href": "vignettes/betareg.html",
    "title": "Beta Regression in R",
    "section": "",
    "text": "Francisco Cribari-Neto\nAchim Zeileis\n\n\nAbstract\nThis introduction to the R package betareg is a (slightly) modified version of Cribari-Neto and Zeileis (2010), published in the Journal of Statistical Software. A follow-up paper with various extensions is Grün, Kosmidis, and Zeileis (2012) – a slightly modified version of which is also provided within the package as vignette(\"betareg-ext\", package = \"betareg\")\nThe class of beta regression models is commonly used by practitioners to model variables that assume values in the standard unit interval \\((0, 1)\\). It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function. The model also includes a precision parameter which may be constant or depend on a (potentially different) set of regressors through a link function as well. This approach naturally incorporates features such as heteroskedasticity or skewness which are commonly observed in data taking values in the standard unit interval, such as rates or proportions. This paper describes the betareg package which provides the class of beta regressions in the R system for statistical computing. The underlying theory is briefly outlined, the implementation discussed and illustrated in various replication exercises."
  },
  {
    "objectID": "vignettes/betareg.html#sec-intro",
    "href": "vignettes/betareg.html#sec-intro",
    "title": "Beta Regression in R",
    "section": "\n1 Introduction",
    "text": "1 Introduction\nHow should one perform a regression analysis in which the dependent variable (or response variable), \\(y\\), assumes values in the standard unit interval \\((0,1)\\)? The usual practice used to be to transform the data so that the transformed response, say \\(\\tilde y\\), assumes values in the real line and then apply a standard linear regression analysis. A commonly used transformation is the logit, \\(\\tilde y =\n\\log(y/(1-y))\\). This approach, nonetheless, has shortcomings. First, the regression parameters are interpretable in terms of the mean of \\(\\tilde y\\), and not in terms of the mean of \\(y\\) (given Jensen’s inequality). Second, regressions involving data from the unit interval such as rates and proportions are typically heteroskedastic: they display more variation around the mean and less variation as we approach the lower and upper limits of the standard unit interval. Finally, the distributions of rates and proportions are typically asymmetric, and thus Gaussian-based approximations for interval estimation and hypothesis testing can be quite inaccurate in small samples. Ferrari and Cribari-Neto (2004) proposed a regression model for continuous variates that assume values in the standard unit interval, e.g., rates, proportions, or concentration indices. Since the model is based on the assumption that the response is beta-distributed, they called their model the beta regression model. In their model, the regression parameters are interpretable in terms of the mean of \\(y\\) (the variable of interest) and the model is naturally heteroskedastic and easily accomodates asymmetries. A variant of the beta regression model that allows for nonlinearities and variable dispersion was proposed by Simas, Barreto-Souza, and Rocha (2010). In particular, in this more general model, the parameter accounting for the precision of the data is not assumed to be constant across observations but it is allowed to vary, leading to the variable dispersion beta regression model.\nThe chief motivation for the beta regression model lies in the flexibility delivered by the assumed beta law. The beta density can assume a number of different shapes depending on the combination of parameter values, including left- and right-skewed or the flat shape of the uniform density (which is a special case of the more general beta density). This is illustrated in Figure 1 which depicts several different beta densities. Following Ferrari and Cribari-Neto (2004), the densities are parameterized in terms of the mean \\(\\mu\\) and the precision parameter \\(\\phi\\); all details are explained in the next section. The evident flexiblity makes the beta distribution an attractive candidate for data-driven statistical modeling.\n\n\n\n\n\n\n\nFigure 1: Probability density functions for beta distributions with varying parameters \\(\\mu = 0.10, 0.25, 0.50, 0.75, 0.90\\) and \\(\\phi = 5\\) (left) and \\(\\phi = 100\\) (right).\n\n\n\n\nThe idea underlying beta regression models dates back to earlier approaches such as Williams (1982) or Prentice (1986). The initial motivation was to model binomial random variables with extra variation. The model postulated for the (discrete) variate of interest included a more flexible variation structure determined by independent beta-distributed variables which are related to a set of independent variables through a regression structure. However, unlike the more recent literature, the main focus was to model binomial random variables. Our interest in what follows will be more closely related to the recent literature, i.e., modeling continous random variables that assume values in \\((0,1)\\), such as rates, proportions, and concentration or inequality indices (e.g., Gini).\nIn this paper, we describe the betareg package which can be used to perform inference in both fixed and variable dispersion beta regressions. The package is implemented in the R system for statistical computing (R Core Team 2024) and available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=betareg. The initial version of the package was written by Simas and Rocha (2006) up to version 1.2 which was orphaned and archived on CRAN in mid-2009. Starting from version 2.0-0, Achim Zeileis took over maintenance after rewriting/extending the package’s functionality.\nThe paper unfolds as follows: Section 2 outlines the theory underlying the beta regression model before Section 3 describes its implementation in R. Section 4 and Section 5 provide various empirical applications: The former focuses on illustrating various aspects of beta regressions in practice while the latter provides further replications of previously published empirical research. Finally, Section 6 contains concluding remarks and directions for future research and implementation."
  },
  {
    "objectID": "vignettes/betareg.html#sec-model",
    "href": "vignettes/betareg.html#sec-model",
    "title": "Beta Regression in R",
    "section": "\n2 Beta regression",
    "text": "2 Beta regression\nThe class of beta regression models, as introduced by Ferrari and Cribari-Neto (2004), is useful for modeling continuous variables \\(y\\) that assume values in the open standard unit interval \\((0,1)\\). Note that if the variable takes on values in \\((a, b)\\) (with \\(a &lt; b\\) known) one can model \\((y - a)/(b - a)\\). Furthermore, if \\(y\\) also assumes the extremes \\(0\\) and \\(1\\), a useful transformation in practice is \\((y \\cdot (n - 1) + 0.5) / n\\) where \\(n\\) is the sample size (Smithson and Verkuilen 2006).\nThe beta regression model is based on an alternative parameterization of the beta density in terms of the variate mean and a precision parameter. The beta density is usually expressed as\n\\[\nf(y;p,q) = \\frac{\\Gamma(p+q)}{\\Gamma(p)\\Gamma(q)}y^{p-1}(1-y)^{q-1}, \\quad 0&lt;y&lt;1,\n\\]\nwhere \\(p,q &gt;0\\) and \\(\\Gamma(\\cdot)\\) is the gamma function.1 Ferrari and Cribari-Neto (2004) proposed a different parameterization by setting \\(\\mu = p/(p+q)\\) and \\(\\phi = p+q\\):\n\\[\nf(y;\\mu,\\phi) = \\frac{\\Gamma(\\phi)}{\\Gamma(\\mu\\phi)\\Gamma((1-\\mu)\\phi)}y^{\\mu\\phi-1}(1-y)^{(1-\\mu)\\phi-1}, \\quad 0&lt;y&lt;1,\n\\tag{1}\\]\nwith \\(0&lt;\\mu&lt;1\\) and \\(\\phi&gt;0\\). We write \\(y \\,\\sim\\, \\mathcal{B}(\\mu, \\phi)\\). Here, \\(\\text{E}(y) = \\mu\\) and \\(\\text{Var}(y) = \\mu(1-\\mu)/(1+\\phi)\\). The parameter \\(\\phi\\) is known as the precision parameter since, for fixed \\(\\mu\\), the larger \\(\\phi\\) the smaller the variance of \\(y\\); \\(\\phi^{-1}\\) is a dispersion parameter.\nLet \\(y_1,\\ldots,y_n\\) be a random sample such that \\(y_i \\sim\n{\\mathcal{B}}(\\mu_i,\\phi)\\), \\(i=1,\\ldots,n\\). The beta regression model is defined as\n\\[\ng(\\mu_i) = x_{i}^\\top \\beta = \\eta_i,\n\\]\nwhere \\(\\beta=(\\beta_1,\\ldots,\\beta_k)^\\top\\) is a \\(k \\times 1\\) vector of unknown regression parameters (\\(k&lt;n\\)), \\(x_i = (x_{i1},\\ldots,x_{ik})^\\top\\) is the vector of \\(k\\) regressors (or independent variables or covariates) and \\(\\eta_i\\) is a linear predictor (i.e., \\(\\eta_i = \\beta_1 x_{i1} + \\cdots + \\beta_k x_{ik}\\); usually \\(x_{i1}=1\\) for all \\(i\\) so that the model has an intercept).\nHere, \\(g(\\cdot): (0,1) \\mapsto\n\\mathrm{I\\! R}\\) is a link function, which is strictly increasing and twice differentiable. The main motivation for using a link function in the regression structure is twofold. First, both sides of the regression equation assume values in the real line when a link function is applied to \\(\\mu_i\\). Second, there is an added flexibility since the practitioner can choose the function that yields the best fit. Some useful link functions are: logit \\(g(\\mu) = \\log(\\mu/(1-\\mu))\\); probit \\(g(\\mu) = \\Phi^{-1}(\\mu)\\), where \\(\\Phi(\\cdot)\\) is the standard normal distribution function; complementary log-log \\(g(\\mu) = \\log\\{-\\log(1-\\mu)\\}\\); log-log \\(g(\\mu) = -\\log\\{-\\log(\\mu)\\}\\); and Cauchy \\(g(\\mu) = \\tan\\{\\pi(\\mu - 0.5)\\}\\). Note that the variance of \\(y\\) is a function of \\(\\mu\\) which renders the regression model based on this parameterization naturally heteroskedastic. In particular,\n\\[\n   \\text{Var}(y_i) = \\frac{\\mu_i(1-\\mu_i)}{1+\\phi}\n             = \\frac{g^{-1}(x_i^{\\top}\\beta)[1-g^{-1}(x_i^{\\top}\\beta)]}{1+\\phi}.\n\\tag{2}\\]\nThe log-likelihood function is \\(\\ell(\\beta,\\phi)=\\sum_{i=1}^n\\ell_i(\\mu_i,\\phi)\\), where\n\\[\n\\begin{eqnarray}\n  \\ell_i(\\mu_i,\\phi) & = & \\log \\Gamma(\\phi)-\\log\\Gamma(\\mu_i\\phi) - \\log \\Gamma((1-\\mu_i)\\phi) +(\\mu_i\\phi-1)\\log y_i \\\\\n                     &   & + \\{(1-\\mu_i)\\phi-1\\}\\log(1-y_i).\n\\end{eqnarray}\n\\tag{3}\\]\nNotice that \\(\\mu_i=g^{-1}(x_i^{\\top}\\beta)\\) is a function of \\(\\beta\\), the vector of regression parameters. Parameter estimation is performed by maximum likelihood (ML).\nAn extension of the beta regression model above which was employed by Smithson and Verkuilen (2006) and formally introduced (along with further extensions) by Simas, Barreto-Souza, and Rocha (2010) is the variable dispersion beta regression model. In this model the precision parameter is not constant for all observations but instead modeled in a similar fashion as the mean parameter. More specifically, \\(y_i \\, \\sim \\, {\\mathcal B}(\\mu_i, \\phi_i)\\) independently, \\(i=1,\\ldots,n\\), and\n\\[\n\\begin{eqnarray}\n  g_1(\\mu_i)  & = & \\eta_{1i} = x_i^\\top \\beta,  \\\\\n  g_2(\\phi_i) & = & \\eta_{2i} = z_i^\\top \\gamma,\n\\end{eqnarray}\n\\tag{4}\\]\nwhere \\(\\beta=(\\beta_1, \\ldots, \\beta_k)^{\\top}\\), \\(\\gamma=(\\gamma_1,\\ldots,\\gamma_h)^{\\top}\\), \\(k+h&lt;n\\), are the sets of regression coefficients in the two equations, \\(\\eta_{1i}\\) and \\(\\eta_{2i}\\) are the linear predictors, and \\(x_i\\) and \\(z_i\\) are regressor vectors. As before, both coefficient vectors are estimated by ML, simply replacing \\(\\phi\\) by \\(\\phi_i\\) in Equation 3.\nSimas, Barreto-Souza, and Rocha (2010) further extend the model above by allowing nonlinear predictors in Equation 4. Also, they have obtained analytical bias corrections for the ML estimators of the parameters, thus generalizing the results of Ospina, Cribari-Neto, and Vasconcellos (2006), who derived bias corrections for fixed dispersion beta regressions. However, as these extensions are not (yet) part of the betareg package, we confine ourselves to these short references and do not provide detailed formulas.\nVarious types of residuals are available for beta regression models. The raw response residuals \\(y_i - \\hat \\mu_i\\) are typically not used due to the heteroskedasticity inherent in the model (see Equation 2). Hence, a natural alternative are Pearson residuals which Ferrari and Cribari-Neto (2004) call standardized ordinary residuals and define as\n\\[\n  r_{\\mathrm{P}, i} = \\frac{y_i - \\hat{\\mu}_i}{\\sqrt{\\widehat{\\text{Var}}(y_i)}},\n\\tag{5}\\]\nwhere \\(\\widehat{\\text{Var}}(y_i) = \\hat{\\mu}_i(1-\\hat{\\mu}_i)/\n(1+\\hat{\\phi_i})\\), \\(\\hat{\\mu}_i = g_1^{-1}(x_i^\\top\\,\\hat{\\beta})\\), and \\(\\hat{\\phi}_i = g_2^{-1}(z_i^\\top\\,\\hat{\\gamma})\\). Similarly, deviance residuals can be defined in the standard way via signed contributions to the excess likelihood. Further residuals were proposed by Espinheira, Ferrari, and Cribari-Neto (2008b), in particular one residual with better properties that they named standardized weighted residual 2:\n\\[\n  r_{\\mathrm{sw2}, i} = \\frac{y^*_i - {\\hat{{\\mu}}^*}_i}{\\sqrt{\\hat{v}_i(1 - h_{ii})}},\n\\tag{6}\\]\nwhere \\(y_i^* = \\log\\{ y_i / (1-y_i)\\}\\) and \\(\\mu_i^* = \\psi(\\mu_i\\phi)- \\psi((1-\\mu_i)\\phi)\\), \\(\\psi(\\cdot)\\) denoting the digamma function. Standardization is then by \\(v_i = \\left\\{ \\psi'(\\mu_i\\phi) + \\psi'((1-\\mu_i)\\phi)\\right\\}\\) and \\(h_{ii}\\), the \\(i\\)th diagonal element of the hat matrix . Hats denote evaluation at the ML estimates.\nIt is noteworthy that the beta regression model described above was developed to allow practitioners to model continuous variates that assume values in the unit interval such as rates, proportions, and concentration or inequality indices (e.g., Gini). However, the data types that can be modeled using beta regressions also encompass proportions of “successes” from a number of trials, if the number of trials is large enough to justify a continuous model. In this case, beta regression is similar to a binomial generalized linear model (GLM) but provides some more flexibility – in particular when the trials are not independent and the standard binomial model might be too strict. In such a situation, the fixed dispersion beta regression is similar to the quasi-binomial model (McCullagh and Nelder 1989) but fully parametric. Furthermore, it can be naturally extended to variable dispersions."
  },
  {
    "objectID": "vignettes/betareg.html#sec-implementation",
    "href": "vignettes/betareg.html#sec-implementation",
    "title": "Beta Regression in R",
    "section": "\n3 Implementation in R",
    "text": "3 Implementation in R\nTo turn the conceptual model from the previous section into computational tools in R, it helps to emphasize some properties of the model: It is a standard maximum likelihood (ML) task for which there is no closed-form solution but numerical optimization is required. Furthermore, the model shares some properties (such as linear predictor, link function, dispersion parameter) with generalized linear models (GLMS, McCullagh and Nelder 1989), but it is not a special case of this framework (not even for fixed dispersion). There are various models with implementations in R that have similar features – here, we specifically reuse some of the ideas employed for generalized count data regression by Zeileis, Kleiber, and Jackman (2008).\nThe main model-fitting function in betareg is betareg() which takes a fairly standard approach for implementing ML regression models in R: formula plus data is used for model and data specification, then the likelihood and corresponding gradient (or estimating function) is set up, optim() is called for maximizing the likelihood, and finally an object of S3 class \"betareg\" is returned for which a large set of methods to standard generics is available. The workhorse function is betareg.fit() which provides the core computations without formula-related data pre- and post-processing. Update: Recently, betareg() has been extended to optionally include an additional Fisher scoring iteration after the optim() optimization in order to improve the ML result (or apply a bias correction or reduction).\nThe model-fitting function betareg() and its associated class are designed to be as similar as possible to the standard glm() function (R Core Team 2024) for fitting GLMs. An important difference is that there are potentially two equations for mean and precision (Equation 4), and consequently two regressor matrices, two linear predictors, two sets of coefficients, etc. In this respect, the design of betareg() is similar to the functions described by Zeileis, Kleiber, and Jackman (2008) for fitting zero-inflation and hurdle models which also have two model components. The arguments of betareg() are\n\nbetareg(formula, data, subset, na.action, weights, offset,\n  link = \"logit\", link.phi = NULL, control = betareg.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\nwhere the first line contains the standard model-frame specifications (see Chambers and Hastie 1992), the second line has the arguments specific to beta regression models and the arguments in the last line control some components of the return value.\n\n\nTable 1: Functions and methods for objects of class \"betareg\". The first 17 functions refer to methods, the last five are generic functions whose default methods work because of the information supplied by the methods above.\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nprint()\nSimple printed display with coefficient estimates\n\n\nsummary()\nStandard regression output (coefficient estimates, standard errors, partial Wald tests); returns an object of class \"summary.betareg\" containing the relevant summary statistics (which has a print() method)\n\n\ncoef()\nExtract coefficients of model (full, mean, or precision components), a single vector of all coefficients by default\n\n\nvcov()\nAssociated covariance matrix (with matching names)\n\n\npredict()\nPredictions (of means \\(\\mu_i\\), linear predictors \\(\\eta_{1i}\\), precision parameter \\(\\phi_i\\), or variances \\(\\mu_i (1 - \\mu_i) / (1 + \\phi_i)\\)) for new data\n\n\nfitted()\nFitted means for observed data\n\n\nresiduals()\nExtract residuals (deviance, Pearson, response, quantile, or different weighted residuals), see Espinheira, Ferrari, and Cribari-Neto (2008b), defaulting to quantile residuals since version 3.2-0\n\n\nestfun()\nCompute empirical estimating functions (or score functions), evaluated at observed data and estimated parameters, see Zeileis (2006b)\n\n\n\nbread()\nExtract “bread” matrix for sandwich estimators, see Zeileis (2006b)\n\n\n\nterms()\nExtract terms of model components\n\n\nmodel.matrix()\nExtract model matrix of model components\n\n\nmodel.frame()\nExtract full original model frame\n\n\nlogLik()\nExtract fitted log-likelihood\n\n\nplot()\nDiagnostic plots of residuals, predictions, leverages etc.\n\n\nhatvalues()\nHat values (diagonal of hat matrix)\n\n\ncooks.distance()\n(Approximation of) Cook’s distance\n\n\ngleverage()\nCompute generalized leverage, see Wei, Hu, and Fung (1998) and Rocha and Simas (2010)\n\n\n\ncoeftest()\nPartial Wald tests of coefficients\n\n\nwaldtest()\nWald tests of nested models\n\n\nlinear.hypothesis()\nWald tests of linear hypotheses\n\n\nlrtest()\nLikelihood ratio tests of nested models\n\n\nAIC()\nCompute information criteria (AIC, BIC, etc.)\n\n\n\n\n\n\nIf a formula of type y ~ x1 + x2 is supplied, it describes \\(y_i\\) and \\(x_i\\) for the mean equation of the beta regression (Equation 4). In this case a constant \\(\\phi_i\\) is assumed, i.e., \\(z_i = 1\\) and \\(g_2\\) is the identity link, corresponding to the basic beta regression model as introduced in Ferrari and Cribari-Neto (2004). However, a second set of regressors can be specified by a two-part formula of type y ~ x1 + x2 | z1 + z2 + z3 as provided in the Formula package (Zeileis and Croissant 2010). This model has the same mean equation as above but the regressors \\(z_i\\) in the precision equation (Equation 4) are taken from the ~ z1 + z2 + z3 part. The default link function in this case is the log link \\(g_2(\\cdot) = \\log(\\cdot)\\). Consequently, y ~ x1 + x2 and y ~ x1 + x2 | 1 correspond to equivalent beta likelihoods but use different parametrizations for \\(\\phi_i\\): simply \\(\\phi_i = \\gamma_1\\) in the former case and \\(\\log(\\phi_i) = \\gamma_1\\) in the latter case. The link for the \\(\\phi_i\\) precision equation can be changed by link.phi in both cases where \"identity\", \"log\", and \"sqrt\" are allowed as admissible values. The default for the \\(\\mu_i\\) mean equation is always the logit link but all link functions for the binomial family in glm() are allowed as well as the log-log link: \"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\", and \"loglog\".\nML estimation of all parameters employing analytical gradients is carried out using R’s optim() with control options set in betareg.control(). All of optim()’s methods are available but the default is\"BFGS\", which is typically regarded to be the best-performing method (Mittelhammer, Judge, and Miller 2000, sec. 8.13) with the most effective updating formula of all quasi-Newton methods (Nocedal and Wright 1999, 197). Starting values can be user-supplied, otherwise the \\(\\beta\\) starting values are estimated by a regression of \\(g_1(y_i)\\) on \\(x_i\\). The starting values for the \\(\\gamma\\) intercept are chosen as described in Ferrari and Cribari-Neto (2004, 805), corresponding to a constant \\(\\phi_i\\) (plus a link transformation, if any). All further \\(\\gamma\\) coefficients (if any) are initially set to zero. The covariance matrix estimate is derived analytically as in Simas, Barreto-Souza, and Rocha (2010). However, by setting hessian = TRUE the numerical Hessian matrix returned by optim() can also be obtained. Update: In recent versions of betareg, the optim() is still performed but optionally it may be complemented by a subsequent additional Fisher scoring iteration to improve the result.\nThe returned fitted-model object of class \"betareg\" is a list similar to \"glm\" objects. Some of its elements – such as coefficients or terms – are lists with a mean and precision component, respectively.\nA set of standard extractor functions for fitted model objects is available for objects of class \"betareg\", including the usual summary() method that includes partial Wald tests for all coefficients. No anova() method is provided, but the general coeftest() and waldtest() from lmtest (Zeileis and Hothorn 2002), and linear.hypothesis() from car (Fox and Weisberg 2019) can be used for Wald tests while lrtest() from lmtest provides for likelihood-ratio tests of nested models. See Table 1 for a list of all available methods. Most of these are standard in base R, however, methods to a few less standard generics are also provided. Specifically, there are tools related to specification testing and computation of sandwich covariance matrices as discussed by Zeileis (2004),betareg:Zeileis:2006a as well as a method to a new generic for computing generalized leverages (Wei, Hu, and Fung 1998)."
  },
  {
    "objectID": "vignettes/betareg.html#sec-illustrations",
    "href": "vignettes/betareg.html#sec-illustrations",
    "title": "Beta Regression in R",
    "section": "\n4 Beta regression in practice",
    "text": "4 Beta regression in practice\nTo illustrate the usage of betareg in practice we replicate and slightly extend some of the analyses from the original papers that suggested the methodology. More specifically, we estimate and compare various flavors of beta regression models for the gasoline yield data of Prater (1956), see Figure 2, and for the household food expenditure data taken from Griffiths, Hill, and Judge (1993), see Figure 4). Further pure replication exercises are provided in Section 5.\n\n4.1 The basic model: Estimation, inference, diagnostics\n\n4.1.1 Prater’s gasoline yield data\nThe basic beta regression model as suggested by Ferrari and Cribari-Neto (2004) is illustrated in Section 4 of their paper using two empirical examples. The first example employs the well-known gasoline yield data taken from Prater (1956). The variable of interest is yield, the proportion of crude oil converted to gasoline after distillation and fractionation, for which a beta regression model is rather natural. Ferrari and Cribari-Neto (2004) employ two explanatory variables: temp, the temperature (in degrees Fahrenheit) at which all gasoline has vaporized, and batch, a factor indicating ten unique batches of conditions in the experiments (depending on further variables). The data, encompassing 32 observations, is visualized in Figure 2.\nFerrari and Cribari-Neto (2004) start out with a model where yield depends on batch and temp, employing the standard logit link. In betareg, this can be fitted via\n\ndata(\"GasolineYield\", package = \"betareg\")\ngy_logit &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\nsummary(gy_logit)\n## \n## Call:\n## betareg(formula = yield ~ batch + temp, data = GasolineYield)\n## \n## Quantile residuals:\n##    Min     1Q Median     3Q    Max \n## -2.140 -0.570  0.120  0.704  1.751 \n## \n## Coefficients (mean model with logit link):\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\n## batch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\n## batch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\n## batch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\n## batch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\n## batch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\n## batch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\n## batch7       0.543692   0.109127    4.98  6.3e-07 ***\n## batch8       0.495901   0.108926    4.55  5.3e-06 ***\n## batch9       0.385793   0.118593    3.25   0.0011 ** \n## temp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n## \n## Phi coefficients (precision model with identity link):\n##       Estimate Std. Error z value Pr(&gt;|z|)    \n## (phi)      440        110       4  6.3e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n## \n## Type of estimator: ML (maximum likelihood)\n## Log-likelihood: 84.8 on 12 Df\n## Pseudo R-squared: 0.962\n## Number of iterations: 51 (BFGS) + 3 (Fisher scoring)\n\nwhich replicates their Table 1. The goodness of fit is assessed using different types of diagnostic displays shown in their Figure 2. This graphic can be reproduced (in a slightly different order) using the plot() method for \"betareg\" objects, see Figure 3.\n\n\n\n\n\n\n\nFigure 2: Gasoline yield data from Prater (1956): Proportion of crude oil converted to gasoline explained by temperature (in degrees Fahrenheit) at which all gasoline has vaporized and given batch (indicated by gray level). Fitted curves correspond to beta regressions gy_loglog with log-log link (solid, red) and gy_logit with logit link (dashed, blue). Both curves were evaluated at varying temperature with the intercept for batch 6 (i.e., roughly the average intercept).\n\n\n\n\n\npar(mfrow = c(3, 2))\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(123)\nplot(gy_logit, which = 1:4, type = \"pearson\")\nplot(gy_logit, which = 5, type = \"deviance\", sub.caption = \"\")\nplot(gy_logit, which = 1, type = \"deviance\", sub.caption = \"\")\n\n\n\n\n\n\nFigure 3: Diagnostic plots for beta regression model gy_logit.\n\n\n\n\nAs observation 4 corresponds to a large Cook’s distance and large residual, Ferrari and Cribari-Neto (2004) decided to refit the model excluding this observation. While this does not change the coefficients in the mean model very much, the precision parameter \\(\\phi\\) increases clearly.\n\ngy_logit4 &lt;- update(gy_logit, subset = -4)\ncoef(gy_logit, model = \"precision\")\n##  (phi) \n## 440.28\ncoef(gy_logit4, model = \"precision\")\n##  (phi) \n## 577.79\n\n\n4.1.2 Household food expenditures\nFerrari and Cribari-Neto (2004) also consider a second example: household food expenditure data for 38 households taken from Griffiths, Hill, and Judge (1993) (Table 15.4). The dependent variable is food/income, the proportion of household income spent on food. Two explanatory variables are available: the previously mentioned household income and the number of persons living in the household. All three variables are visualized in Figure 4.\nTo start their analysis, Ferrari and Cribari-Neto (2004) consider a simple linear regression model fitted by ordinary least squares (OLS):\n\ndata(\"FoodExpenditure\", package = \"betareg\")\nfe_lm &lt;- lm(I(food/income) ~ income + persons, data = FoodExpenditure)\n\nTo show that this model exhibits heteroskedasticity, they employ the studentized Breusch and Pagan (1979) test of Koenker (1981) which is available in R in the lmtest package (Zeileis and Hothorn 2002).\n\nlibrary(\"lmtest\")\nbptest(fe_lm)\n## \n##  studentized Breusch-Pagan test\n## \n## data:  fe_lm\n## BP = 5.93, df = 2, p-value = 0.051\n\nOne alternative would be to consider a logit-transformed response in a traditional OLS regression but this would make the residuals asymmetric. However, both issues – heteroskedasticity and skewness – can be alleviated when a beta regression model with a logit link for the mean is used.\n\nfe_beta &lt;- betareg(I(food/income) ~ income + persons,\n  data = FoodExpenditure)\nsummary(fe_beta)\n## \n## Call:\n## betareg(formula = I(food/income) ~ income + persons, data = FoodExpenditure)\n## \n## Quantile residuals:\n##    Min     1Q Median     3Q    Max \n## -2.533 -0.460  0.170  0.642  1.773 \n## \n## Coefficients (mean model with logit link):\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -0.62255    0.22385   -2.78   0.0054 ** \n## income      -0.01230    0.00304   -4.05  5.1e-05 ***\n## persons      0.11846    0.03534    3.35   0.0008 ***\n## \n## Phi coefficients (precision model with identity link):\n##       Estimate Std. Error z value Pr(&gt;|z|)    \n## (phi)    35.61       8.08    4.41    1e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n## \n## Type of estimator: ML (maximum likelihood)\n## Log-likelihood: 45.3 on 4 Df\n## Pseudo R-squared: 0.388\n## Number of iterations: 28 (BFGS) + 4 (Fisher scoring)\n\nThis replicates Table 2 from Ferrari and Cribari-Neto (2004). The predicted means of the linear and the beta regression model, respectively, are very similar: the proportion of household income spent on food decreases with the overall income level but increases in the number of persons in the household (see also Figure 4).\nBelow, further extended models will be considered for these data sets and hence all model comparisons are deferred.\n\n\n\n\n\n\n\nFigure 4: Household food expenditure data from Griffiths, Hill, and Judge (1993): Proportion of household income spent on food explained by household income and number of persons in household (indicated by gray level). Fitted curves correspond to beta regressions fe_beta with fixed dispersion (long-dashed, blue), fe_beta2 with variable dispersion (solid, red), and the linear regression fe_lin (dashed, black). All curves were evaluated at varying income with the intercept for mean number of persons ($ = r round(mean(FoodExpenditure$persons), digits = 2)$).\n\n\n\n\n\n4.2 Variable dispersion model\n\n4.2.1 Prater’s gasoline yield data\nAlthough the beta model already incorporates naturally a certain pattern in the variances of the response (see Equation 2), it might be necessary to incorporate further regressors to account for heteroskedasticity as in Equation 4. For illustration of this approach, the example from Section 3 of the online supplements to Simas, Barreto-Souza, and Rocha (2010) is considered. This investigates Prater’s gasoline yield data based on the same mean equation as above, but now with temperature temp as an additional regressor for the precision parameter \\(\\phi_i\\):\n\ngy_logit2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\nfor which summary(gy_logit2) yields the MLE column in Table 19 of Simas, Barreto-Souza, and Rocha (2010). To save space, only the parameters pertaining to \\(\\phi_i\\) are reported here\n\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  1.36409    1.22578    1.11     0.27    \n## temp         0.01457    0.00362    4.03  5.7e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwhich signal a significant improvement by including the temp regressor. Instead of using this Wald test, the models can also be compared by means of a likelihood-ratio test (see their Table 18) that confirms the results:\n\nlrtest(gy_logit, gy_logit2)\n## Likelihood ratio test\n## \n## Model 1: yield ~ batch + temp\n## Model 2: yield ~ batch + temp | temp\n##   #Df LogLik Df Chisq Pr(&gt;Chisq)  \n## 1  12   84.8                      \n## 2  13   87.0  1  4.36      0.037 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNote that this can also be interpreted as testing the null hypothesis of equidispersion against a specific alternative of variable dispersion.\n\n4.2.2 Household food expenditures\nFor the household food expenditure data, the Breusch-Pagan test carried out above illustrated that there is heteroskedasticity that can be captured by the regressors income and persons. Closer investigation reveals that this is mostly due to the number of persons in the household, also brought out graphically by some of the outliers with high values in this variable in Figure 4. Hence, it seems natural to consider the model employed above with persons as an additional regressor in the precision equation.\n\nfe_beta2 &lt;- betareg(I(food/income) ~ income + persons | persons,\n  data = FoodExpenditure)\n\nThis leads to significant improvements in terms of the likelihood and the associated BIC.2\n\nlrtest(fe_beta, fe_beta2)\n## Likelihood ratio test\n## \n## Model 1: I(food/income) ~ income + persons\n## Model 2: I(food/income) ~ income + persons | persons\n##   #Df LogLik Df Chisq Pr(&gt;Chisq)   \n## 1   4   45.3                       \n## 2   5   49.2  1   7.7     0.0055 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAIC(fe_beta, fe_beta2, k = log(nrow(FoodExpenditure)))\n##          df     AIC\n## fe_beta   4 -76.117\n## fe_beta2  5 -80.182\n\nThus, there is evidence for variable dispersion and model fe_beta2 seems to be preferable. As visualized in Figure 4, it describes a similar relationship between response and explanatory variables although with a somewhat shrunken income slope.\n\n4.3 Selection of different link functions\n\n4.3.1 Prater’s gasoline yield data\nAs in binomial GLMs, selection of an appropriate link function can greatly improve the model fit (McCullagh and Nelder 1989), especially if extreme proportions (close to \\(0\\) or \\(1\\)) have been observed in the data. To illustrate this problem in beta regressions, we replicate parts of the analysis in Section 5 of Cribari-Neto and Lima (2007). This reconsiders Prater’s gasoline yield data but employs a log-log link instead of the previously used (default) logit link\n\ngy_loglog &lt;- betareg(yield ~ batch + temp, data = GasolineYield,\n  link = \"loglog\")\n\nwhich clearly improves the pseudo \\(R^2\\) of the model:\n\nsummary(gy_logit)$pseudo.r.squared\n## [1] 0.96173\nsummary(gy_loglog)$pseudo.r.squared\n## [1] 0.98523\n\nSimilarly, the AIC3 (and BIC) of the fitted model is not only superior to the logit model with fixed dispersion gy_logit but also to the logit model with variable dispersion gy_logit2 considered in the previous section.\n\nAIC(gy_logit, gy_logit2, gy_loglog)\n##           df     AIC\n## gy_logit  12 -145.60\n## gy_logit2 13 -147.95\n## gy_loglog 12 -168.31\n\nMoreover, if temp were included as a regressor in the precision equation of gy_loglog, it would no longer yield significant improvements. Thus, improvement of the model fit in the mean equation by adoption of the log-log link has waived the need for a variable precision equation.\nTo underline the appropriateness of the log-log specification, Cribari-Neto and Lima (2007) consider a sequence of diagnostic tests inspired by the RESET [regression specification error test; Ramsey (1969)] in linear regression models. To check for misspecifications, they consider powers of fitted means or linear predictors to be included as auxiliary regressors in the mean equation. In well-specified models, these should not yield significant improvements. For the gasoline yield model, this can only be obtained for the log-log link while all other link functions result in significant results indicating misspecification. Below, this is exemplified for a likelihood-ratio test of squared linear predictors. Analogous results can be obtained for type = \"response\" or higher powers.\n\nlrtest(gy_logit, . ~ . + I(predict(gy_logit, type = \"link\")^2))\n## Likelihood ratio test\n## \n## Model 1: yield ~ batch + temp\n## Model 2: yield ~ batch + temp + I(predict(gy_logit, type = \"link\")^2)\n##   #Df LogLik Df Chisq Pr(&gt;Chisq)    \n## 1  12   84.8                        \n## 2  13   96.0  1  22.4    2.2e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlrtest(gy_loglog, . ~ . + I(predict(gy_loglog, type = \"link\")^2))\n## Likelihood ratio test\n## \n## Model 1: yield ~ batch + temp\n## Model 2: yield ~ batch + temp + I(predict(gy_loglog, type = \"link\")^2)\n##   #Df LogLik Df Chisq Pr(&gt;Chisq)\n## 1  12   96.2                    \n## 2  13   97.0  1  1.67        0.2\n\nThe improvement of the model fit can also be brought out graphically by comparing absolute raw residuals (i.e., \\(y_i - \\hat \\mu_i\\)) from both models as in Figure 5.\n\n\n\n\n\n\n\nFigure 5: Scatterplot comparing the absolute raw residuals from beta regression modes with log-log link (x-axis) and logit link (y-axis).\n\n\n\n\nThis shows that there are a few observations clearly above the diagonal (where the log-log-link fits better than the logit link) whereas there are fewer such observations below the diagonal. A different diagnostic display that is useful in this situation (and is employed by Cribari-Neto and Lima 2007) is a plot of predicted values (\\(\\hat \\mu_i\\)) vs. observed values (\\(y_i\\)) for each model. This can be created by plot(gy_logit, which = 6) and plot(gy_loglog, which = 6), respectively.\nIn principle, the link function \\(g_2\\) in the precision equation could also influence the model fit. However, as the best-fitting model gy_loglog has a constant \\(\\phi\\), all links \\(g_2\\) lead to equivalent estimates of \\(\\phi\\) and thus to equivalent fitted log-likelihoods. However, the link function can have consequences in terms of the inference about \\(\\phi\\) and in terms of convergence of the optimization. Typically, a log-link leads to somewhat improved quadratic approximations of the likelihood and less iterations in the optimization. For example, refitting gy_loglog with \\(g_2(\\cdot) = \\log(\\cdot)\\) converges more quickly:\n\ngy_loglog2 &lt;- update(gy_loglog, link.phi = \"log\")\nsummary(gy_loglog2)$iterations\n##   optim scoring \n##      21       2\n\nwith a lower number of iterations than for gy_loglog which had 51, 2 iterations.\n\n4.3.2 Household food expenditures\nOne could conduct a similar analysis as above for the household food expenditure data. However, as the response takes less extreme observations than for the gasoline yield data, the choice of link function is less important. In fact, refitting the model with various link functions shows no large differences in the resulting log-likelihoods. These can be easily extracted from fitted models using the logLik() function, e.g., logLik(fe_beta2). Below we use a compact sapply() call to obtain this for updated versions of fe_beta2 with all available link functions.\n\nsapply(c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"loglog\"),\n  function(x) logLik(update(fe_beta2, link = x)))\n##   logit  probit cloglog cauchit  loglog \n##  49.185  49.080  49.359  50.011  48.867\n\nOnly the Cauchy link performs somewhat better than the logit link and might hence deserve further investigation."
  },
  {
    "objectID": "vignettes/betareg.html#sec-replications",
    "href": "vignettes/betareg.html#sec-replications",
    "title": "Beta Regression in R",
    "section": "\n5 Further replication exercises",
    "text": "5 Further replication exercises\nIn this section, further empirical illustrations of beta regressions are provided. While the emphasis in the previous section was to present how the various features of betareg can be used in pracice, we focus more narrowly on replication of previously published research articles below.\n\n5.1 Dyslexia and IQ predicting reading accuracy\nWe consider an application that analyzes reading accuracy data for nondyslexic and dyslexic Australian children (Smithson and Verkuilen 2006).\nThe variable of interest is accuracy providing the scores on a test of reading accuracy taken by 44 children, which is predicted by the two regressors dyslexia (a factor with sum contrasts separating a dyslexic and a control group) and nonverbal intelligent quotient (iq, converted to \\(z\\) scores), see Figure 6 for a visualization. The sample includes 19 dyslexics and 25 controls who were recruited from primary schools in the Australian Capital Territory. The children’s ages ranged from eight years five months to twelve years three months; mean reading accuracy was 0.606 for dyslexic readers and 0.900 for controls.\nSmithson and Verkuilen (2006) set out to investigate whether dyslexia contributes to the explanation of accuracy even when corrected for iq score (which is on average lower for dyslexics). Hence, they consider separate regressions for the two groups fitted by the interaction of both regressors. To show that OLS regression is no suitable tool in this situation, they first fit a linear regression of the logit-transformed response:\n\ndata(\"ReadingSkills\", package = \"betareg\")\nrs_ols &lt;- lm(qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)\ncoeftest(rs_ols)\n## \n## t test of coefficients:\n## \n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    1.601      0.226    7.09  1.4e-08 ***\n## dyslexia      -1.206      0.226   -5.34  4.0e-06 ***\n## iq             0.359      0.225    1.59    0.119    \n## dyslexia:iq   -0.423      0.225   -1.88    0.068 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe interaction effect does not appear to be significant, however this is a result of the poor fit of the linear regression as will be shown below. Figure 6 clearly shows that the data are asymmetric and heteroskedastic (especially in the control group). Hence, Smithson and Verkuilen (2006) fit a beta regression model, again with separate means for both groups, but they also allow the dispersion to depend on the main effects of both variables.\n\nrs_beta &lt;- betareg(accuracy ~ dyslexia * iq | dyslexia + iq,\n  data = ReadingSkills, hessian = TRUE)\ncoeftest(rs_beta)\n## \n## z test of coefficients:\n## \n##                   Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)          1.123      0.151    7.44  9.8e-14 ***\n## dyslexia            -0.742      0.151   -4.90  9.7e-07 ***\n## iq                   0.486      0.167    2.91  0.00360 ** \n## dyslexia:iq         -0.581      0.173   -3.37  0.00076 ***\n## (phi)_(Intercept)    3.304      0.227   14.59  &lt; 2e-16 ***\n## (phi)_dyslexia       1.747      0.294    5.94  2.8e-09 ***\n## (phi)_iq             1.229      0.460    2.67  0.00749 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThis shows that precision increases with iq and is lower for controls while in the mean equation there is a significant interaction between iq and dyslexia. As Figure 6 illustrates, the beta regression fit does not differ much from the OLS fit for the dyslexics group (with responses close to \\(0.5\\)) but fits much better in the control group (with responses close to \\(1\\)).\nThe estimates above replicate those in Table 5 of Smithson and Verkuilen (2006), except for the signs of the coefficients of the dispersion submodel which they defined in the opposite way. Note that their results have been obtained with numeric rather than analytic standard errors hence hessian = TRUE is set above for replication. The results are also confirmed by Espinheira, Ferrari, and Cribari-Neto (2008a), who have also concluded that the dispersion is variable. As pointed out in Section 4.2, to formally test equidispersion against variable dispersion lrtest(rs_beta, . ~ . | 1) (or the analogous waldtest()) can be used.\nSmithson and Verkuilen (2006) also consider two other psychometric applications of beta regressions the data for which are also provided in the betareg package: see ?MockJurors and ?StressAnxiety. Furthermore, demo(\"SmithsonVerkuilen2006\", package = \"betareg\") is a complete replication script with comments.\n\n\n\n\n\n\n\nFigure 6: Reading skills data from Smithson and Verkuilen (2006) : Linearly transformed reading accuracy by IQ score and dyslexia status (control, blue vs. dyslexic, red). Fitted curves correspond to beta regression rs_beta (solid) and OLS regression with logit-transformed dependent variable rs_ols (dashed).\n\n\n\n\n\n5.2 Structural change testing in beta regressions\nAs already illustrated in Section 4, \"betareg\" objects can be plugged into various inference functions from other packages because they provide suitable methods to standard generic functions (see Table 1). Hence lrtest() could be used for performing likelihood-ratio testing inference and similarly coeftest(), waldtest() from lmtest (Zeileis and Hothorn 2002) and linear.hypothesis() from car (Fox and Weisberg 2019) can be employed for carrying out different flavors of Wald tests.\nIn this section, we illustrate yet another generic inference approach implemented in the strucchange package for structural change testing. This is concerned with a different testing problem compared to the functions above: It assesses whether the model parameters are stable throughout the entire sample or whether they change over the observations \\(i = 1, \\dots, n\\). This is of particular interest in time series applications where the regression coefficients \\(\\beta\\) and \\(\\gamma\\) change at some unknown time in the sample period (see Zeileis 2006a for more details and references to the literature).\nWhile originally written for linear regression models (Zeileis et al. 2002), strucchange was extended by Zeileis (2006a) to compute generalized fluctuation tests for structural change in models that are based on suitable estimating functions. The idea is to capture systematic deviations from parameter stability by cumulative sums of the empirical estimating functions: If the parameters are stable, the cumulative sum process should fluctuate randomly around zero. However, if there is an abrupt shift in the parameters, the cumulative sums will deviate clearly from zero and have a peak at around the time of the shift. If the estimating functions can be extracted by an estfun() method (as for \"betareg\" objects), models can simply be plugged into the gefp() function for computing these cumulative sums (also known as generalized empirical fluctuation processes). To illustrate this, we replicate the example from Section 5.3 in Zeileis (2006a).\nTwo artificial data sets are considered: a series y1 with a change in the mean \\(\\mu\\), and a series y2 with a change in the precision \\(\\phi\\). Both simulated series start with the parameters \\(\\mu = 0.3\\) and \\(\\phi = 4\\) and for the first series \\(\\mu\\) changes to \\(0.5\\) after 75% of the observations while \\(\\phi\\) remains constant whereas for the second series \\(\\phi\\) changes to \\(8\\) after 50% of the observations and \\(\\mu\\) remains constant.\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(123)\ny1 &lt;- c(rbeta(150, 0.3 * 4, 0.7 * 4), rbeta(50, 0.5 * 4, 0.5 * 4))\ny2 &lt;- c(rbeta(100, 0.3 * 4, 0.7 * 4), rbeta(100, 0.3 * 8, 0.7 * 8))\n\nTo capture instabilities in the parameters over “time” (i.e., the ordering of the observations), the generalized empirical fluctuation processes can be derived via\n\nlibrary(\"strucchange\")\ny1_gefp &lt;- gefp(y1 ~ 1, fit = betareg)\ny2_gefp &lt;- gefp(y2 ~ 1, fit = betareg)\n\nand visualized by\n\nplot(y1_gefp, aggregate = FALSE)\n\n\n\n\n\n\nFigure 7: Structural change tests for artificial data y1 with change in \\(\\mu\\).\n\n\n\n\n\nplot(y2_gefp, aggregate = FALSE)\n\n\n\n\n\n\nFigure 8: Structural change tests for artificial data y2 with change in \\(\\phi\\).\n\n\n\n\nThe resulting Figure 7 and Figure 8 replicate Figure 4 from Zeileis (2006a) and show two 2-dimensional fluctuation processes: one for y1 and one for y2. Both fluctuation processes behave as expected: There is no excessive fluctuation of the process pertaining to the parameter that remained constant while there is a clear peak at about the time of the change in the parameter with the shift. In both series the structural change is significant due to the crossing of the red boundary that corresponds to the 5% critical value. For further details see Zeileis (2006a)."
  },
  {
    "objectID": "vignettes/betareg.html#sec-conclusion",
    "href": "vignettes/betareg.html#sec-conclusion",
    "title": "Beta Regression in R",
    "section": "\n6 Summary",
    "text": "6 Summary\nThis paper addressed the R implementation of the class of beta regression models available in the betareg package. We have presented the fixed and variable dispersion beta regression models, described how one can model rates and proportions using betareg and presented several empirical examples reproducing previously published results. Future research and implementation shall focus on the situation where the data contain zeros and/or ones (see e.g., Kieschnick and McCullough 2003). An additional line of research and implementation is that of dynamic beta regression models, such as the class of \\(\\beta\\)ARMA models proposed by Rocha and Cribari-Neto (2009)."
  },
  {
    "objectID": "vignettes/betareg.html#acknowledgments",
    "href": "vignettes/betareg.html#acknowledgments",
    "title": "Beta Regression in R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nFCN gratefully acknowledges financial support from CNPq/Brazil. Both authors are grateful to A.B. Simas and A.V. Rocha for their work on the previous versions of the betareg package (up to version 1.2). Furthermore, detailed and constructive feedback from two anonymous reviewers, the associated editor, as well as from B. Grün was very helpful for enhancing both software and manuscript."
  },
  {
    "objectID": "vignettes/betareg.html#footnotes",
    "href": "vignettes/betareg.html#footnotes",
    "title": "Beta Regression in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nA beta regression model based on this parameterization was proposed by Vasconcellos and Cribari-Neto (2005). We shall, however, focus on the parameterization indexed by the mean and a precision parameter.↩︎\nIn R, the BIC can be computed by means of AIC() when \\(\\log(n)\\) is supplied as the penalty term k.↩︎\nNote that Cribari-Neto and Lima (2007) did not account for estimation of \\(\\phi\\) in their degrees of freedom. Hence, their reported AICs differ by 2.↩︎"
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "betareg 3.2-2",
    "section": "",
    "text": "betareg 3.2-2\n\nAdded family functions for generalized additive models for location, scale, and shape (GAMLSS) with package bamlss or gamlss2 using frequentist and Bayesian estimation, respectively. Standard beta regression using the parameterization with mu and phi is provided by betar_family() and xbetax_family() provides the new extended-support beta mixture (XBX) regression.\nImproved vignette source to consistently use pipe comments and avoid duplicated figure labels.\n\n\n\nbetareg 3.2-1\n\nNew working paper “Extended-Support Beta Regression for [0, 1] Responses” by Ioannis Kosmidis and Achim Zeileis in the arXiv.org E-Print Archive, doi:10.48550/arXiv.2409.07233.\nNew package web page (via altdoc/quarto) at https://topmodels.R-Forge.R-project.org/betareg/\nExtended functionality of predict() method for betareg objects and enhanced the corresponding documentation, see ?predict.betareg.\nTurned vignette(\"betareg\", package = \"betareg\") and vignette(\"betareg-ext\", package = \"betareg\") from Sweave into Quarto vignettes. Some improvements/updates in the text.\n\n\n\nbetareg 3.2-0\n\nMajor extension in betareg(): In addition to classic beta regression for responses in the open interval (0, 1), extended-support beta regression is added which can model responses in the closed interval [0, 1] (i.e., including boundary observations at 0 and/or 1). This is accomplished by adding two new response distributions: The extended-support beta distribution (\"xbeta\") leverages an underlying symmetric four-parameter beta distribution with exceedence parameter nu to obtain support [-nu, 1 + nu] that is subsequently censored to [0, 1] in order to obtain point masses at the boundary values 0 and 1. The extended-support beta mixture distribution (\"xbetax\") is a continuous mixture of extended-support beta distributions where the exceedence parameter follows an exponential distribution with mean nu (rather than a fixed value of nu). The latter \"xbetax\" specification is used by default in case of boundary observations at 0 and/or 1. The \"xbeta\" specification with fixed nu is mostly for testing and debugging purposes.\nQuantile residuals are added to the residuals() method for betareg objects. They are easy to compute and have good distributional properties. Hence, they are the new default residuals.\nBug fix in pseudo.r.squared computation for weighted models where previously the weights were erroneously ignored (reported by Ray Tayek).\nBug fixes in betatree(): Split points were computed incorrectly due to wrong sign of the log-likelihood (reported by Se-Wan Jeong). And trees with only intercepts for both mu and phi could not be fitted (reported by Ludwig Hothorn).\n\n\n\nbetareg 3.1-4\n\nIn betatree() the \"xlevels\" attribute from partykit::mob is now correctly stored in $levels (rather than $xlevels) of the returned object.\n\n\n\nbetareg 3.1-3\n\nAdded IGNORE_RDIFF flags in some examples in order to avoid showing diffs due to small numeric deviations in some checks (especially on CRAN).\n\n\n\nbetareg 3.1-2\n\nAdded suppressWarnings(RNGversion(\"3.5.0\")) in those places where set.seed() was used to assure exactly reproducible results from R 3.6.0 onwards.\n\n\n\nbetareg 3.1-1\n\nConditional registration of sctest() method for betatree objects when strucchange package is loaded.\n\n\n\nbetareg 3.1-0\n\nThe betatree() function now uses the new mob() implementation from the partykit package (instead of the old party package). The user interface essentially remained the same but now many more options are available through the new mob() function. The returned model object is now inheriting from modelparty/party.\nIncluded grDevices in Imports.\nFixed model.frame() method for betareg objects which do not store the model frame in $model.\nbetamix() gained arguments weights (case weights for observations) and offset (for the mean linear predictor).\n\n\n\nbetareg 3.0-5\n\nThe Formula package is now only in Imports but not Depends (see below).\nMethod FLXgetModelmatrix for FLXMRbeta objects modified due to changes in flexmix 2.3.12.\n\n\n\nbetareg 3.0-4\n\nFor some datasets betareg() would just “hang” because dbeta() “hangs” for certain extreme parameter combinations (in current R versions). betareg() now tries to catch these cases in order to avoid the problem.\nDepends/Imports/Suggests have been rearranged to conform with current CRAN check policies. This is the last version of betareg to have the Formula package in Depends - from the next version onwards it will only be in Imports.\n\n\n\nbetareg 3.0-3\n\nThe predict() method gained support for type = \"quantile\", so that quantiles of the response distribution can be predicted.\nThe Formula package is now not only in the list of dependencies but is also imported in the NAMESPACE, in order to facilitate importing betareg in other packages.\n\n\n\nbetareg 3.0-2\n\nAvoid .Call()-ing logit link functions directly, instead use elements of make.link(\"logit\").\n\n\n\nbetareg 3.0-1\n\nSmall consistency updates in labeling coefficients for current R-devel.\n\n\n\nbetareg 3.0-0\n\nNew release accompanying the second JSS paper: “Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned” by Gruen, Kosmidis, and Zeileis which appears as Journal of Statistical Software 48(11). See also citation(\"betareg\"). The paper presents the recently introduced features: bias correction/reduction in betareg(), recursive partitioning via betatree(), and finite mixture modeling via betamix(). See also vignette(\"betareg-ext\",   package = \"betareg\") for the vignette version within the package.\n\n\n\nbetareg 2.4-1\n\nFormula interface for betamix() changed to allow for three parts in the right hand side where the third part relates to the concomitant variables.\nModified the internal structure of vignettes/tests. The original vignettes are now moved to the vignettes directory, containing also .Rout.save files. Similarly, an .Rout.save for the examples is added in the tests directory.\n\n\n\nbetareg 2.4-0\n\nSupport bias-corrected (BC) and bias-reduced (BR) maximum likelihood estimation of beta regressions. See the type argument of betareg(). To enable BC/BR, an additional Fisher scoring iteration was added that (by default) also enhances the usual ML results.\nNew vignette(\"betareg-ext\", package = \"betareg\") introducing BC/BR estimation along with the recent additions beta regression trees and latent class beta regression (aka finite mixture beta regression models).\nEnabled fitting of beta regression models without coefficients in the mean equation.\nEnabled usage of offsets in both parts of the model, i.e., one can use betareg(y ~ x + offset(o1) | z + offset(o2)) which is also equivalent to betareg(y ~ x | z + offset(o2), offset = o1), i.e., the offset argument of betareg is employed for the mean equation only. Consequently, betareg_object$offset is now a list with two elements (mean/precision).\nAdded warning and ad-hoc workaround in the starting value selection of betareg.fit() for the precision model. If no valid starting value can be obtained, a warning is issued and c(1, 0, ..., 0) is employed.\nAdded betareg_object$nobs in the return object containing the number of observations with non-zero weights. Then nobs() can be used to extract this and consequently BIC() can be used to compute the BIC.\n\n\n\nbetareg 2.3-0\n\nNew betatree() function for beta regression trees based on model-based recursive partitioning. betatree() leverages the mob() function from the party package. For enabling this plug-in, a StatModel constructor betaReg() is provided based on the modeltools package.\nNew betamix() function for latent class beta regression, or finite mixture beta regression models. betamix() leverages the flexmix() function from the flexmix package. For enabling this plug-in, the driver FLXMRbeta() is provided.\nAdded tests/vignette-betareg.R based on the models fitted in vignette(\"betareg\", package = \"betareg\").\n\n\n\nbetareg 2.2-3\n\nThe \"levels\" element of a betareg object is now a list with components \"mean\", \"precision\", and \"full\" to match the \"terms\" of the object.\nImproved data handling bug in predict() method.\n\n\n\nbetareg 2.2-2\n\nDocumentation updates for ?gleverage.\n\n\n\nbetareg 2.2-1\n\nPackage now published in Journal of Statistical Software, see https://www.jstatsoft.org/v34/i02/ and citation(\"betareg\") within R.\nBug fix and improvements in gleverage() method for betareg objects: Analytic second derivatives are now used and variable dispersion models are handled correctly.\n\n\n\nbetareg 2.2-0\n\ndbeta(..., log = TRUE) is now used for computing the log-likelihood which is numerically more stable than the previous hand-crafted version.\nThe starting values in the dispersion regression are now chosen differently, resulting in a somewhat more robust specification of starting values. The intercept is computed as described in Ferrari & Cribari-Neto (2004), plus a link transformation (if any). All further parameters (if any) are initially set to zero. See also the vignette for details.\nVarious documentation improvements, especially in the vignette.\n\n\n\nbetareg 2.1-2\n\nNew vignette (written by Francisco Cribari-Neto and Z)\nintroducing the package and replicating a range of publications related to beta regression: vignette(\"betareg\", package = \"betareg\") provides some theoretical background, a discussion of the implementation and several hands-on examples.\nImplemented an optional precision model, yielding variable dispersion. The precision parameter phi may depend on a linear predictor, as suggested by Simas, Barreto-Souza, and Rocha (2010). In single part formulas of type y ~ x1 + x2, phi is by default assumed to be constant, i.e., an intercept plus identity link. But it can be extended to y ~ x1 + x2 | z1 + z2 where phi depends on z1 + z2, by default through a log link.\nAllowed all link functions (in mean model) that are available in make.link() for binary responses, and added log-log link.\nAdded data and replication code for Smithson & Verkuilen (2006, Psychological Methods). See ?ReadingSkills, ?MockJurors, ?StressAnxiety as well as the complete replication code in demo(\"SmithsonVerkuilen2006\").\nDefault in residuals() (as well as in the related plot() and summary() components) is now to use standardized weighted residuals 2 (type = \"sweighted2\").\n\n\n\nbetareg 2.0-0\n\nPackage betareg was orphaned on CRAN, Z took over as maintainer, ended up re-writing the whole package. The package still provides all functionality as before but the interface is not fully backward-compatible.\nbetareg(): More standard formula-interface arguments; betareg objects do not inherit from lm anymore.\nbetareg.fit(): Renamed from br.fit(), enhanced interface with more arguments and returned information. Untested support of weighted regressions is enabled.\nbetareg.control(): New function encapsulating control of optim(), slightly modified default values.\nanova() method was removed, use lrtest() from lmtest package instead.\ngen.lev.betareg() was changed to gleverage() method (with new generic) and a bug in the method was fixed.\nenvelope.beta() was removed and is now included in plot() method for betareg objects.\nDatasets prater and pratergrouped were incorporated into a single GasolineYield dataset.\nNew data set FoodExpenditure from Griffiths et al. (1993), replicating second application from Ferrari and Cribari-Neto (2004).\nAdded NAMESPACE.\nThe residuals() method now has three further types of residuals suggested by Espinheira et al. (2008) who recommend to use “standardized weighted residuals 2” (type = \"sweighted2\"). The default are Pearson (aka standardized) residuals."
  },
  {
    "objectID": "man/GasolineYield.html",
    "href": "man/GasolineYield.html",
    "title": "betareg",
    "section": "",
    "text": "Operational data of the proportion of crude oil converted to gasoline after distillation and fractionation.\n\ndata(\"GasolineYield\", package = \"betareg\")\n\nA data frame containing 32 observations on 6 variables.\n\n\nyield\n\n\nproportion of crude oil converted to gasoline after distillation and fractionation.\n\n\ngravity\n\n\ncrude oil gravity (degrees API).\n\n\npressure\n\n\nvapor pressure of crude oil (lbf/in2).\n\n\ntemp10\n\n\ntemperature (degrees F) at which 10 percent of crude oil has vaporized.\n\n\ntemp\n\n\ntemperature (degrees F) at which all gasoline has vaporized.\n\n\nbatch\n\n\nfactor indicating unique batch of conditions gravity, pressure, and temp10.\n\n\nThis dataset was collected by Prater (1956), its dependent variable is the proportion of crude oil after distillation and fractionation. This dataset was analyzed by Atkinson (1985), who used the linear regression model and noted that there is “indication that the error distribution is not quite symmetrical, giving rise to some unduly large and small residuals” (p. 60).\nThe dataset contains 32 observations on the response and on the independent variables. It has been noted (Daniel and Wood, 1971, Chapter 8) that there are only ten sets of values of the first three explanatory variables which correspond to ten different crudes and were subjected to experimentally controlled distillation conditions. These conditions are captured in variable batch and the data were ordered according to the ascending order of temp10.\n\nTaken from Prater (1956).\n\nAtkinson, A.C. (1985). Plots, Transformations and Regression: An Introduction to Graphical Methods of Diagnostic Regression Analysis. New York: Oxford University Press.\nCribari-Neto, F., and Zeileis, A. (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nDaniel, C., and Wood, F.S. (1971). Fitting Equations to Data. New York: John Wiley and Sons.\nFerrari, S.L.P., and Cribari-Neto, F. (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nPrater, N.H. (1956). Estimate Gasoline Yields from Crudes. Petroleum Refiner, 35(5), 236–238.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\n## IGNORE_RDIFF_BEGIN\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy1 &lt;- betareg(yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\nsummary(gy1)\n\n\nCall:\nbetareg(formula = yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-1.9010 -0.6829 -0.0385  0.5531  2.1314 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.6949422  0.7625693  -3.534 0.000409 ***\ngravity      0.0045412  0.0071419   0.636 0.524871    \npressure     0.0304135  0.0281007   1.082 0.279117    \ntemp10      -0.0110449  0.0022640  -4.879 1.07e-06 ***\ntemp         0.0105650  0.0005154  20.499  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)   248.24      62.02   4.003 6.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 75.68 on 6 Df\nPseudo R-squared: 0.9398\nNumber of iterations: 147 (BFGS) + 4 (Fisher scoring) \n\n## Ferrari and Cribari-Neto (2004)\ngy2 &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\n## Table 1\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.1396 -0.5698  0.1202  0.7040  1.7506 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.1595710  0.1823247 -33.784  &lt; 2e-16 ***\nbatch1       1.7277289  0.1012294  17.067  &lt; 2e-16 ***\nbatch2       1.3225969  0.1179020  11.218  &lt; 2e-16 ***\nbatch3       1.5723099  0.1161045  13.542  &lt; 2e-16 ***\nbatch4       1.0597141  0.1023598  10.353  &lt; 2e-16 ***\nbatch5       1.1337518  0.1035232  10.952  &lt; 2e-16 ***\nbatch6       1.0401618  0.1060365   9.809  &lt; 2e-16 ***\nbatch7       0.5436922  0.1091275   4.982 6.29e-07 ***\nbatch8       0.4959007  0.1089257   4.553 5.30e-06 ***\nbatch9       0.3857930  0.1185933   3.253  0.00114 ** \ntemp         0.0109669  0.0004126  26.577  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    440.3      110.0   4.002 6.29e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  84.8 on 12 Df\nPseudo R-squared: 0.9617\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring) \n\n## Figure 2\npar(mfrow = c(3, 2))\nplot(gy2, which = 1, type = \"pearson\", sub.caption = \"\")\nplot(gy2, which = 1, type = \"deviance\", sub.caption = \"\")\nplot(gy2, which = 5, type = \"deviance\", sub.caption = \"\")\nplot(gy2, which = 4, type = \"pearson\", sub.caption = \"\")\nplot(gy2, which = 2:3)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n## exclude 4th observation\ngy2a &lt;- update(gy2, subset = -4)\ngy2a\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, subset = -4)\n\nCoefficients (mean model with logit link):\n(Intercept)       batch1       batch2       batch3       batch4       batch5  \n   -6.35647      1.88688      1.37039      1.62512      1.08066      1.15158  \n     batch6       batch7       batch8       batch9         temp  \n    1.05766      0.56522      0.50066      0.38523      0.01146  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n577.8  \n\nsummary(gy2a)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, subset = -4)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.0153 -0.8176  0.0897  0.6948  2.0746 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.3564713  0.1716020 -37.042  &lt; 2e-16 ***\nbatch1       1.8868782  0.1001837  18.834  &lt; 2e-16 ***\nbatch2       1.3703911  0.1042352  13.147  &lt; 2e-16 ***\nbatch3       1.6251199  0.1028326  15.804  &lt; 2e-16 ***\nbatch4       1.0806596  0.0897855  12.036  &lt; 2e-16 ***\nbatch5       1.1515826  0.0906857  12.699  &lt; 2e-16 ***\nbatch6       1.0576556  0.0929172  11.383  &lt; 2e-16 ***\nbatch7       0.5652219  0.0956100   5.912 3.39e-09 ***\nbatch8       0.5006625  0.0953210   5.252 1.50e-07 ***\nbatch9       0.3852258  0.1037500   3.713 0.000205 ***\ntemp         0.0114588  0.0003945  29.050  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    577.8      146.7   3.938 8.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 86.62 on 12 Df\nPseudo R-squared: 0.9662\nNumber of iterations: 51 (BFGS) + 4 (Fisher scoring) \n\n## IGNORE_RDIFF_END",
    "crumbs": [
      "Data sets",
      "GasolineYield"
    ]
  },
  {
    "objectID": "man/GasolineYield.html#estimation-of-gasoline-yields-from-crude-oil",
    "href": "man/GasolineYield.html#estimation-of-gasoline-yields-from-crude-oil",
    "title": "betareg",
    "section": "",
    "text": "Operational data of the proportion of crude oil converted to gasoline after distillation and fractionation.\n\ndata(\"GasolineYield\", package = \"betareg\")\n\nA data frame containing 32 observations on 6 variables.\n\n\nyield\n\n\nproportion of crude oil converted to gasoline after distillation and fractionation.\n\n\ngravity\n\n\ncrude oil gravity (degrees API).\n\n\npressure\n\n\nvapor pressure of crude oil (lbf/in2).\n\n\ntemp10\n\n\ntemperature (degrees F) at which 10 percent of crude oil has vaporized.\n\n\ntemp\n\n\ntemperature (degrees F) at which all gasoline has vaporized.\n\n\nbatch\n\n\nfactor indicating unique batch of conditions gravity, pressure, and temp10.\n\n\nThis dataset was collected by Prater (1956), its dependent variable is the proportion of crude oil after distillation and fractionation. This dataset was analyzed by Atkinson (1985), who used the linear regression model and noted that there is “indication that the error distribution is not quite symmetrical, giving rise to some unduly large and small residuals” (p. 60).\nThe dataset contains 32 observations on the response and on the independent variables. It has been noted (Daniel and Wood, 1971, Chapter 8) that there are only ten sets of values of the first three explanatory variables which correspond to ten different crudes and were subjected to experimentally controlled distillation conditions. These conditions are captured in variable batch and the data were ordered according to the ascending order of temp10.\n\nTaken from Prater (1956).\n\nAtkinson, A.C. (1985). Plots, Transformations and Regression: An Introduction to Graphical Methods of Diagnostic Regression Analysis. New York: Oxford University Press.\nCribari-Neto, F., and Zeileis, A. (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nDaniel, C., and Wood, F.S. (1971). Fitting Equations to Data. New York: John Wiley and Sons.\nFerrari, S.L.P., and Cribari-Neto, F. (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nPrater, N.H. (1956). Estimate Gasoline Yields from Crudes. Petroleum Refiner, 35(5), 236–238.\n\nbetareg\n\n\nlibrary(\"betareg\")\n\n## IGNORE_RDIFF_BEGIN\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy1 &lt;- betareg(yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\nsummary(gy1)\n\n\nCall:\nbetareg(formula = yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-1.9010 -0.6829 -0.0385  0.5531  2.1314 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.6949422  0.7625693  -3.534 0.000409 ***\ngravity      0.0045412  0.0071419   0.636 0.524871    \npressure     0.0304135  0.0281007   1.082 0.279117    \ntemp10      -0.0110449  0.0022640  -4.879 1.07e-06 ***\ntemp         0.0105650  0.0005154  20.499  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)   248.24      62.02   4.003 6.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 75.68 on 6 Df\nPseudo R-squared: 0.9398\nNumber of iterations: 147 (BFGS) + 4 (Fisher scoring) \n\n## Ferrari and Cribari-Neto (2004)\ngy2 &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\n## Table 1\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.1396 -0.5698  0.1202  0.7040  1.7506 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.1595710  0.1823247 -33.784  &lt; 2e-16 ***\nbatch1       1.7277289  0.1012294  17.067  &lt; 2e-16 ***\nbatch2       1.3225969  0.1179020  11.218  &lt; 2e-16 ***\nbatch3       1.5723099  0.1161045  13.542  &lt; 2e-16 ***\nbatch4       1.0597141  0.1023598  10.353  &lt; 2e-16 ***\nbatch5       1.1337518  0.1035232  10.952  &lt; 2e-16 ***\nbatch6       1.0401618  0.1060365   9.809  &lt; 2e-16 ***\nbatch7       0.5436922  0.1091275   4.982 6.29e-07 ***\nbatch8       0.4959007  0.1089257   4.553 5.30e-06 ***\nbatch9       0.3857930  0.1185933   3.253  0.00114 ** \ntemp         0.0109669  0.0004126  26.577  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    440.3      110.0   4.002 6.29e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:  84.8 on 12 Df\nPseudo R-squared: 0.9617\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring) \n\n## Figure 2\npar(mfrow = c(3, 2))\nplot(gy2, which = 1, type = \"pearson\", sub.caption = \"\")\nplot(gy2, which = 1, type = \"deviance\", sub.caption = \"\")\nplot(gy2, which = 5, type = \"deviance\", sub.caption = \"\")\nplot(gy2, which = 4, type = \"pearson\", sub.caption = \"\")\nplot(gy2, which = 2:3)\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n## exclude 4th observation\ngy2a &lt;- update(gy2, subset = -4)\ngy2a\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, subset = -4)\n\nCoefficients (mean model with logit link):\n(Intercept)       batch1       batch2       batch3       batch4       batch5  \n   -6.35647      1.88688      1.37039      1.62512      1.08066      1.15158  \n     batch6       batch7       batch8       batch9         temp  \n    1.05766      0.56522      0.50066      0.38523      0.01146  \n\nPhi coefficients (precision model with identity link):\n(phi)  \n577.8  \n\nsummary(gy2a)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield, subset = -4)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.0153 -0.8176  0.0897  0.6948  2.0746 \n\nCoefficients (mean model with logit link):\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.3564713  0.1716020 -37.042  &lt; 2e-16 ***\nbatch1       1.8868782  0.1001837  18.834  &lt; 2e-16 ***\nbatch2       1.3703911  0.1042352  13.147  &lt; 2e-16 ***\nbatch3       1.6251199  0.1028326  15.804  &lt; 2e-16 ***\nbatch4       1.0806596  0.0897855  12.036  &lt; 2e-16 ***\nbatch5       1.1515826  0.0906857  12.699  &lt; 2e-16 ***\nbatch6       1.0576556  0.0929172  11.383  &lt; 2e-16 ***\nbatch7       0.5652219  0.0956100   5.912 3.39e-09 ***\nbatch8       0.5006625  0.0953210   5.252 1.50e-07 ***\nbatch9       0.3852258  0.1037500   3.713 0.000205 ***\ntemp         0.0114588  0.0003945  29.050  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    577.8      146.7   3.938 8.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 86.62 on 12 Df\nPseudo R-squared: 0.9662\nNumber of iterations: 51 (BFGS) + 4 (Fisher scoring) \n\n## IGNORE_RDIFF_END",
    "crumbs": [
      "Data sets",
      "GasolineYield"
    ]
  },
  {
    "objectID": "man/LossAversion.html",
    "href": "man/LossAversion.html",
    "title": "betareg",
    "section": "",
    "text": "Data from a behavioral economics experiment assessing the extent of myopic loss aversion among adolescents (mostly aged 11 to 19).\n\ndata(\"LossAversion\", package = \"betareg\")\n\nA data frame containing 570 observations on 7 variables.\n\n\ninvest\n\n\nnumeric. Average proportion of tokens invested across all 9 rounds.\n\n\ngender\n\n\nfactor. Gender of the player (or team of players).\n\n\nmale\n\n\nfactor. Was (at least one of) the player(s) male (in the team)?\n\n\nage\n\n\nnumeric. Age in years (averaged for teams).\n\n\ntreatment\n\n\nfactor. Type of treatment: long vs. short.\n\n\ngrade\n\n\nfactor. School grades: 6-8 (11-14 years) vs. 10-12 (15-18 years).\n\n\narrangement\n\n\nfactor. Is the player a single player or team of two?\n\n\nMyopic loss aversion is a phenomenon in behavioral economics, where individuals do not behave economically rationally when making short-term decisions under uncertainty. Example: In lotteries with positive expected payouts investments are lower than the maximum possible (loss aversion). This effect is enhanced for short-term investments (myopia or short-sightedness).\nThe data in LossAversion were collected by Matthias Sutter and Daniela Glätzle-Rützler (Universität Innsbruck) in an experiment with high-school students in Tyrol, Austria (Schwaz and Innsbruck). The students could invest X tokens (0-100) in each of 9 rounds in a lottery. The payouts were 100 + 2.5 * X tokens with probability 1/3 and 100 - X tokens with probability 2/3. Thus, the expected payouts were 100 + 1/6 * X tokens. Depending on the treatment in the experiment, the investments could either be modified in each round (treatment: \"short\") or only in round 1, 4, 7 (treatment \"long\"). Decisions were either made alone or in teams of two. The tokens were converted to monetary payouts using a conversion of EUR 0.5 per 100 tokens for lower grades (Unterstufe, 6-8) or EUR 1.0 per 100 tokens for upper grades (Oberstufe, 10-12).\nFrom the myopic loss aversion literature (on adults) one would expect that the investments of the players (either single players or teams of two) would depend on all factors: Investments should be\n\n\nlower in the short treatment (which would indicate myopia),\n\n\nhigher for teams (indicating a reduction in loss aversion),\n\n\nhigher for (teams with) male players,\n\n\nincrease with age/grade.\n\n\nSee Glätzle-Rützler et al. (2015) for more details and references to the literature. In their original analysis, the investments are analyzes using a panel structure (i.e., 9 separate investments for each team). Here, the data are averaged across rounds for each player, leading to qualitatively similar results. The full data along with replication materials are available in the Harvard Dataverse.\nKosmidis and Zeileis (2024) revisit the data using extended-support beta mixture (XBX) regression, which can simultaneously capture both the probability of rational behavior and the mean amount of loss aversion.\n\nGlätzle-Rützler D, Sutter M, Zeileis A (2020). Replication Data for: No Myopic Loss Aversion in Adolescents? - An Experimental Note. Harvard Dataverse, UNF:6:6hVtbHavJAFYfL7dDl7jqA==. doi:10.7910/DVN/IHFZAK\n\nGlätzle-Rützler D, Sutter M, Zeileis A (2015). No Myopic Loss Aversion in Adolescents? – An Experimental Note. Journal of Economic Behavior & Organization, 111, 169–176. doi:10.1016/j.jebo.2014.12.021\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\n## data and add ad-hoc scaling (a la Smithson & Verkuilen)\ndata(\"LossAversion\", package = \"betareg\")\nLossAversion &lt;- transform(LossAversion,\n  invests = (invest * (nrow(LossAversion) - 1) + 0.5)/nrow(LossAversion))\n\n\n## models: normal (with constant variance), beta, extended-support beta mixture\nla_n &lt;- lm(invest ~ grade * (arrangement + age) + male, data = LossAversion)\nsummary(la_n)\n\n\nCall:\nlm(formula = invest ~ grade * (arrangement + age) + male, data = LossAversion)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7735 -0.1967  0.0024  0.1916  0.5724 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  0.2844     0.1575    1.81  0.07136 .  \ngrade10-12                  -0.8437     0.2815   -3.00  0.00284 ** \narrangementteam              0.0628     0.0302    2.08  0.03788 *  \nage                          0.0115     0.0124    0.93  0.35041    \nmaleyes                      0.1035     0.0232    4.46  9.9e-06 ***\ngrade10-12:arrangementteam   0.1507     0.0455    3.32  0.00097 ***\ngrade10-12:age               0.0458     0.0185    2.47  0.01380 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.247 on 563 degrees of freedom\nMultiple R-squared:  0.158, Adjusted R-squared:  0.149 \nF-statistic: 17.7 on 6 and 563 DF,  p-value: &lt;2e-16\n\nla_b &lt;- betareg(invests ~ grade * (arrangement + age) + male | arrangement + male + grade,\n  data = LossAversion)\nsummary(la_b)\n\n\nCall:\nbetareg(formula = invests ~ grade * (arrangement + age) + male | arrangement + \n    male + grade, data = LossAversion)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-3.948 -0.594 -0.042  0.554  4.439 \n\nCoefficients (mean model with logit link):\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.4139     0.6197   -2.28   0.0225 *  \ngrade10-12                  -2.9435     1.2520   -2.35   0.0187 *  \narrangementteam              0.2250     0.1175    1.92   0.0554 .  \nage                          0.0906     0.0486    1.87   0.0621 .  \nmaleyes                      0.4553     0.0990    4.60  4.2e-06 ***\ngrade10-12:arrangementteam   0.6549     0.2003    3.27   0.0011 ** \ngrade10-12:age               0.1513     0.0806    1.88   0.0605 .  \n\nPhi coefficients (precision model with log link):\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.194      0.084   14.21  &lt; 2e-16 ***\narrangementteam    0.406      0.122    3.33  0.00087 ***\nmaleyes           -0.555      0.113   -4.93  8.2e-07 ***\ngrade10-12        -0.553      0.104   -5.31  1.1e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 94.4 on 11 Df\nPseudo R-squared: 0.154\nNumber of iterations: 24 (BFGS) + 3 (Fisher scoring) \n\nla_xbx &lt;- betareg(invest ~ grade * (arrangement + age) + male | arrangement + male + grade,\n  data = LossAversion)\nsummary(la_xbx)\n\n\nCall:\nbetareg(formula = invest ~ grade * (arrangement + age) + male | arrangement + \n    male + grade, data = LossAversion)\n\nRandomized quantile residuals:\n   Min     1Q Median     3Q    Max \n-3.020 -0.693 -0.015  0.698  3.594 \n\nCoefficients (mu model with logit link):\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -0.8650     0.5193   -1.67  0.09577 .  \ngrade10-12                  -3.0962     1.0529   -2.94  0.00328 ** \narrangementteam              0.2079     0.0987    2.11  0.03508 *  \nage                          0.0489     0.0406    1.20  0.22857    \nmaleyes                      0.3792     0.0842    4.50  6.6e-06 ***\ngrade10-12:arrangementteam   0.5672     0.1690    3.36  0.00079 ***\ngrade10-12:age               0.1687     0.0677    2.49  0.01275 *  \n\nPhi coefficients (phi model with log link):\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.756      0.128   13.70  &lt; 2e-16 ***\narrangementteam    0.325      0.145    2.25  0.02446 *  \nmaleyes           -0.484      0.136   -3.56  0.00037 ***\ngrade10-12        -0.316      0.131   -2.41  0.01608 *  \n\nExceedence parameter (extended-support xbetax model):\n        Estimate Std. Error z value Pr(&gt;|z|)    \nLog(nu)   -2.273      0.245   -9.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nExceedence parameter nu: 0.103\nType of estimator: ML (maximum likelihood)\nLog-likelihood: -71.8 on 12 Df\nNumber of iterations in BFGS optimization: 45 \n\n## coefficients in XBX are typically somewhat shrunken compared to beta\ncbind(XBX = coef(la_xbx), Beta = c(coef(la_b), NA))\n\n                                XBX     Beta\n(Intercept)                -0.86495 -1.41389\ngrade10-12                 -3.09624 -2.94347\narrangementteam             0.20790  0.22498\nage                         0.04889  0.09061\nmaleyes                     0.37925  0.45534\ngrade10-12:arrangementteam  0.56724  0.65487\ngrade10-12:age              0.16866  0.15129\n(phi)_(Intercept)           1.75629  1.19429\n(phi)_arrangementteam       0.32525  0.40631\n(phi)_maleyes              -0.48406 -0.55481\n(phi)_grade10-12           -0.31593 -0.55271\nLog(nu)                    -2.27289       NA\n\n## predictions on subset: (at least one) male players, higher grades, around age 16\nla &lt;- subset(LossAversion, male == \"yes\" & grade == \"10-12\" & age &gt;= 15 &  age &lt;= 17)\nla_nd &lt;- data.frame(arrangement = c(\"single\", \"team\"), male = \"yes\", age = 16, grade = \"10-12\")\n\n## empirical vs fitted E(Y)\nla_nd$mean_emp &lt;- aggregate(invest ~ arrangement, data = la, FUN = mean)$invest \nla_nd$mean_n &lt;- predict(la_n, la_nd)\nla_nd$mean_b &lt;- predict(la_b, la_nd)\nla_nd$mean_xbx &lt;- predict(la_xbx, la_nd)\nla_nd\n\n  arrangement male age grade mean_emp mean_n mean_b mean_xbx\n1      single  yes  16 10-12   0.4824 0.4612 0.4921   0.4713\n2        team  yes  16 10-12   0.6648 0.6747 0.7002   0.6861\n\n## visualization: all means rather similar\nla_mod &lt;- c(\"Emp\", \"N\", \"B\", \"XBX\")\nla_col &lt;- unname(palette.colors())[c(1, 2, 4, 4)]\nla_lty &lt;- c(1, 5, 5, 1)\nmatplot(la_nd[, paste0(\"mean_\", tolower(la_mod))], type = \"l\",\n  col = la_col, lty = la_lty, lwd = 2, ylab = \"E(Y)\", main = \"E(Y)\", xaxt = \"n\")\naxis(1, at = 1:2, labels = la_nd$arrangement)\nlegend(\"topleft\", la_mod, col = la_col, lty = la_lty, lwd = 2, bty = \"n\")\n\n\n\n\n\n\n## empirical vs. fitted P(Y &gt; 0.95)\nla_nd$prob_emp &lt;- aggregate(invest &gt;= 0.95 ~ arrangement, data = la, FUN = mean)$invest\nla_nd$prob_n &lt;- pnorm(0.95, mean = la_nd$mean_n, sd = summary(la_n)$sigma, lower.tail = FALSE)\nla_nd$prob_b &lt;- 1 - predict(la_b, la_nd, type = \"probability\", at = 0.95)\nla_nd$prob_xbx &lt;- 1 - predict(la_xbx, la_nd, type = \"probability\", at = 0.95)\nla_nd[, -(5:8)]\n\n  arrangement male age grade prob_emp  prob_n prob_b prob_xbx\n1      single  yes  16 10-12  0.08696 0.02403 0.1245  0.07161\n2        team  yes  16 10-12  0.20690 0.13280 0.2487  0.18501\n\n## visualization: only XBX works well\nmatplot(la_nd[, paste0(\"prob_\", tolower(la_mod))], type = \"l\",\n  col = la_col, lty = la_lty, lwd = 2, ylab = \"P(Y &gt; 0.95)\", main = \"P(Y &gt; 0.95)\", xaxt = \"n\")\naxis(1, at = 1:2, labels = la_nd$arrangement)\nlegend(\"topleft\", la_mod, col = la_col, lty = la_lty, lwd = 2, bty = \"n\")",
    "crumbs": [
      "Data sets",
      "LossAversion"
    ]
  },
  {
    "objectID": "man/LossAversion.html#no-myopic-loss-aversion-in-adolescents",
    "href": "man/LossAversion.html#no-myopic-loss-aversion-in-adolescents",
    "title": "betareg",
    "section": "",
    "text": "Data from a behavioral economics experiment assessing the extent of myopic loss aversion among adolescents (mostly aged 11 to 19).\n\ndata(\"LossAversion\", package = \"betareg\")\n\nA data frame containing 570 observations on 7 variables.\n\n\ninvest\n\n\nnumeric. Average proportion of tokens invested across all 9 rounds.\n\n\ngender\n\n\nfactor. Gender of the player (or team of players).\n\n\nmale\n\n\nfactor. Was (at least one of) the player(s) male (in the team)?\n\n\nage\n\n\nnumeric. Age in years (averaged for teams).\n\n\ntreatment\n\n\nfactor. Type of treatment: long vs. short.\n\n\ngrade\n\n\nfactor. School grades: 6-8 (11-14 years) vs. 10-12 (15-18 years).\n\n\narrangement\n\n\nfactor. Is the player a single player or team of two?\n\n\nMyopic loss aversion is a phenomenon in behavioral economics, where individuals do not behave economically rationally when making short-term decisions under uncertainty. Example: In lotteries with positive expected payouts investments are lower than the maximum possible (loss aversion). This effect is enhanced for short-term investments (myopia or short-sightedness).\nThe data in LossAversion were collected by Matthias Sutter and Daniela Glätzle-Rützler (Universität Innsbruck) in an experiment with high-school students in Tyrol, Austria (Schwaz and Innsbruck). The students could invest X tokens (0-100) in each of 9 rounds in a lottery. The payouts were 100 + 2.5 * X tokens with probability 1/3 and 100 - X tokens with probability 2/3. Thus, the expected payouts were 100 + 1/6 * X tokens. Depending on the treatment in the experiment, the investments could either be modified in each round (treatment: \"short\") or only in round 1, 4, 7 (treatment \"long\"). Decisions were either made alone or in teams of two. The tokens were converted to monetary payouts using a conversion of EUR 0.5 per 100 tokens for lower grades (Unterstufe, 6-8) or EUR 1.0 per 100 tokens for upper grades (Oberstufe, 10-12).\nFrom the myopic loss aversion literature (on adults) one would expect that the investments of the players (either single players or teams of two) would depend on all factors: Investments should be\n\n\nlower in the short treatment (which would indicate myopia),\n\n\nhigher for teams (indicating a reduction in loss aversion),\n\n\nhigher for (teams with) male players,\n\n\nincrease with age/grade.\n\n\nSee Glätzle-Rützler et al. (2015) for more details and references to the literature. In their original analysis, the investments are analyzes using a panel structure (i.e., 9 separate investments for each team). Here, the data are averaged across rounds for each player, leading to qualitatively similar results. The full data along with replication materials are available in the Harvard Dataverse.\nKosmidis and Zeileis (2024) revisit the data using extended-support beta mixture (XBX) regression, which can simultaneously capture both the probability of rational behavior and the mean amount of loss aversion.\n\nGlätzle-Rützler D, Sutter M, Zeileis A (2020). Replication Data for: No Myopic Loss Aversion in Adolescents? - An Experimental Note. Harvard Dataverse, UNF:6:6hVtbHavJAFYfL7dDl7jqA==. doi:10.7910/DVN/IHFZAK\n\nGlätzle-Rützler D, Sutter M, Zeileis A (2015). No Myopic Loss Aversion in Adolescents? – An Experimental Note. Journal of Economic Behavior & Organization, 111, 169–176. doi:10.1016/j.jebo.2014.12.021\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\n## data and add ad-hoc scaling (a la Smithson & Verkuilen)\ndata(\"LossAversion\", package = \"betareg\")\nLossAversion &lt;- transform(LossAversion,\n  invests = (invest * (nrow(LossAversion) - 1) + 0.5)/nrow(LossAversion))\n\n\n## models: normal (with constant variance), beta, extended-support beta mixture\nla_n &lt;- lm(invest ~ grade * (arrangement + age) + male, data = LossAversion)\nsummary(la_n)\n\n\nCall:\nlm(formula = invest ~ grade * (arrangement + age) + male, data = LossAversion)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7735 -0.1967  0.0024  0.1916  0.5724 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  0.2844     0.1575    1.81  0.07136 .  \ngrade10-12                  -0.8437     0.2815   -3.00  0.00284 ** \narrangementteam              0.0628     0.0302    2.08  0.03788 *  \nage                          0.0115     0.0124    0.93  0.35041    \nmaleyes                      0.1035     0.0232    4.46  9.9e-06 ***\ngrade10-12:arrangementteam   0.1507     0.0455    3.32  0.00097 ***\ngrade10-12:age               0.0458     0.0185    2.47  0.01380 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.247 on 563 degrees of freedom\nMultiple R-squared:  0.158, Adjusted R-squared:  0.149 \nF-statistic: 17.7 on 6 and 563 DF,  p-value: &lt;2e-16\n\nla_b &lt;- betareg(invests ~ grade * (arrangement + age) + male | arrangement + male + grade,\n  data = LossAversion)\nsummary(la_b)\n\n\nCall:\nbetareg(formula = invests ~ grade * (arrangement + age) + male | arrangement + \n    male + grade, data = LossAversion)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-3.948 -0.594 -0.042  0.554  4.439 \n\nCoefficients (mean model with logit link):\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -1.4139     0.6197   -2.28   0.0225 *  \ngrade10-12                  -2.9435     1.2520   -2.35   0.0187 *  \narrangementteam              0.2250     0.1175    1.92   0.0554 .  \nage                          0.0906     0.0486    1.87   0.0621 .  \nmaleyes                      0.4553     0.0990    4.60  4.2e-06 ***\ngrade10-12:arrangementteam   0.6549     0.2003    3.27   0.0011 ** \ngrade10-12:age               0.1513     0.0806    1.88   0.0605 .  \n\nPhi coefficients (precision model with log link):\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.194      0.084   14.21  &lt; 2e-16 ***\narrangementteam    0.406      0.122    3.33  0.00087 ***\nmaleyes           -0.555      0.113   -4.93  8.2e-07 ***\ngrade10-12        -0.553      0.104   -5.31  1.1e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 94.4 on 11 Df\nPseudo R-squared: 0.154\nNumber of iterations: 24 (BFGS) + 3 (Fisher scoring) \n\nla_xbx &lt;- betareg(invest ~ grade * (arrangement + age) + male | arrangement + male + grade,\n  data = LossAversion)\nsummary(la_xbx)\n\n\nCall:\nbetareg(formula = invest ~ grade * (arrangement + age) + male | arrangement + \n    male + grade, data = LossAversion)\n\nRandomized quantile residuals:\n   Min     1Q Median     3Q    Max \n-3.020 -0.693 -0.015  0.698  3.594 \n\nCoefficients (mu model with logit link):\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -0.8650     0.5193   -1.67  0.09577 .  \ngrade10-12                  -3.0962     1.0529   -2.94  0.00328 ** \narrangementteam              0.2079     0.0987    2.11  0.03508 *  \nage                          0.0489     0.0406    1.20  0.22857    \nmaleyes                      0.3792     0.0842    4.50  6.6e-06 ***\ngrade10-12:arrangementteam   0.5672     0.1690    3.36  0.00079 ***\ngrade10-12:age               0.1687     0.0677    2.49  0.01275 *  \n\nPhi coefficients (phi model with log link):\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.756      0.128   13.70  &lt; 2e-16 ***\narrangementteam    0.325      0.145    2.25  0.02446 *  \nmaleyes           -0.484      0.136   -3.56  0.00037 ***\ngrade10-12        -0.316      0.131   -2.41  0.01608 *  \n\nExceedence parameter (extended-support xbetax model):\n        Estimate Std. Error z value Pr(&gt;|z|)    \nLog(nu)   -2.273      0.245   -9.27   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nExceedence parameter nu: 0.103\nType of estimator: ML (maximum likelihood)\nLog-likelihood: -71.8 on 12 Df\nNumber of iterations in BFGS optimization: 45 \n\n## coefficients in XBX are typically somewhat shrunken compared to beta\ncbind(XBX = coef(la_xbx), Beta = c(coef(la_b), NA))\n\n                                XBX     Beta\n(Intercept)                -0.86495 -1.41389\ngrade10-12                 -3.09624 -2.94347\narrangementteam             0.20790  0.22498\nage                         0.04889  0.09061\nmaleyes                     0.37925  0.45534\ngrade10-12:arrangementteam  0.56724  0.65487\ngrade10-12:age              0.16866  0.15129\n(phi)_(Intercept)           1.75629  1.19429\n(phi)_arrangementteam       0.32525  0.40631\n(phi)_maleyes              -0.48406 -0.55481\n(phi)_grade10-12           -0.31593 -0.55271\nLog(nu)                    -2.27289       NA\n\n## predictions on subset: (at least one) male players, higher grades, around age 16\nla &lt;- subset(LossAversion, male == \"yes\" & grade == \"10-12\" & age &gt;= 15 &  age &lt;= 17)\nla_nd &lt;- data.frame(arrangement = c(\"single\", \"team\"), male = \"yes\", age = 16, grade = \"10-12\")\n\n## empirical vs fitted E(Y)\nla_nd$mean_emp &lt;- aggregate(invest ~ arrangement, data = la, FUN = mean)$invest \nla_nd$mean_n &lt;- predict(la_n, la_nd)\nla_nd$mean_b &lt;- predict(la_b, la_nd)\nla_nd$mean_xbx &lt;- predict(la_xbx, la_nd)\nla_nd\n\n  arrangement male age grade mean_emp mean_n mean_b mean_xbx\n1      single  yes  16 10-12   0.4824 0.4612 0.4921   0.4713\n2        team  yes  16 10-12   0.6648 0.6747 0.7002   0.6861\n\n## visualization: all means rather similar\nla_mod &lt;- c(\"Emp\", \"N\", \"B\", \"XBX\")\nla_col &lt;- unname(palette.colors())[c(1, 2, 4, 4)]\nla_lty &lt;- c(1, 5, 5, 1)\nmatplot(la_nd[, paste0(\"mean_\", tolower(la_mod))], type = \"l\",\n  col = la_col, lty = la_lty, lwd = 2, ylab = \"E(Y)\", main = \"E(Y)\", xaxt = \"n\")\naxis(1, at = 1:2, labels = la_nd$arrangement)\nlegend(\"topleft\", la_mod, col = la_col, lty = la_lty, lwd = 2, bty = \"n\")\n\n\n\n\n\n\n## empirical vs. fitted P(Y &gt; 0.95)\nla_nd$prob_emp &lt;- aggregate(invest &gt;= 0.95 ~ arrangement, data = la, FUN = mean)$invest\nla_nd$prob_n &lt;- pnorm(0.95, mean = la_nd$mean_n, sd = summary(la_n)$sigma, lower.tail = FALSE)\nla_nd$prob_b &lt;- 1 - predict(la_b, la_nd, type = \"probability\", at = 0.95)\nla_nd$prob_xbx &lt;- 1 - predict(la_xbx, la_nd, type = \"probability\", at = 0.95)\nla_nd[, -(5:8)]\n\n  arrangement male age grade prob_emp  prob_n prob_b prob_xbx\n1      single  yes  16 10-12  0.08696 0.02403 0.1245  0.07161\n2        team  yes  16 10-12  0.20690 0.13280 0.2487  0.18501\n\n## visualization: only XBX works well\nmatplot(la_nd[, paste0(\"prob_\", tolower(la_mod))], type = \"l\",\n  col = la_col, lty = la_lty, lwd = 2, ylab = \"P(Y &gt; 0.95)\", main = \"P(Y &gt; 0.95)\", xaxt = \"n\")\naxis(1, at = 1:2, labels = la_nd$arrangement)\nlegend(\"topleft\", la_mod, col = la_col, lty = la_lty, lwd = 2, bty = \"n\")",
    "crumbs": [
      "Data sets",
      "LossAversion"
    ]
  },
  {
    "objectID": "man/dbeta01.html",
    "href": "man/dbeta01.html",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the zero- and/or one-inflated beta distribution in regression parameterization.\n\n\n\ndbeta01(x, mu, phi, p0 = 0, p1 = 0, log = FALSE)\n\npbeta01(q, mu, phi, p0 = 0, p1 = 0, lower.tail = TRUE, log.p = FALSE)\n\nqbeta01(p, mu, phi, p0 = 0, p1 = 0, lower.tail = TRUE, log.p = FALSE)\n\nrbeta01(n, mu, phi, p0 = 0, p1 = 0)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution (on the open unit interval).\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\n\np0\n\n\nnumeric. The probability for an observation of zero (often referred to as zero inflation).\n\n\n\n\np1\n\n\nnumeric. The probability for an observation of one (often referred to as one inflation).\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nThe zero- and/or one-inflated beta distribution is obtained by adding point masses at zero and/or one to a standard beta distribution.\nNote that the support of the standard beta distribution is the open unit interval where values of exactly zero or one cannot occur. Thus, the inflation jargon is rather misleading as there is no probability that could be inflated. It is rather a hurdle or two-part (or three-part) model.\n\n\n\ndbeta01 gives the density, pbeta01 gives the distribution function, qbeta01 gives the quantile function, and rbeta01 generates random deviates.\n\n\n\ndbetar, Beta01",
    "crumbs": [
      "Distributions",
      "dbeta01"
    ]
  },
  {
    "objectID": "man/dbeta01.html#the-zero--andor-one-inflated-beta-distribution-in-regression-parameterization",
    "href": "man/dbeta01.html#the-zero--andor-one-inflated-beta-distribution-in-regression-parameterization",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the zero- and/or one-inflated beta distribution in regression parameterization.\n\n\n\ndbeta01(x, mu, phi, p0 = 0, p1 = 0, log = FALSE)\n\npbeta01(q, mu, phi, p0 = 0, p1 = 0, lower.tail = TRUE, log.p = FALSE)\n\nqbeta01(p, mu, phi, p0 = 0, p1 = 0, lower.tail = TRUE, log.p = FALSE)\n\nrbeta01(n, mu, phi, p0 = 0, p1 = 0)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution (on the open unit interval).\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\n\np0\n\n\nnumeric. The probability for an observation of zero (often referred to as zero inflation).\n\n\n\n\np1\n\n\nnumeric. The probability for an observation of one (often referred to as one inflation).\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nThe zero- and/or one-inflated beta distribution is obtained by adding point masses at zero and/or one to a standard beta distribution.\nNote that the support of the standard beta distribution is the open unit interval where values of exactly zero or one cannot occur. Thus, the inflation jargon is rather misleading as there is no probability that could be inflated. It is rather a hurdle or two-part (or three-part) model.\n\n\n\ndbeta01 gives the density, pbeta01 gives the distribution function, qbeta01 gives the quantile function, and rbeta01 generates random deviates.\n\n\n\ndbetar, Beta01",
    "crumbs": [
      "Distributions",
      "dbeta01"
    ]
  },
  {
    "objectID": "man/ImpreciseTask.html",
    "href": "man/ImpreciseTask.html",
    "title": "betareg",
    "section": "",
    "text": "In this study participants were asked to estimate upper and lower probabilities for event to occur and not to occur.\n\ndata(\"ImpreciseTask\", package = \"betareg\")\n\nA data frame with 242 observations on the following 3 variables.\n\n\ntask\n\n\na factor with levels Boeing stock and Sunday weather.\n\n\nlocation\n\n\na numeric vector of the average of the lower estimate for the event not to occur and the upper estimate for the event to occur.\n\n\ndifference\n\n\na numeric vector of the differences of the lower and upper estimate for the event to occur.\n\n\nAll participants in the study were either first- or second-year undergraduate students in psychology, none of whom had a strong background in probability or were familiar with imprecise probability theories.\nFor the sunday weather task see WeatherTask. For the Boeing stock task participants were asked to estimate the probability that Boeing’s stock would rise more than those in a list of 30 companies.\nFor each task participants were asked to provide lower and upper estimates for the event to occur and not to occur.\n\nTaken from Smithson et al. (2011) supplements.\n\nSmithson M, Merkle EC, Verkuilen J (2011). Beta Regression Finite Mixture Models of Polarization and Priming. Journal of Educational and Behavioral Statistics, 36(6), 804–831. doi:10.3102/1076998610396893\nSmithson M, Segale C (2009). Partition Priming in Judgments of Imprecise Probabilities. Journal of Statistical Theory and Practice, 3(1), 169–181.\n\n\nlibrary(\"betareg\")\n\ndata(\"ImpreciseTask\", package = \"betareg\")\nlibrary(\"flexmix\")\nwt_betamix &lt;- betamix(location ~ difference * task, data = ImpreciseTask, k = 2,\n  extra_components = extraComponent(type = \"betareg\", coef =\n    list(mean = 0, precision = 8)),\n  FLXconcomitant = FLXPmultinom(~ task))",
    "crumbs": [
      "Data sets",
      "ImpreciseTask"
    ]
  },
  {
    "objectID": "man/ImpreciseTask.html#imprecise-probabilities-for-sunday-weather-and-boeing-stock-task",
    "href": "man/ImpreciseTask.html#imprecise-probabilities-for-sunday-weather-and-boeing-stock-task",
    "title": "betareg",
    "section": "",
    "text": "In this study participants were asked to estimate upper and lower probabilities for event to occur and not to occur.\n\ndata(\"ImpreciseTask\", package = \"betareg\")\n\nA data frame with 242 observations on the following 3 variables.\n\n\ntask\n\n\na factor with levels Boeing stock and Sunday weather.\n\n\nlocation\n\n\na numeric vector of the average of the lower estimate for the event not to occur and the upper estimate for the event to occur.\n\n\ndifference\n\n\na numeric vector of the differences of the lower and upper estimate for the event to occur.\n\n\nAll participants in the study were either first- or second-year undergraduate students in psychology, none of whom had a strong background in probability or were familiar with imprecise probability theories.\nFor the sunday weather task see WeatherTask. For the Boeing stock task participants were asked to estimate the probability that Boeing’s stock would rise more than those in a list of 30 companies.\nFor each task participants were asked to provide lower and upper estimates for the event to occur and not to occur.\n\nTaken from Smithson et al. (2011) supplements.\n\nSmithson M, Merkle EC, Verkuilen J (2011). Beta Regression Finite Mixture Models of Polarization and Priming. Journal of Educational and Behavioral Statistics, 36(6), 804–831. doi:10.3102/1076998610396893\nSmithson M, Segale C (2009). Partition Priming in Judgments of Imprecise Probabilities. Journal of Statistical Theory and Practice, 3(1), 169–181.\n\n\nlibrary(\"betareg\")\n\ndata(\"ImpreciseTask\", package = \"betareg\")\nlibrary(\"flexmix\")\nwt_betamix &lt;- betamix(location ~ difference * task, data = ImpreciseTask, k = 2,\n  extra_components = extraComponent(type = \"betareg\", coef =\n    list(mean = 0, precision = 8)),\n  FLXconcomitant = FLXPmultinom(~ task))",
    "crumbs": [
      "Data sets",
      "ImpreciseTask"
    ]
  },
  {
    "objectID": "man/betar_family.html",
    "href": "man/betar_family.html",
    "title": "betareg",
    "section": "",
    "text": "Family objects for distributional regression with the (extended-support) beta distribution via bamlss or gamlss2.\n\nbetar_family(link = \"logit\", link.phi = \"log\", ...)\n\nxbetax_family(link = \"logit\", link.phi = \"log\", link.nu = \"log\",\n  quad = 20, tol = .Machine\\$double.eps^0.7, ...)\n\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “log”, “identity”, “sqrt” are supported.\n\n\n\n\nlink.nu\n\n\ncharacter specification of the link function in the exceedence model (nu). Currently, “log”, “identity”, “sqrt” are supported.\n\n\n\n\nquad\n\n\nnumeric. The number of quadrature points for numeric integration of the continuous mixture in dxbetax. Alternatively, a matrix with nodes and weights for the quadrature points can be specified.\n\n\n\n\ntol\n\n\nnumeric. Accuracy (convergence tolerance) for numerically determining quantiles based on uniroot and pxbetax.\n\n\n\n\n…\n\n\nArguments passed to functions that are called within the family object.\n\n\n\nFamily objects for bamlss (Umlauf et al. 2019, 2021) and gamlss2 (Umlauf et al. 2024) are essentially lists of functions providing a standardized interface to the d/p/q/r functions of distributions. Hence, betar_family interfaces the classical beta distribution in regression specification, see dbetar. Analogously, xbetax_family interfaces the extended-support beta mixture specification (Kosmidis and Zeileis 2024), see dxbetax.\n\nA list of class family.bamlss.\n\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\nUmlauf N, Klein N, Zeileis A (2019). BAMLSS: Bayesian Additive Models for Location, Scale and Shape (and Beyond). Journal of Computational and Graphical Statistics, 27(3), 612–627. doi:10.1080/10618600.2017.1407325\nUmlauf N, Klein N, Simon T, Zeileis A (2021). bamlss: A Lego Toolbox for Flexible Bayesian Regression (and Beyond). Journal of Statistical Software, 100(4), 1–53. doi:10.18637/jss.v100.i04\nUmlauf N, Stasinopoulos M, Rigby R, Stauffer R (2024). gamlss2: Infrastructure for Flexible Distributional Regression. R package version 0.1-0. https://gamlss-dev.github.io/gamlss2/\n\ndbetar, dxbetax, family.bamlss\n\n\nlibrary(\"betareg\")\n\n\n## package and data\nlibrary(\"betareg\")\nlibrary(\"bamlss\")\ndata(\"ReadingSkills\", package = \"betareg\")\n\n## classical beta regression via ML\nrs1 &lt;- betareg(accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\n\n## IGNORE_RDIFF_BEGIN\n## Bayesian additive model (with low number of iterations to speed up the example)\nset.seed(0)\nrs2 &lt;- bamlss(accuracy ~ s(iq, by = dyslexia) | dyslexia + iq, data = ReadingSkills,\n  family = betar_family(), eps = 1e-7, n.iter = 400, burnin = 100)\n\nAICc  -0.8829 logPost -64.6792 logLik  21.6241 edf 13.973 eps 1.0000 iteration   1\nAICc -51.8915 logPost -25.8044 logLik  39.7035 edf 10.242 eps 2.8731 iteration   2\nAICc -68.1251 logPost  -7.3858 logLik  46.8183 edf 9.6642 eps 0.5561 iteration   3\nAICc -73.1290 logPost   4.7269 logLik  48.3399 edf 9.0783 eps 0.0895 iteration   4\nAICc -74.7842 logPost   5.5545 logLik  49.1675 edf 9.0783 eps 0.0629 iteration   5\nAICc -75.7464 logPost   6.0357 logLik  49.6487 edf 9.0783 eps 0.0559 iteration   6\nAICc -76.3072 logPost   6.3160 logLik  49.9291 edf 9.0783 eps 0.2490 iteration   7\nAICc -77.3399 logPost   6.8324 logLik  50.4454 edf 9.0783 eps 0.2694 iteration   8\nAICc -77.6557 logPost   6.9903 logLik  50.6033 edf 9.0783 eps 0.0207 iteration   9\nAICc -77.8176 logPost   7.0713 logLik  50.6843 edf 9.0783 eps 0.0123 iteration  10\nAICc -80.2391 logPost   8.7295 logLik  50.7321 edf 8.3559 eps 0.0099 iteration  11\nAICc -81.1323 logPost -61.6299 logLik  50.7922 edf 8.1090 eps 0.0333 iteration  12\nAICc -81.3164 logPost -654.389 logLik  50.8351 edf 8.0774 eps 0.0089 iteration  13\nAICc -81.3726 logPost -4694.96 logLik  50.8579 edf 8.0740 eps 0.0054 iteration  14\nAICc -81.4322 logPost -25766.3 logLik  50.8816 edf 8.0700 eps 0.0077 iteration  15\nAICc -81.6598 logPost -2043.53 logLik  50.9498 edf 8.0406 eps 0.0179 iteration  16\nAICc -82.3830 logPost -8852.97 logLik  51.1594 edf 7.9421 eps 0.0137 iteration  17\nAICc -82.7904 logPost -13085.5 logLik  51.2898 edf 7.8944 eps 0.0124 iteration  18\nAICc -82.9776 logPost -13740.2 logLik  51.3740 edf 7.8883 eps 0.0431 iteration  19\nAICc -83.0956 logPost -13809.0 logLik  51.4431 edf 7.8949 eps 0.0096 iteration  20\nAICc -83.2026 logPost -13815.6 logLik  51.4812 edf 7.8848 eps 0.0058 iteration  21\nAICc -83.2582 logPost -13816.0 logLik  51.5109 edf 7.8861 eps 0.0574 iteration  22\nAICc -83.3488 logPost -13815.9 logLik  51.5729 edf 7.8970 eps 0.0084 iteration  23\nAICc -83.4298 logPost -13815.7 logLik  51.5929 edf 7.8836 eps 0.0035 iteration  24\nAICc -83.4578 logPost -13815.6 logLik  51.6069 edf 7.8837 eps 0.0029 iteration  25\nAICc -83.4771 logPost -13815.4 logLik  51.6166 edf 7.8837 eps 0.0025 iteration  26\nAICc -83.4921 logPost -13815.3 logLik  51.6242 edf 7.8837 eps 0.0042 iteration  27\nAICc -83.5087 logPost -13815.3 logLik  51.6291 edf 7.8815 eps 0.0021 iteration  28\nAICc -83.5136 logPost -13815.2 logLik  51.6323 edf 7.8820 eps 0.0016 iteration  29\nAICc -83.5176 logPost -13815.1 logLik  51.6343 edf 7.8820 eps 0.0013 iteration  30\nAICc -83.5199 logPost -13815.1 logLik  51.6355 edf 7.8820 eps 0.0011 iteration  31\nAICc -83.5219 logPost -13815.1 logLik  51.6365 edf 7.8820 eps 0.0042 iteration  32\nAICc -83.5216 logPost -13815.0 logLik  51.6382 edf 7.8832 eps 0.0022 iteration  33\nAICc -83.5300 logPost -13815.0 logLik  51.6383 edf 7.8806 eps 0.0009 iteration  34\nAICc -83.5294 logPost -13815.0 logLik  51.6384 edf 7.8808 eps 0.0003 iteration  35\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  36\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  37\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  38\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  39\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  39\nelapsed time:  2.67sec\nStarting the sampler...\n\n|                    |   0%  2.57sec\n|*                   |   5%  2.47sec  0.13sec\n|**                  |  10%  2.26sec  0.25sec\n|***                 |  15%  2.20sec  0.39sec\n|****                |  20%  2.08sec  0.52sec\n|*****               |  25%  1.93sec  0.64sec\n|******              |  30%  1.81sec  0.78sec\n|*******             |  35%  1.70sec  0.92sec\n|********            |  40%  1.60sec  1.07sec\n|*********           |  45%  1.47sec  1.20sec\n|**********          |  50%  1.34sec  1.34sec\n|***********         |  55%  1.21sec  1.48sec\n|************        |  60%  1.08sec  1.61sec\n|*************       |  65%  0.94sec  1.75sec\n|**************      |  70%  0.81sec  1.89sec\n|***************     |  75%  0.68sec  2.03sec\n|****************    |  80%  0.54sec  2.16sec\n|*****************   |  85%  0.41sec  2.30sec\n|******************  |  90%  0.27sec  2.44sec\n|******************* |  95%  0.14sec  2.57sec\n|********************| 100%  0.00sec  2.71sec\n\n## Bayesian model shrinks the effects compared to ML\nplot(accuracy ~ iq, data = ReadingSkills, pch = 19, col = dyslexia)\nnd &lt;- data.frame(\n  iq = rep(-19:20/10, 2),\n  dyslexia = factor(rep(c(\"no\", \"yes\"), each = 40), levels = c(\"no\", \"yes\"))\n)\nnd$betareg &lt;- predict(rs1, newdata = nd, type = \"response\")\nnd$bamlss  &lt;- predict(rs2, newdata = nd, type = \"parameter\", model = \"mu\")\nlines(betareg ~ iq, data = nd, subset = dyslexia == \"no\",  col = 1, lwd = 2, lty = 1)\nlines(betareg ~ iq, data = nd, subset = dyslexia == \"yes\", col = 2, lwd = 2, lty = 1)\nlines(bamlss  ~ iq, data = nd, subset = dyslexia == \"no\",  col = 1, lwd = 2, lty = 2)\nlines(bamlss  ~ iq, data = nd, subset = dyslexia == \"yes\", col = 2, lwd = 2, lty = 2)\nlegend(\"topleft\", c(\"Dyslexia: no\", \"Dyslexia: yes\", \"betareg\", \"bamlss\"),\n  lty = c(0, 0, 1, 2), pch = c(19, 19, NA, NA), col = c(1, 2, 1, 1), bty = \"n\")\n\n\n\n\n\n\n## IGNORE_RDIFF_END\n\n## xbetax_family(): requires more time due to Gaussian quadrature\n## for gamlss2: install.packages(\"gamlss2\", repos = \"https://gamlss-dev.R-universe.dev\")",
    "crumbs": [
      "Beta regression extensions",
      "betar_family"
    ]
  },
  {
    "objectID": "man/betar_family.html#family-objects-for-extended-support-beta-regression",
    "href": "man/betar_family.html#family-objects-for-extended-support-beta-regression",
    "title": "betareg",
    "section": "",
    "text": "Family objects for distributional regression with the (extended-support) beta distribution via bamlss or gamlss2.\n\nbetar_family(link = \"logit\", link.phi = \"log\", ...)\n\nxbetax_family(link = \"logit\", link.phi = \"log\", link.nu = \"log\",\n  quad = 20, tol = .Machine\\$double.eps^0.7, ...)\n\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “log”, “identity”, “sqrt” are supported.\n\n\n\n\nlink.nu\n\n\ncharacter specification of the link function in the exceedence model (nu). Currently, “log”, “identity”, “sqrt” are supported.\n\n\n\n\nquad\n\n\nnumeric. The number of quadrature points for numeric integration of the continuous mixture in dxbetax. Alternatively, a matrix with nodes and weights for the quadrature points can be specified.\n\n\n\n\ntol\n\n\nnumeric. Accuracy (convergence tolerance) for numerically determining quantiles based on uniroot and pxbetax.\n\n\n\n\n…\n\n\nArguments passed to functions that are called within the family object.\n\n\n\nFamily objects for bamlss (Umlauf et al. 2019, 2021) and gamlss2 (Umlauf et al. 2024) are essentially lists of functions providing a standardized interface to the d/p/q/r functions of distributions. Hence, betar_family interfaces the classical beta distribution in regression specification, see dbetar. Analogously, xbetax_family interfaces the extended-support beta mixture specification (Kosmidis and Zeileis 2024), see dxbetax.\n\nA list of class family.bamlss.\n\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\nUmlauf N, Klein N, Zeileis A (2019). BAMLSS: Bayesian Additive Models for Location, Scale and Shape (and Beyond). Journal of Computational and Graphical Statistics, 27(3), 612–627. doi:10.1080/10618600.2017.1407325\nUmlauf N, Klein N, Simon T, Zeileis A (2021). bamlss: A Lego Toolbox for Flexible Bayesian Regression (and Beyond). Journal of Statistical Software, 100(4), 1–53. doi:10.18637/jss.v100.i04\nUmlauf N, Stasinopoulos M, Rigby R, Stauffer R (2024). gamlss2: Infrastructure for Flexible Distributional Regression. R package version 0.1-0. https://gamlss-dev.github.io/gamlss2/\n\ndbetar, dxbetax, family.bamlss\n\n\nlibrary(\"betareg\")\n\n\n## package and data\nlibrary(\"betareg\")\nlibrary(\"bamlss\")\ndata(\"ReadingSkills\", package = \"betareg\")\n\n## classical beta regression via ML\nrs1 &lt;- betareg(accuracy ~ dyslexia * iq | dyslexia + iq, data = ReadingSkills)\n\n## IGNORE_RDIFF_BEGIN\n## Bayesian additive model (with low number of iterations to speed up the example)\nset.seed(0)\nrs2 &lt;- bamlss(accuracy ~ s(iq, by = dyslexia) | dyslexia + iq, data = ReadingSkills,\n  family = betar_family(), eps = 1e-7, n.iter = 400, burnin = 100)\n\nAICc  -0.8829 logPost -64.6792 logLik  21.6241 edf 13.973 eps 1.0000 iteration   1\nAICc -51.8915 logPost -25.8044 logLik  39.7035 edf 10.242 eps 2.8731 iteration   2\nAICc -68.1251 logPost  -7.3858 logLik  46.8183 edf 9.6642 eps 0.5561 iteration   3\nAICc -73.1290 logPost   4.7269 logLik  48.3399 edf 9.0783 eps 0.0895 iteration   4\nAICc -74.7842 logPost   5.5545 logLik  49.1675 edf 9.0783 eps 0.0629 iteration   5\nAICc -75.7464 logPost   6.0357 logLik  49.6487 edf 9.0783 eps 0.0559 iteration   6\nAICc -76.3072 logPost   6.3160 logLik  49.9291 edf 9.0783 eps 0.2490 iteration   7\nAICc -77.3399 logPost   6.8324 logLik  50.4454 edf 9.0783 eps 0.2694 iteration   8\nAICc -77.6557 logPost   6.9903 logLik  50.6033 edf 9.0783 eps 0.0207 iteration   9\nAICc -77.8176 logPost   7.0713 logLik  50.6843 edf 9.0783 eps 0.0123 iteration  10\nAICc -80.2391 logPost   8.7295 logLik  50.7321 edf 8.3559 eps 0.0099 iteration  11\nAICc -81.1323 logPost -61.6299 logLik  50.7922 edf 8.1090 eps 0.0333 iteration  12\nAICc -81.3164 logPost -654.389 logLik  50.8351 edf 8.0774 eps 0.0089 iteration  13\nAICc -81.3726 logPost -4694.96 logLik  50.8579 edf 8.0740 eps 0.0054 iteration  14\nAICc -81.4322 logPost -25766.3 logLik  50.8816 edf 8.0700 eps 0.0077 iteration  15\nAICc -81.6598 logPost -2043.53 logLik  50.9498 edf 8.0406 eps 0.0179 iteration  16\nAICc -82.3830 logPost -8852.97 logLik  51.1594 edf 7.9421 eps 0.0137 iteration  17\nAICc -82.7904 logPost -13085.5 logLik  51.2898 edf 7.8944 eps 0.0124 iteration  18\nAICc -82.9776 logPost -13740.2 logLik  51.3740 edf 7.8883 eps 0.0431 iteration  19\nAICc -83.0956 logPost -13809.0 logLik  51.4431 edf 7.8949 eps 0.0096 iteration  20\nAICc -83.2026 logPost -13815.6 logLik  51.4812 edf 7.8848 eps 0.0058 iteration  21\nAICc -83.2582 logPost -13816.0 logLik  51.5109 edf 7.8861 eps 0.0574 iteration  22\nAICc -83.3488 logPost -13815.9 logLik  51.5729 edf 7.8970 eps 0.0084 iteration  23\nAICc -83.4298 logPost -13815.7 logLik  51.5929 edf 7.8836 eps 0.0035 iteration  24\nAICc -83.4578 logPost -13815.6 logLik  51.6069 edf 7.8837 eps 0.0029 iteration  25\nAICc -83.4771 logPost -13815.4 logLik  51.6166 edf 7.8837 eps 0.0025 iteration  26\nAICc -83.4921 logPost -13815.3 logLik  51.6242 edf 7.8837 eps 0.0042 iteration  27\nAICc -83.5087 logPost -13815.3 logLik  51.6291 edf 7.8815 eps 0.0021 iteration  28\nAICc -83.5136 logPost -13815.2 logLik  51.6323 edf 7.8820 eps 0.0016 iteration  29\nAICc -83.5176 logPost -13815.1 logLik  51.6343 edf 7.8820 eps 0.0013 iteration  30\nAICc -83.5199 logPost -13815.1 logLik  51.6355 edf 7.8820 eps 0.0011 iteration  31\nAICc -83.5219 logPost -13815.1 logLik  51.6365 edf 7.8820 eps 0.0042 iteration  32\nAICc -83.5216 logPost -13815.0 logLik  51.6382 edf 7.8832 eps 0.0022 iteration  33\nAICc -83.5300 logPost -13815.0 logLik  51.6383 edf 7.8806 eps 0.0009 iteration  34\nAICc -83.5294 logPost -13815.0 logLik  51.6384 edf 7.8808 eps 0.0003 iteration  35\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  36\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  37\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  38\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  39\nAICc -83.5293 logPost -13815.0 logLik  51.6384 edf 7.8809 eps 0.0000 iteration  39\nelapsed time:  2.67sec\nStarting the sampler...\n\n|                    |   0%  2.57sec\n|*                   |   5%  2.47sec  0.13sec\n|**                  |  10%  2.26sec  0.25sec\n|***                 |  15%  2.20sec  0.39sec\n|****                |  20%  2.08sec  0.52sec\n|*****               |  25%  1.93sec  0.64sec\n|******              |  30%  1.81sec  0.78sec\n|*******             |  35%  1.70sec  0.92sec\n|********            |  40%  1.60sec  1.07sec\n|*********           |  45%  1.47sec  1.20sec\n|**********          |  50%  1.34sec  1.34sec\n|***********         |  55%  1.21sec  1.48sec\n|************        |  60%  1.08sec  1.61sec\n|*************       |  65%  0.94sec  1.75sec\n|**************      |  70%  0.81sec  1.89sec\n|***************     |  75%  0.68sec  2.03sec\n|****************    |  80%  0.54sec  2.16sec\n|*****************   |  85%  0.41sec  2.30sec\n|******************  |  90%  0.27sec  2.44sec\n|******************* |  95%  0.14sec  2.57sec\n|********************| 100%  0.00sec  2.71sec\n\n## Bayesian model shrinks the effects compared to ML\nplot(accuracy ~ iq, data = ReadingSkills, pch = 19, col = dyslexia)\nnd &lt;- data.frame(\n  iq = rep(-19:20/10, 2),\n  dyslexia = factor(rep(c(\"no\", \"yes\"), each = 40), levels = c(\"no\", \"yes\"))\n)\nnd$betareg &lt;- predict(rs1, newdata = nd, type = \"response\")\nnd$bamlss  &lt;- predict(rs2, newdata = nd, type = \"parameter\", model = \"mu\")\nlines(betareg ~ iq, data = nd, subset = dyslexia == \"no\",  col = 1, lwd = 2, lty = 1)\nlines(betareg ~ iq, data = nd, subset = dyslexia == \"yes\", col = 2, lwd = 2, lty = 1)\nlines(bamlss  ~ iq, data = nd, subset = dyslexia == \"no\",  col = 1, lwd = 2, lty = 2)\nlines(bamlss  ~ iq, data = nd, subset = dyslexia == \"yes\", col = 2, lwd = 2, lty = 2)\nlegend(\"topleft\", c(\"Dyslexia: no\", \"Dyslexia: yes\", \"betareg\", \"bamlss\"),\n  lty = c(0, 0, 1, 2), pch = c(19, 19, NA, NA), col = c(1, 2, 1, 1), bty = \"n\")\n\n\n\n\n\n\n## IGNORE_RDIFF_END\n\n## xbetax_family(): requires more time due to Gaussian quadrature\n## for gamlss2: install.packages(\"gamlss2\", repos = \"https://gamlss-dev.R-universe.dev\")",
    "crumbs": [
      "Beta regression extensions",
      "betar_family"
    ]
  },
  {
    "objectID": "man/residuals.betareg.html",
    "href": "man/residuals.betareg.html",
    "title": "betareg",
    "section": "",
    "text": "Extract various types of residuals from beta regression models: raw response residuals (observed - fitted), Pearson residuals (raw residuals scaled by square root of variance function), deviance residuals (scaled log-likelihood contributions), and different kinds of weighted residuals suggested by Espinheira et al. (2008).\n\n## S3 method for class 'betareg'\nresiduals(object, type = c(\"quantile\",\n  \"deviance\", \"pearson\", \"response\", \"weighted\", \"sweighted\", \"sweighted2\"),\n  ...)\n\n\n\n\n\nobject\n\n\nfitted model object of class “betareg”.\n\n\n\n\ntype\n\n\ncharacter indicating type of residuals.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nThe default residuals (starting from version 3.2-0) are quantile residuals as proposed by Dunn and Smyth (1996) and explored in the context of beta regression by Pereira (2017). In case of extended-support beta regression with boundary observations at 0 and/or 1, the quantile residuals for the boundary observations are randomized.\nThe definitions of all other residuals are provided in Espinheira et al. (2008): Equation 2 for “pearson”, last equation on page 409 for “deviance”, Equation 6 for “weighted”, Equation 7 for “sweighted”, and Equation 8 for “sweighted2”.\nEspinheira et al. (2008) recommend to use “sweighted2”, hence this was the default prior to version 3.2-0. However, these are rather burdensome to compute because they require operations of \\(O(n^2)\\) and hence are typically prohibitively costly in large sample. Also they are not available for extended-support beta regression. Finally, Pereira (2017) found quantile residuals to have better distributional properties.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nDunn PK, Smyth GK (1996). Randomized Quantile Residuals. Journal of Computational and Graphical Statistics, 5(3), 236–244. doi:10.2307/1390802\nEspinheira PL, Ferrari SLP, Cribari-Neto F (2008). On Beta Regression Residuals. Journal of Applied Statistics, 35(4), 407–419. doi:10.1080/02664760701834931\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815. doi:10.1080/0266476042000214501\nPereira GHA (2017). On Quantile Residuals in Beta Regression. Communications in Statistics – Simulation and Computation, 48(1), 302–316. doi:10.1080/03610918.2017.1381740\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy &lt;- betareg(yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\ngy_res &lt;- cbind(\n  \"quantile\"   = residuals(gy, type = \"quantile\"),\n  \"pearson\"    = residuals(gy, type = \"pearson\"),\n  \"deviance\"   = residuals(gy, type = \"deviance\"),\n  \"response\"   = residuals(gy, type = \"response\"),\n  \"weighted\"   = residuals(gy, type = \"weighted\"),\n  \"sweighted\"  = residuals(gy, type = \"sweighted\"),\n  \"sweighted2\" = residuals(gy, type = \"sweighted2\")\n)\npairs(gy_res)\n\n\n\n\n\n\ncor(gy_res)\n\n           quantile pearson deviance response weighted sweighted sweighted2\nquantile     1.0000  0.9980   0.9997   0.9659   0.9995    0.9995     0.9980\npearson      0.9980  1.0000   0.9984   0.9739   0.9956    0.9956     0.9941\ndeviance     0.9997  0.9984   1.0000   0.9682   0.9989    0.9989     0.9976\nresponse     0.9659  0.9739   0.9682   1.0000   0.9609    0.9609     0.9652\nweighted     0.9995  0.9956   0.9989   0.9609   1.0000    1.0000     0.9985\nsweighted    0.9995  0.9956   0.9989   0.9609   1.0000    1.0000     0.9985\nsweighted2   0.9980  0.9941   0.9976   0.9652   0.9985    0.9985     1.0000",
    "crumbs": [
      "Beta regression",
      "residuals.betareg"
    ]
  },
  {
    "objectID": "man/residuals.betareg.html#residuals-method-for-betareg-objects",
    "href": "man/residuals.betareg.html#residuals-method-for-betareg-objects",
    "title": "betareg",
    "section": "",
    "text": "Extract various types of residuals from beta regression models: raw response residuals (observed - fitted), Pearson residuals (raw residuals scaled by square root of variance function), deviance residuals (scaled log-likelihood contributions), and different kinds of weighted residuals suggested by Espinheira et al. (2008).\n\n## S3 method for class 'betareg'\nresiduals(object, type = c(\"quantile\",\n  \"deviance\", \"pearson\", \"response\", \"weighted\", \"sweighted\", \"sweighted2\"),\n  ...)\n\n\n\n\n\nobject\n\n\nfitted model object of class “betareg”.\n\n\n\n\ntype\n\n\ncharacter indicating type of residuals.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nThe default residuals (starting from version 3.2-0) are quantile residuals as proposed by Dunn and Smyth (1996) and explored in the context of beta regression by Pereira (2017). In case of extended-support beta regression with boundary observations at 0 and/or 1, the quantile residuals for the boundary observations are randomized.\nThe definitions of all other residuals are provided in Espinheira et al. (2008): Equation 2 for “pearson”, last equation on page 409 for “deviance”, Equation 6 for “weighted”, Equation 7 for “sweighted”, and Equation 8 for “sweighted2”.\nEspinheira et al. (2008) recommend to use “sweighted2”, hence this was the default prior to version 3.2-0. However, these are rather burdensome to compute because they require operations of \\(O(n^2)\\) and hence are typically prohibitively costly in large sample. Also they are not available for extended-support beta regression. Finally, Pereira (2017) found quantile residuals to have better distributional properties.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nDunn PK, Smyth GK (1996). Randomized Quantile Residuals. Journal of Computational and Graphical Statistics, 5(3), 236–244. doi:10.2307/1390802\nEspinheira PL, Ferrari SLP, Cribari-Neto F (2008). On Beta Regression Residuals. Journal of Applied Statistics, 35(4), 407–419. doi:10.1080/02664760701834931\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815. doi:10.1080/0266476042000214501\nPereira GHA (2017). On Quantile Residuals in Beta Regression. Communications in Statistics – Simulation and Computation, 48(1), 302–316. doi:10.1080/03610918.2017.1381740\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\n\nbetareg\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy &lt;- betareg(yield ~ gravity + pressure + temp10 + temp, data = GasolineYield)\n\ngy_res &lt;- cbind(\n  \"quantile\"   = residuals(gy, type = \"quantile\"),\n  \"pearson\"    = residuals(gy, type = \"pearson\"),\n  \"deviance\"   = residuals(gy, type = \"deviance\"),\n  \"response\"   = residuals(gy, type = \"response\"),\n  \"weighted\"   = residuals(gy, type = \"weighted\"),\n  \"sweighted\"  = residuals(gy, type = \"sweighted\"),\n  \"sweighted2\" = residuals(gy, type = \"sweighted2\")\n)\npairs(gy_res)\n\n\n\n\n\n\ncor(gy_res)\n\n           quantile pearson deviance response weighted sweighted sweighted2\nquantile     1.0000  0.9980   0.9997   0.9659   0.9995    0.9995     0.9980\npearson      0.9980  1.0000   0.9984   0.9739   0.9956    0.9956     0.9941\ndeviance     0.9997  0.9984   1.0000   0.9682   0.9989    0.9989     0.9976\nresponse     0.9659  0.9739   0.9682   1.0000   0.9609    0.9609     0.9652\nweighted     0.9995  0.9956   0.9989   0.9609   1.0000    1.0000     0.9985\nsweighted    0.9995  0.9956   0.9989   0.9609   1.0000    1.0000     0.9985\nsweighted2   0.9980  0.9941   0.9976   0.9652   0.9985    0.9985     1.0000",
    "crumbs": [
      "Beta regression",
      "residuals.betareg"
    ]
  },
  {
    "objectID": "man/Beta01.html",
    "href": "man/Beta01.html",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for zero- and/or one-inflated beta distributions in regression specification using the workflow from the distributions3 package.\n\nBeta01(mu, phi, p0 = 0, p1 = 0)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution (on the open unit interval).\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\n\np0\n\n\nnumeric. The probability for an observation of zero (often referred to as zero inflation).\n\n\n\n\np1\n\n\nnumeric. The probability for an observation of one (often referred to as one inflation).\n\n\n\nThe zero- and/or one-inflated beta distribution is obtained by adding point masses at zero and/or one to a standard beta distribution.\nNote that the support of the standard beta distribution is the open unit interval where values of exactly zero or one cannot occur. Thus, the inflation jargon is rather misleading as there is no probability that could be inflated. It is rather a hurdle or two-part (or three-part) model.\n\nA Beta01 distribution object.\n\ndbeta01, BetaR\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- Beta01(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  p0 = c(0.1, 0, 0),\n  p1 = c(0, 0, 0.3)\n)\n\nX\n\n[1] \"Beta01(mu = 0.25, phi = 1, p0 = 0.1, p1 = 0.0)\"\n[2] \"Beta01(mu = 0.50, phi = 1, p0 = 0.0, p1 = 0.0)\"\n[3] \"Beta01(mu = 0.75, phi = 2, p0 = 0.0, p1 = 0.3)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.225 0.500 0.825\n\nvariance(X)\n\n[1] 0.090000 0.125000 0.056875\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## simulate random variables\nrandom(X, 5)\n\n            r_1         r_2         r_3        r_4       r_5\n[1,] 0.01770077 0.031967958 0.009185013 0.09774271 0.5297302\n[2,] 0.11888790 0.004743561 0.209872794 0.56026234 0.7010201\n[3,] 0.43181699 0.757365599 0.612582662 0.76991426 0.3122026\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6156832 0.6366198 0.7718605\n\npdf(X, x, log = TRUE)\n\n[1] -0.4850227 -0.4515827 -0.2589515\n\nlog_pdf(X, x)\n\n[1] -0.4850227 -0.4515827 -0.2589515\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6808374 0.5000000 0.2737016\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.05868041 0.50000000 0.94876688\n\n## cdf() and quantile() are inverses\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## point mass probabilities (if any) on boundary\ncdf(X, 0, lower.tail = TRUE)\n\n[1] 0.1 0.0 0.0\n\ncdf(X, 1, lower.tail = FALSE)\n\n[1] 0.0 0.0 0.3\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n         q_0.05      q_0.5    q_0.95\n[1,] 0.00000000 0.05868041 0.8991438\n[2,] 0.00615583 0.50000000 0.9938442\n[3,] 0.28573175 0.94876688 1.0000000\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 0.0 0.5 1.0\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n     quantile\n[1,]      0.0\n[2,]      0.5\n[3,]      1.0\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]       0.225 0.2275291\n[2,]       0.500 0.5091324\n[3,]       0.825 0.8103725",
    "crumbs": [
      "distributions3 objects",
      "Beta01"
    ]
  },
  {
    "objectID": "man/Beta01.html#create-a-zero--andor-one-inflated-beta-distribution",
    "href": "man/Beta01.html#create-a-zero--andor-one-inflated-beta-distribution",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for zero- and/or one-inflated beta distributions in regression specification using the workflow from the distributions3 package.\n\nBeta01(mu, phi, p0 = 0, p1 = 0)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the beta distribution (on the open unit interval).\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the beta distribution.\n\n\n\n\np0\n\n\nnumeric. The probability for an observation of zero (often referred to as zero inflation).\n\n\n\n\np1\n\n\nnumeric. The probability for an observation of one (often referred to as one inflation).\n\n\n\nThe zero- and/or one-inflated beta distribution is obtained by adding point masses at zero and/or one to a standard beta distribution.\nNote that the support of the standard beta distribution is the open unit interval where values of exactly zero or one cannot occur. Thus, the inflation jargon is rather misleading as there is no probability that could be inflated. It is rather a hurdle or two-part (or three-part) model.\n\nA Beta01 distribution object.\n\ndbeta01, BetaR\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- Beta01(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  p0 = c(0.1, 0, 0),\n  p1 = c(0, 0, 0.3)\n)\n\nX\n\n[1] \"Beta01(mu = 0.25, phi = 1, p0 = 0.1, p1 = 0.0)\"\n[2] \"Beta01(mu = 0.50, phi = 1, p0 = 0.0, p1 = 0.0)\"\n[3] \"Beta01(mu = 0.75, phi = 2, p0 = 0.0, p1 = 0.3)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.225 0.500 0.825\n\nvariance(X)\n\n[1] 0.090000 0.125000 0.056875\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## simulate random variables\nrandom(X, 5)\n\n            r_1         r_2         r_3        r_4       r_5\n[1,] 0.01770077 0.031967958 0.009185013 0.09774271 0.5297302\n[2,] 0.11888790 0.004743561 0.209872794 0.56026234 0.7010201\n[3,] 0.43181699 0.757365599 0.612582662 0.76991426 0.3122026\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6156832 0.6366198 0.7718605\n\npdf(X, x, log = TRUE)\n\n[1] -0.4850227 -0.4515827 -0.2589515\n\nlog_pdf(X, x)\n\n[1] -0.4850227 -0.4515827 -0.2589515\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6808374 0.5000000 0.2737016\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.05868041 0.50000000 0.94876688\n\n## cdf() and quantile() are inverses\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## point mass probabilities (if any) on boundary\ncdf(X, 0, lower.tail = TRUE)\n\n[1] 0.1 0.0 0.0\n\ncdf(X, 1, lower.tail = FALSE)\n\n[1] 0.0 0.0 0.3\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n         q_0.05      q_0.5    q_0.95\n[1,] 0.00000000 0.05868041 0.8991438\n[2,] 0.00615583 0.50000000 0.9938442\n[3,] 0.28573175 0.94876688 1.0000000\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 0.0 0.5 1.0\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n     quantile\n[1,]      0.0\n[2,]      0.5\n[3,]      1.0\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]       0.225 0.2275291\n[2,]       0.500 0.5091324\n[3,]       0.825 0.8103725",
    "crumbs": [
      "distributions3 objects",
      "Beta01"
    ]
  },
  {
    "objectID": "man/XBetaX.html",
    "href": "man/XBetaX.html",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for extended-support beta distributions using the workflow from the distributions3 package.\n\nXBetaX(mu, phi, nu = 0)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Mean of the exponentially-distributed exceedence parameter for the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\nThe extended-support beta mixture distribution is a continuous mixture of extended-support beta distributions on [0, 1] where the underlying exceedence parameter is exponentially distributed with mean nu. Thus, if nu &gt; 0, the resulting distribution has point masses on the boundaries 0 and 1 with larger values of nu leading to higher boundary probabilities. For nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\nA XBetaX distribution object.\n\ndxbetax, XBeta\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- XBetaX(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  nu = c(0, 0.1, 0.2)\n)\n\nX\n\n[1] \"XBetaX(mu = 0.25, phi = 1, nu = 0.0)\"\n[2] \"XBetaX(mu = 0.50, phi = 1, nu = 0.1)\"\n[3] \"XBetaX(mu = 0.75, phi = 2, nu = 0.2)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.2500000 0.5000000 0.7812779\n\nvariance(X)\n\n[1] 0.09375000 0.14932803 0.08290156\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## it is only continuous when there are no point masses on the boundary\nis_continuous(X)\n\n[1]  TRUE FALSE FALSE\n\ncdf(X, 0)\n\n[1] 0.00000000 0.16127596 0.02230181\n\ncdf(X, 1, lower.tail = FALSE)\n\n[1] 0.0000000 0.1612760 0.4004398\n\n## simulate random variables\nrandom(X, 5)\n\n            r_1        r_2         r_3       r_4        r_5\n[1,] 0.01770077 0.03196796 0.009185013 0.3511111 0.03417845\n[2,] 0.03065274 0.00000000 0.159692910 0.4697419 0.62766314\n[3,] 0.40195437 1.00000000 0.675081997 0.9255527 1.00000000\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.5424706 0.7405552\n\npdf(X, x, log = TRUE)\n\n[1] -0.3796622 -0.6116213 -0.3003551\n\nlog_pdf(X, x)\n\n[1] -0.3796622 -0.6116213 -0.3003551\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.3312063\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 0.93231291\n\n## cdf() and quantile() are inverses (except at censoring points)\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n           q_0.05      q_0.5    q_0.95\n[1,] 9.512588e-06 0.09331223 0.9118445\n[2,] 0.000000e+00 0.50000000 1.0000000\n[3,] 1.353857e-01 0.93231291 1.0000000\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 1.000000e+00\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 1.000000e+00\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]   0.2500000 0.2403159\n[2,]   0.5000000 0.4935615\n[3,]   0.7812779 0.7936076",
    "crumbs": [
      "distributions3 objects",
      "XBetaX"
    ]
  },
  {
    "objectID": "man/XBetaX.html#create-an-extended-support-beta-mixture-distribution",
    "href": "man/XBetaX.html#create-an-extended-support-beta-mixture-distribution",
    "title": "betareg",
    "section": "",
    "text": "Class and methods for extended-support beta distributions using the workflow from the distributions3 package.\n\nXBetaX(mu, phi, nu = 0)\n\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Mean of the exponentially-distributed exceedence parameter for the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\nThe extended-support beta mixture distribution is a continuous mixture of extended-support beta distributions on [0, 1] where the underlying exceedence parameter is exponentially distributed with mean nu. Thus, if nu &gt; 0, the resulting distribution has point masses on the boundaries 0 and 1 with larger values of nu leading to higher boundary probabilities. For nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\nA XBetaX distribution object.\n\ndxbetax, XBeta\n\n\nlibrary(\"betareg\")\n\n\n## package and random seed\nlibrary(\"distributions3\")\nset.seed(6020)\n\n## three beta distributions\nX &lt;- XBetaX(\n  mu  = c(0.25, 0.50, 0.75),\n  phi = c(1, 1, 2),\n  nu = c(0, 0.1, 0.2)\n)\n\nX\n\n[1] \"XBetaX(mu = 0.25, phi = 1, nu = 0.0)\"\n[2] \"XBetaX(mu = 0.50, phi = 1, nu = 0.1)\"\n[3] \"XBetaX(mu = 0.75, phi = 2, nu = 0.2)\"\n\n## compute moments of the distribution\nmean(X)\n\n[1] 0.2500000 0.5000000 0.7812779\n\nvariance(X)\n\n[1] 0.09375000 0.14932803 0.08290156\n\n## support interval (minimum and maximum)\nsupport(X)\n\n     min max\n[1,]   0   1\n[2,]   0   1\n[3,]   0   1\n\n## it is only continuous when there are no point masses on the boundary\nis_continuous(X)\n\n[1]  TRUE FALSE FALSE\n\ncdf(X, 0)\n\n[1] 0.00000000 0.16127596 0.02230181\n\ncdf(X, 1, lower.tail = FALSE)\n\n[1] 0.0000000 0.1612760 0.4004398\n\n## simulate random variables\nrandom(X, 5)\n\n            r_1        r_2         r_3       r_4        r_5\n[1,] 0.01770077 0.03196796 0.009185013 0.3511111 0.03417845\n[2,] 0.03065274 0.00000000 0.159692910 0.4697419 0.62766314\n[3,] 0.40195437 1.00000000 0.675081997 0.9255527 1.00000000\n\n## histograms of 1,000 simulated observations\nx &lt;- random(X, 1000)\nhist(x[1, ])\n\n\n\n\n\n\nhist(x[2, ])\n\n\n\n\n\n\nhist(x[3, ])\n\n\n\n\n\n\n## probability density function (PDF) and log-density (or log-likelihood)\nx &lt;- c(0.25, 0.5, 0.75)\npdf(X, x)\n\n[1] 0.6840925 0.5424706 0.7405552\n\npdf(X, x, log = TRUE)\n\n[1] -0.3796622 -0.6116213 -0.3003551\n\nlog_pdf(X, x)\n\n[1] -0.3796622 -0.6116213 -0.3003551\n\n## cumulative distribution function (CDF)\ncdf(X, x)\n\n[1] 0.6453748 0.5000000 0.3312063\n\n## quantiles\nquantile(X, 0.5)\n\n[1] 0.09331223 0.50000000 0.93231291\n\n## cdf() and quantile() are inverses (except at censoring points)\ncdf(X, quantile(X, 0.5))\n\n[1] 0.5 0.5 0.5\n\nquantile(X, cdf(X, 1))\n\n[1] 1 1 1\n\n## all methods above can either be applied elementwise or for\n## all combinations of X and x, if length(X) = length(x),\n## also the result can be assured to be a matrix via drop = FALSE\np &lt;- c(0.05, 0.5, 0.95)\nquantile(X, p, elementwise = FALSE)\n\n           q_0.05      q_0.5    q_0.95\n[1,] 9.512588e-06 0.09331223 0.9118445\n[2,] 0.000000e+00 0.50000000 1.0000000\n[3,] 1.353857e-01 0.93231291 1.0000000\n\nquantile(X, p, elementwise = TRUE)\n\n[1] 9.512588e-06 5.000000e-01 1.000000e+00\n\nquantile(X, p, elementwise = TRUE, drop = FALSE)\n\n         quantile\n[1,] 9.512588e-06\n[2,] 5.000000e-01\n[3,] 1.000000e+00\n\n## compare theoretical and empirical mean from 1,000 simulated observations\ncbind(\n  \"theoretical\" = mean(X),\n  \"empirical\" = rowMeans(random(X, 1000))\n)\n\n     theoretical empirical\n[1,]   0.2500000 0.2403159\n[2,]   0.5000000 0.4935615\n[3,]   0.7812779 0.7936076",
    "crumbs": [
      "distributions3 objects",
      "XBetaX"
    ]
  },
  {
    "objectID": "man/WeatherTask.html",
    "href": "man/WeatherTask.html",
    "title": "betareg",
    "section": "",
    "text": "In this study participants were asked to judge how likely Sunday is to be the hottest day of the week.\n\ndata(\"WeatherTask\", package = \"betareg\")\n\nA data frame with 345 observations on the following 3 variables.\n\n\npriming\n\n\na factor with levels two-fold (case prime) and seven-fold (class prime).\n\n\neliciting\n\n\na factor with levels precise and imprecise (lower and upper limit).\n\n\nagreement\n\n\na numeric vector, probability indicated by participants or the average between minimum and maximum probability indicated.\n\n\nAll participants in the study were either first- or second-year undergraduate students in psychology, none of whom had a strong background in probability or were familiar with imprecise probability theories.\nFor priming the questions were:\n\n\ntwo-fold\n\n\n[What is the probability that] the temperature at Canberra airport on Sunday will be higher than every other day next week?\n\n\nseven-fold\n\n\n[What is the probability that] the highest temperature of the week at Canberra airport will occur on Sunday?\n\n\nFor eliciting the instructions were if\n\n\nprecise\n\n\nto assign a probability estimate,\n\n\nimprecise\n\n\nto assign a lower and upper probability estimate.\n\n\nTaken from Smithson et al. (2011) supplements.\n\nSmithson M, Merkle EC, Verkuilen J (2011). Beta Regression Finite Mixture Models of Polarization and Priming. Journal of Educational and Behavioral Statistics, 36(6), 804–831. doi:10.3102/1076998610396893\nSmithson M, Segale C (2009). Partition Priming in Judgments of Imprecise Probabilities. Journal of Statistical Theory and Practice, 3(1), 169–181.\n\n\nlibrary(\"betareg\")\n\ndata(\"WeatherTask\", package = \"betareg\")\nlibrary(\"flexmix\")\nwt_betamix &lt;- betamix(agreement ~ 1, data = WeatherTask, k = 2,\n  extra_components = extraComponent(type = \"betareg\", coef =\n    list(mean = 0, precision = 2)),\n  FLXconcomitant = FLXPmultinom(~ priming + eliciting))",
    "crumbs": [
      "Data sets",
      "WeatherTask"
    ]
  },
  {
    "objectID": "man/WeatherTask.html#weather-task-with-priming-and-precise-and-imprecise-probabilities",
    "href": "man/WeatherTask.html#weather-task-with-priming-and-precise-and-imprecise-probabilities",
    "title": "betareg",
    "section": "",
    "text": "In this study participants were asked to judge how likely Sunday is to be the hottest day of the week.\n\ndata(\"WeatherTask\", package = \"betareg\")\n\nA data frame with 345 observations on the following 3 variables.\n\n\npriming\n\n\na factor with levels two-fold (case prime) and seven-fold (class prime).\n\n\neliciting\n\n\na factor with levels precise and imprecise (lower and upper limit).\n\n\nagreement\n\n\na numeric vector, probability indicated by participants or the average between minimum and maximum probability indicated.\n\n\nAll participants in the study were either first- or second-year undergraduate students in psychology, none of whom had a strong background in probability or were familiar with imprecise probability theories.\nFor priming the questions were:\n\n\ntwo-fold\n\n\n[What is the probability that] the temperature at Canberra airport on Sunday will be higher than every other day next week?\n\n\nseven-fold\n\n\n[What is the probability that] the highest temperature of the week at Canberra airport will occur on Sunday?\n\n\nFor eliciting the instructions were if\n\n\nprecise\n\n\nto assign a probability estimate,\n\n\nimprecise\n\n\nto assign a lower and upper probability estimate.\n\n\nTaken from Smithson et al. (2011) supplements.\n\nSmithson M, Merkle EC, Verkuilen J (2011). Beta Regression Finite Mixture Models of Polarization and Priming. Journal of Educational and Behavioral Statistics, 36(6), 804–831. doi:10.3102/1076998610396893\nSmithson M, Segale C (2009). Partition Priming in Judgments of Imprecise Probabilities. Journal of Statistical Theory and Practice, 3(1), 169–181.\n\n\nlibrary(\"betareg\")\n\ndata(\"WeatherTask\", package = \"betareg\")\nlibrary(\"flexmix\")\nwt_betamix &lt;- betamix(agreement ~ 1, data = WeatherTask, k = 2,\n  extra_components = extraComponent(type = \"betareg\", coef =\n    list(mean = 0, precision = 2)),\n  FLXconcomitant = FLXPmultinom(~ priming + eliciting))",
    "crumbs": [
      "Data sets",
      "WeatherTask"
    ]
  },
  {
    "objectID": "man/StressAnxiety.html",
    "href": "man/StressAnxiety.html",
    "title": "betareg",
    "section": "",
    "text": "Stress and anxiety among nonclinical women in Townsville, Queensland, Australia.\n\ndata(\"StressAnxiety\", package = \"betareg\")\n\nA data frame containing 166 observations on 2 variables.\n\n\nstress\n\n\nscore, linearly transformed to the open unit interval (see below).\n\n\nanxiety\n\n\nscore, linearly transformed to the open unit interval (see below).\n\n\nBoth variables were assess on the Depression Anxiety Stress Scales, ranging from 0 to 42. Smithson and Verkuilen (2006) transformed these to the open unit interval (without providing details about this transformation).\n\nExample 2 from Smithson and Verkuilen (2006) supplements.\n\nSmithson M, Verkuilen J (2006). A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables. Psychological Methods, 11(7), 54–71.\n\nbetareg, MockJurors, ReadingSkills\n\n\nlibrary(\"betareg\")\n\ndata(\"StressAnxiety\", package = \"betareg\")\nStressAnxiety &lt;- StressAnxiety[order(StressAnxiety$stress),]\n\n## Smithson & Verkuilen (2006, Table 4)\nsa_null &lt;- betareg(anxiety ~ 1 | 1,\n  data = StressAnxiety, hessian = TRUE)\nsa_stress &lt;- betareg(anxiety ~ stress | stress,\n  data = StressAnxiety, hessian = TRUE)\nsummary(sa_null)\n\n\nCall:\nbetareg(formula = anxiety ~ 1 | 1, data = StressAnxiety, hessian = TRUE)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-0.8377 -0.8377 -0.4467  0.6217  3.2396 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.24396    0.09879  -22.71   &lt;2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.796      0.123    14.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 239.4 on 2 Df\nNumber of iterations in BFGS optimization: 9 \n\nsummary(sa_stress)\n\n\nCall:\nbetareg(formula = anxiety ~ stress | stress, data = StressAnxiety, hessian = TRUE)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.0119 -0.7953 -0.1833  0.5658  3.1141 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -4.0237     0.1442  -27.90   &lt;2e-16 ***\nstress        4.9414     0.4409   11.21   &lt;2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   3.9608     0.2511  15.776  &lt; 2e-16 ***\nstress       -4.2733     0.7532  -5.674  1.4e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   302 on 4 Df\nPseudo R-squared: 0.4748\nNumber of iterations in BFGS optimization: 16 \n\nAIC(sa_null, sa_stress)\n\n          df       AIC\nsa_null    2 -474.8960\nsa_stress  4 -595.9202\n\n1 - as.vector(logLik(sa_null)/logLik(sa_stress))\n\n[1] 0.207021\n\n## visualization\nattach(StressAnxiety)\nplot(jitter(anxiety) ~ jitter(stress),\n  xlab = \"Stress\", ylab = \"Anxiety\",\n  xlim = c(0, 1), ylim = c(0, 1))\nlines(lowess(anxiety ~ stress))\nlines(fitted(sa_stress) ~ stress, lty = 2)\nlines(fitted(lm(anxiety ~ stress)) ~ stress, lty = 3)\nlegend(\"topleft\", c(\"lowess\", \"betareg\", \"lm\"), lty = 1:3, bty = \"n\")\n\n\n\n\n\n\ndetach(StressAnxiety)\n\n## see demo(\"SmithsonVerkuilen2006\", package = \"betareg\") for more details",
    "crumbs": [
      "Data sets",
      "StressAnxiety"
    ]
  },
  {
    "objectID": "man/StressAnxiety.html#dependency-of-anxiety-on-stress",
    "href": "man/StressAnxiety.html#dependency-of-anxiety-on-stress",
    "title": "betareg",
    "section": "",
    "text": "Stress and anxiety among nonclinical women in Townsville, Queensland, Australia.\n\ndata(\"StressAnxiety\", package = \"betareg\")\n\nA data frame containing 166 observations on 2 variables.\n\n\nstress\n\n\nscore, linearly transformed to the open unit interval (see below).\n\n\nanxiety\n\n\nscore, linearly transformed to the open unit interval (see below).\n\n\nBoth variables were assess on the Depression Anxiety Stress Scales, ranging from 0 to 42. Smithson and Verkuilen (2006) transformed these to the open unit interval (without providing details about this transformation).\n\nExample 2 from Smithson and Verkuilen (2006) supplements.\n\nSmithson M, Verkuilen J (2006). A Better Lemon Squeezer? Maximum-Likelihood Regression with Beta-Distributed Dependent Variables. Psychological Methods, 11(7), 54–71.\n\nbetareg, MockJurors, ReadingSkills\n\n\nlibrary(\"betareg\")\n\ndata(\"StressAnxiety\", package = \"betareg\")\nStressAnxiety &lt;- StressAnxiety[order(StressAnxiety$stress),]\n\n## Smithson & Verkuilen (2006, Table 4)\nsa_null &lt;- betareg(anxiety ~ 1 | 1,\n  data = StressAnxiety, hessian = TRUE)\nsa_stress &lt;- betareg(anxiety ~ stress | stress,\n  data = StressAnxiety, hessian = TRUE)\nsummary(sa_null)\n\n\nCall:\nbetareg(formula = anxiety ~ 1 | 1, data = StressAnxiety, hessian = TRUE)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-0.8377 -0.8377 -0.4467  0.6217  3.2396 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.24396    0.09879  -22.71   &lt;2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.796      0.123    14.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 239.4 on 2 Df\nNumber of iterations in BFGS optimization: 9 \n\nsummary(sa_stress)\n\n\nCall:\nbetareg(formula = anxiety ~ stress | stress, data = StressAnxiety, hessian = TRUE)\n\nQuantile residuals:\n    Min      1Q  Median      3Q     Max \n-2.0119 -0.7953 -0.1833  0.5658  3.1141 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -4.0237     0.1442  -27.90   &lt;2e-16 ***\nstress        4.9414     0.4409   11.21   &lt;2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   3.9608     0.2511  15.776  &lt; 2e-16 ***\nstress       -4.2733     0.7532  -5.674  1.4e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   302 on 4 Df\nPseudo R-squared: 0.4748\nNumber of iterations in BFGS optimization: 16 \n\nAIC(sa_null, sa_stress)\n\n          df       AIC\nsa_null    2 -474.8960\nsa_stress  4 -595.9202\n\n1 - as.vector(logLik(sa_null)/logLik(sa_stress))\n\n[1] 0.207021\n\n## visualization\nattach(StressAnxiety)\nplot(jitter(anxiety) ~ jitter(stress),\n  xlab = \"Stress\", ylab = \"Anxiety\",\n  xlim = c(0, 1), ylim = c(0, 1))\nlines(lowess(anxiety ~ stress))\nlines(fitted(sa_stress) ~ stress, lty = 2)\nlines(fitted(lm(anxiety ~ stress)) ~ stress, lty = 3)\nlegend(\"topleft\", c(\"lowess\", \"betareg\", \"lm\"), lty = 1:3, bty = \"n\")\n\n\n\n\n\n\ndetach(StressAnxiety)\n\n## see demo(\"SmithsonVerkuilen2006\", package = \"betareg\") for more details",
    "crumbs": [
      "Data sets",
      "StressAnxiety"
    ]
  },
  {
    "objectID": "man/predict.betareg.html",
    "href": "man/predict.betareg.html",
    "title": "betareg",
    "section": "",
    "text": "Extract various types of predictions from beta regression models: First, GLM-style predictions on the scale of responses in (0, 1) or the scale of the linear predictor are provided. Second, various quantities based on the predicted beta distributions are available, e.g., moments, quantiles, probabilities, densities, etc.\n\n## S3 method for class 'betareg'\npredict(object, newdata = NULL,\n  type = c(\"response\", \"link\", \"precision\", \"variance\", \"parameters\",\n    \"distribution\", \"density\", \"probability\", \"quantile\"),\n  na.action = na.pass, at = 0.5, elementwise = NULL, ...)\n\n\n\n\n\nobject\n\n\nfitted model object of class “betareg”.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter indicating type of predictions: fitted means of the response (default, “response” or equivalently “mean”), corresponding linear predictor (“link”), fitted precision parameter phi (“precision”), fitted variances of the response (“variance”), all “parameters” of the response distribution, or the corresponding “distribution” object (using the infrastructure from distributions3). Finally, standard functions for the distribution can be evaluated (at argument at, see below), namely the “density” (or equivalently “pdf”), the “quantile” function, or the cumulative “probability” (or equivalently “cdf”).\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nat\n\n\nnumeric vector at which the predictions should be evaluated if type specifies a function that takes an additional argument.\n\n\n\n\nelementwise\n\n\nlogical. Should each element of the distribution only be evaluated at the corresponding element of at (elementwise = TRUE) or at all elements in at (elementwise = FALSE). Elementwise evaluation is only possible if the number of observations is the same as the length of at and in that case a vector of the same length is returned. Otherwise a matrix is returned. The default is to use elementwise = TRUE if possible, and otherwise elementwise = FALSE.\n\n\n\n\n…\n\n\nfurther arguments when type specifies a function, e.g., type = “density” with log = TRUE computes log-densities (or log-likelihoods).\n\n\n\nEach prediction for a betareg model internally first computes the parameters for all observations in newdata (or the original data if newdata is missing). These parameters correspond to a predicted beta distribution (or extended-support beta distribution) for each observation. Then the actual predictions can also be moments of the distributions or standard quantities such as densities, cumulative probabilities, or quantiles. The latter are computed with the d/p/q functions such as dbetar (or dxbetax or dxbeta).\n\nEither a vector or matrix of predictions with the same number of observations as rows in newdata.\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\ncbind(\n  predict(gy2, type = \"response\"),\n  predict(gy2, type = \"link\"),\n  predict(gy2, type = \"precision\"),\n  predict(gy2, type = \"variance\"),\n  predict(gy2, type = \"quantile\", at = c(0.25, 0.5, 0.75))\n)\n\n                                      q_0.25   q_0.5  q_0.75\n1  0.09997 -2.1976   77.56 1.145e-03 0.07549 0.09653 0.12074\n2  0.18658 -1.4724  215.06 7.024e-04 0.16816 0.18561 0.20394\n3  0.32143 -0.7472  596.36 3.651e-04 0.30842 0.32123 0.33422\n4  0.47379 -0.1049 1471.75 1.693e-04 0.46500 0.47378 0.48256\n5  0.08568 -2.3676   93.73 8.269e-04 0.06490 0.08273 0.10328\n6  0.14212 -1.7978  208.89 5.809e-04 0.12525 0.14097 0.15774\n7  0.26285 -1.0312  614.00 3.151e-04 0.25073 0.26259 0.27469\n8  0.10324 -2.1617   85.88 1.066e-03 0.07972 0.10017 0.12344\n9  0.17652 -1.5401  205.86 7.027e-04 0.15806 0.17547 0.19384\n10 0.30245 -0.8357  554.46 3.798e-04 0.28916 0.30221 0.31547\n11 0.07881 -2.4587  120.07 5.996e-04 0.06119 0.07647 0.09390\n12 0.14365 -1.7853  309.57 3.961e-04 0.12981 0.14288 0.15665\n13 0.24751 -1.1120  798.12 2.331e-04 0.23709 0.24730 0.25769\n14 0.34394 -0.6458 1537.51 1.467e-04 0.33573 0.34387 0.35208\n15 0.16957 -1.5887  342.81 4.096e-04 0.15556 0.16892 0.18287\n16 0.27545 -0.9671  821.72 2.426e-04 0.26484 0.27527 0.28586\n17 0.33691 -0.6771 1235.66 1.806e-04 0.32780 0.33683 0.34594\n18 0.10548 -2.1378  191.40 4.904e-04 0.08984 0.10410 0.11962\n19 0.23606 -1.1744  742.04 2.427e-04 0.22542 0.23583 0.24645\n20 0.32316 -0.7393 1368.34 1.597e-04 0.31459 0.32308 0.33164\n21 0.05383 -2.8665  120.07 4.207e-04 0.03893 0.05137 0.06608\n22 0.07928 -2.4521  215.06 3.379e-04 0.06624 0.07798 0.09091\n23 0.16906 -1.5923  720.73 1.946e-04 0.15949 0.16876 0.17831\n24 0.27063 -0.9914 1677.97 1.176e-04 0.26326 0.27054 0.27789\n25 0.08270 -2.4062  248.80 3.037e-04 0.07039 0.08158 0.09380\n26 0.17116 -1.5774  798.12 1.775e-04 0.16202 0.17088 0.18000\n27 0.31885 -0.7590 2523.27 8.604e-05 0.31257 0.31881 0.32509\n28 0.12701 -1.9276  650.84 1.701e-04 0.11801 0.12663 0.13560\n29 0.23661 -1.1714 1885.42 9.575e-05 0.22995 0.23651 0.24316\n30 0.10508 -2.1420  798.12 1.177e-04 0.09759 0.10475 0.11221\n31 0.11952 -1.9970  978.71 1.074e-04 0.11239 0.11926 0.12637\n32 0.18402 -1.4894 1998.57 7.509e-05 0.17811 0.18391 0.18980\n\n## evaluate cumulative _p_robabilities for (small) new data set\ngyd &lt;- GasolineYield[c(1, 5, 10), ]\n\n## CDF at 0.1 for each observation\npredict(gy2, newdata = gyd, type = \"probability\", at = 0.1)\n\n        1         5        10 \n5.407e-01 7.165e-01 6.053e-40 \n\n## CDF at each combination of 0.1/0.2 and observations\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2))\n\n       p_0.1     p_0.2\n1  5.407e-01 9.933e-01\n5  7.165e-01 9.991e-01\n10 6.053e-40 5.828e-09\n\n## CDF at elementwise combinations of 0.1/0.2/0.3 and observations\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2, 0.3))\n\n     1      5     10 \n0.5407 0.9991 0.4549 \n\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2, 0.3), elementwise = TRUE)\n\n     1      5     10 \n0.5407 0.9991 0.4549 \n\n## CDF at all combinations of 0.1/0.2/0.3 and observations\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2, 0.3), elementwise = FALSE)\n\n       p_0.1     p_0.2  p_0.3\n1  5.407e-01 9.933e-01 1.0000\n5  7.165e-01 9.991e-01 1.0000\n10 6.053e-40 5.828e-09 0.4549",
    "crumbs": [
      "Beta regression",
      "predict.betareg"
    ]
  },
  {
    "objectID": "man/predict.betareg.html#prediction-method-for-betareg-objects",
    "href": "man/predict.betareg.html#prediction-method-for-betareg-objects",
    "title": "betareg",
    "section": "",
    "text": "Extract various types of predictions from beta regression models: First, GLM-style predictions on the scale of responses in (0, 1) or the scale of the linear predictor are provided. Second, various quantities based on the predicted beta distributions are available, e.g., moments, quantiles, probabilities, densities, etc.\n\n## S3 method for class 'betareg'\npredict(object, newdata = NULL,\n  type = c(\"response\", \"link\", \"precision\", \"variance\", \"parameters\",\n    \"distribution\", \"density\", \"probability\", \"quantile\"),\n  na.action = na.pass, at = 0.5, elementwise = NULL, ...)\n\n\n\n\n\nobject\n\n\nfitted model object of class “betareg”.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter indicating type of predictions: fitted means of the response (default, “response” or equivalently “mean”), corresponding linear predictor (“link”), fitted precision parameter phi (“precision”), fitted variances of the response (“variance”), all “parameters” of the response distribution, or the corresponding “distribution” object (using the infrastructure from distributions3). Finally, standard functions for the distribution can be evaluated (at argument at, see below), namely the “density” (or equivalently “pdf”), the “quantile” function, or the cumulative “probability” (or equivalently “cdf”).\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nat\n\n\nnumeric vector at which the predictions should be evaluated if type specifies a function that takes an additional argument.\n\n\n\n\nelementwise\n\n\nlogical. Should each element of the distribution only be evaluated at the corresponding element of at (elementwise = TRUE) or at all elements in at (elementwise = FALSE). Elementwise evaluation is only possible if the number of observations is the same as the length of at and in that case a vector of the same length is returned. Otherwise a matrix is returned. The default is to use elementwise = TRUE if possible, and otherwise elementwise = FALSE.\n\n\n\n\n…\n\n\nfurther arguments when type specifies a function, e.g., type = “density” with log = TRUE computes log-densities (or log-likelihoods).\n\n\n\nEach prediction for a betareg model internally first computes the parameters for all observations in newdata (or the original data if newdata is missing). These parameters correspond to a predicted beta distribution (or extended-support beta distribution) for each observation. Then the actual predictions can also be moments of the distributions or standard quantities such as densities, cumulative probabilities, or quantiles. The latter are computed with the d/p/q functions such as dbetar (or dxbetax or dxbeta).\n\nEither a vector or matrix of predictions with the same number of observations as rows in newdata.\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\ndata(\"GasolineYield\", package = \"betareg\")\n\ngy2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\ncbind(\n  predict(gy2, type = \"response\"),\n  predict(gy2, type = \"link\"),\n  predict(gy2, type = \"precision\"),\n  predict(gy2, type = \"variance\"),\n  predict(gy2, type = \"quantile\", at = c(0.25, 0.5, 0.75))\n)\n\n                                      q_0.25   q_0.5  q_0.75\n1  0.09997 -2.1976   77.56 1.145e-03 0.07549 0.09653 0.12074\n2  0.18658 -1.4724  215.06 7.024e-04 0.16816 0.18561 0.20394\n3  0.32143 -0.7472  596.36 3.651e-04 0.30842 0.32123 0.33422\n4  0.47379 -0.1049 1471.75 1.693e-04 0.46500 0.47378 0.48256\n5  0.08568 -2.3676   93.73 8.269e-04 0.06490 0.08273 0.10328\n6  0.14212 -1.7978  208.89 5.809e-04 0.12525 0.14097 0.15774\n7  0.26285 -1.0312  614.00 3.151e-04 0.25073 0.26259 0.27469\n8  0.10324 -2.1617   85.88 1.066e-03 0.07972 0.10017 0.12344\n9  0.17652 -1.5401  205.86 7.027e-04 0.15806 0.17547 0.19384\n10 0.30245 -0.8357  554.46 3.798e-04 0.28916 0.30221 0.31547\n11 0.07881 -2.4587  120.07 5.996e-04 0.06119 0.07647 0.09390\n12 0.14365 -1.7853  309.57 3.961e-04 0.12981 0.14288 0.15665\n13 0.24751 -1.1120  798.12 2.331e-04 0.23709 0.24730 0.25769\n14 0.34394 -0.6458 1537.51 1.467e-04 0.33573 0.34387 0.35208\n15 0.16957 -1.5887  342.81 4.096e-04 0.15556 0.16892 0.18287\n16 0.27545 -0.9671  821.72 2.426e-04 0.26484 0.27527 0.28586\n17 0.33691 -0.6771 1235.66 1.806e-04 0.32780 0.33683 0.34594\n18 0.10548 -2.1378  191.40 4.904e-04 0.08984 0.10410 0.11962\n19 0.23606 -1.1744  742.04 2.427e-04 0.22542 0.23583 0.24645\n20 0.32316 -0.7393 1368.34 1.597e-04 0.31459 0.32308 0.33164\n21 0.05383 -2.8665  120.07 4.207e-04 0.03893 0.05137 0.06608\n22 0.07928 -2.4521  215.06 3.379e-04 0.06624 0.07798 0.09091\n23 0.16906 -1.5923  720.73 1.946e-04 0.15949 0.16876 0.17831\n24 0.27063 -0.9914 1677.97 1.176e-04 0.26326 0.27054 0.27789\n25 0.08270 -2.4062  248.80 3.037e-04 0.07039 0.08158 0.09380\n26 0.17116 -1.5774  798.12 1.775e-04 0.16202 0.17088 0.18000\n27 0.31885 -0.7590 2523.27 8.604e-05 0.31257 0.31881 0.32509\n28 0.12701 -1.9276  650.84 1.701e-04 0.11801 0.12663 0.13560\n29 0.23661 -1.1714 1885.42 9.575e-05 0.22995 0.23651 0.24316\n30 0.10508 -2.1420  798.12 1.177e-04 0.09759 0.10475 0.11221\n31 0.11952 -1.9970  978.71 1.074e-04 0.11239 0.11926 0.12637\n32 0.18402 -1.4894 1998.57 7.509e-05 0.17811 0.18391 0.18980\n\n## evaluate cumulative _p_robabilities for (small) new data set\ngyd &lt;- GasolineYield[c(1, 5, 10), ]\n\n## CDF at 0.1 for each observation\npredict(gy2, newdata = gyd, type = \"probability\", at = 0.1)\n\n        1         5        10 \n5.407e-01 7.165e-01 6.053e-40 \n\n## CDF at each combination of 0.1/0.2 and observations\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2))\n\n       p_0.1     p_0.2\n1  5.407e-01 9.933e-01\n5  7.165e-01 9.991e-01\n10 6.053e-40 5.828e-09\n\n## CDF at elementwise combinations of 0.1/0.2/0.3 and observations\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2, 0.3))\n\n     1      5     10 \n0.5407 0.9991 0.4549 \n\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2, 0.3), elementwise = TRUE)\n\n     1      5     10 \n0.5407 0.9991 0.4549 \n\n## CDF at all combinations of 0.1/0.2/0.3 and observations\npredict(gy2, newdata = gyd, type = \"probability\", at = c(0.1, 0.2, 0.3), elementwise = FALSE)\n\n       p_0.1     p_0.2  p_0.3\n1  5.407e-01 9.933e-01 1.0000\n5  7.165e-01 9.991e-01 1.0000\n10 6.053e-40 5.828e-09 0.4549",
    "crumbs": [
      "Beta regression",
      "predict.betareg"
    ]
  },
  {
    "objectID": "man/betatree.html",
    "href": "man/betatree.html",
    "title": "betareg",
    "section": "",
    "text": "Fit beta regression trees via model-based recursive partitioning.\n\nbetatree(formula, partition,\n  data, subset = NULL, na.action = na.omit, weights, offset, cluster,\n  link = \"logit\", link.phi = \"log\", control = betareg.control(),\n  ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model of type y ~ x or y ~ x | z, specifying the variables influencing mean and precision of y, respectively. For details see betareg.\n\n\n\n\npartition\n\n\nsymbolic description of the partitioning variables, e.g., ~ p1 + p2. The argument partition can be omitted if formula is a three-part formula of type y ~ x | z | p1 + p2.\n\n\n\n\ndata, subset, na.action, weights, offset, cluster\n\n\narguments controlling data/model processing passed to mob.\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “identity”, “log”, “sqrt” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\ncontrol\n\n\na list of control arguments for the beta regression specified via betareg.control.\n\n\n\n\n…\n\n\nfurther control arguments for the recursive partitioning passed to mob_control.\n\n\n\nBeta regression trees are an application of model-based recursive partitioning (implemented in mob, see Zeileis et al. 2008) to beta regression (implemented in betareg, see Cribari-Neto and Zeileis 2010). See also Grün at al. (2012) for more details.\nVarious methods are provided for “betatree” objects, most of them inherit their behavior from “mob” objects (e.g., print, summary, coef, etc.). The plot method employs the node_bivplot panel-generating function.\n\nbetatree() returns an object of S3 class “betatree” which inherits from “modelparty”.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nZeileis A, Hothorn T, Hornik K (2008). Model-Based Recursive Partitioning. Journal of Computational and Graphical Statistics, 17(2), 492–514.\n\nbetareg, betareg.fit, mob\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\nsuppressWarnings(RNGversion(\"3.5.0\"))\n\n## data with two groups of dyslexic and non-dyslexic children\ndata(\"ReadingSkills\", package = \"betareg\")\n## additional random noise (not associated with reading scores)\nset.seed(1071)\nReadingSkills$x1 &lt;- rnorm(nrow(ReadingSkills))\nReadingSkills$x2 &lt;- runif(nrow(ReadingSkills))\nReadingSkills$x3 &lt;- factor(rnorm(nrow(ReadingSkills)) &gt; 0)\n\n## fit beta regression tree: in each node\n##   - accurcay's mean and precision depends on iq\n##   - partitioning is done by dyslexia and the noise variables x1, x2, x3\n## only dyslexia is correctly selected for splitting\nbt &lt;- betatree(accuracy ~ iq | iq, ~ dyslexia + x1 + x2 + x3,\n  data = ReadingSkills, minsize = 10)\nplot(bt)\n\n\n\n\n\n\n## inspect result\ncoef(bt)\n\n  (Intercept)       iq (phi)_(Intercept) (phi)_iq\n2      1.6565  1.46571             1.273    2.048\n3      0.3809 -0.08623             4.808    0.826\n\nif(require(\"strucchange\")) sctest(bt)\n\n$`1`\n           dyslexia     x1     x2     x3\nstatistic 2.269e+01 8.5251 5.5699 1.0568\np.value   5.848e-04 0.9095 0.9987 0.9999\n\n$`2`\n          dyslexia     x1     x2     x3\nstatistic        0 6.4116 4.5170 4.2308\np.value         NA 0.8412 0.9752 0.7566\n\n$`3`\nNULL\n\n## IGNORE_RDIFF_BEGIN\nsummary(bt, node = 2)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.495 -0.437  0.210  0.953  1.090 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.657      0.286    5.78  7.3e-09 ***\niq             1.466      0.248    5.92  3.2e-09 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.273      0.307    4.15  3.4e-05 ***\niq             2.048      0.331    6.19  5.9e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 39.4 on 4 Df\nPseudo R-squared: 0.149\nNumber of iterations: 17 (BFGS) + 2 (Fisher scoring) \n\nsummary(bt, node = 3)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.426 -0.631 -0.067  0.778  1.555 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.3809     0.0486    7.83  4.8e-15 ***\niq           -0.0862     0.0549   -1.57     0.12    \n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    4.808      0.414   11.61   &lt;2e-16 ***\niq             0.826      0.395    2.09    0.036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 27.3 on 4 Df\nPseudo R-squared: 0.0391\nNumber of iterations: 16 (BFGS) + 2 (Fisher scoring) \n\n## IGNORE_RDIFF_END\n\n## add a numerical variable with relevant information for splitting\nReadingSkills$x4 &lt;- rnorm(nrow(ReadingSkills), c(-1.5, 1.5)[ReadingSkills$dyslexia])\n\nbt2 &lt;- betatree(accuracy ~ iq | iq, ~ x1 + x2 + x3 + x4,\n  data = ReadingSkills, minsize = 10)\nplot(bt2)\n\n\n\n\n\n\n## inspect result\ncoef(bt2)\n\n  (Intercept)      iq (phi)_(Intercept) (phi)_iq\n2      1.7060 1.47402             1.293   2.0841\n3      0.5048 0.03391             3.131  -0.7684\n\nif(require(\"strucchange\")) sctest(bt2)\n\n$`1`\n              x1     x2     x3       x4\nstatistic 8.5251 5.5699 1.0568 19.94405\np.value   0.9095 0.9987 0.9999  0.03485\n\n$`2`\n              x1     x2     x3     x4\nstatistic 8.9467 3.5888 3.5677 4.7049\np.value   0.5964 0.9985 0.9197 0.9848\n\n$`3`\n              x1     x2     x3     x4\nstatistic 5.5413 1.2373 4.8649 4.9921\np.value   0.6595 0.9997 0.7619 0.7432\n\n## IGNORE_RDIFF_BEGIN\nsummary(bt2, node = 2)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.583 -0.393  0.177  0.923  1.054 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.706      0.292    5.85  4.9e-09 ***\niq             1.474      0.248    5.95  2.7e-09 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.293      0.312    4.14  3.4e-05 ***\niq             2.084      0.333    6.25  4.0e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 38.6 on 4 Df\nPseudo R-squared: 0.163\nNumber of iterations: 17 (BFGS) + 1 (Fisher scoring) \n\nsummary(bt2, node = 3)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.070 -0.584 -0.156  0.639  2.188 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.5048     0.1245    4.05    5e-05 ***\niq            0.0339     0.0998    0.34     0.73    \n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    3.131      0.370    8.45   &lt;2e-16 ***\niq            -0.768      0.359   -2.14    0.032 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 22.4 on 4 Df\nPseudo R-squared: 0.0378\nNumber of iterations: 16 (BFGS) + 1 (Fisher scoring) \n\n## IGNORE_RDIFF_END",
    "crumbs": [
      "Beta regression extensions",
      "betatree"
    ]
  },
  {
    "objectID": "man/betatree.html#beta-regression-trees",
    "href": "man/betatree.html#beta-regression-trees",
    "title": "betareg",
    "section": "",
    "text": "Fit beta regression trees via model-based recursive partitioning.\n\nbetatree(formula, partition,\n  data, subset = NULL, na.action = na.omit, weights, offset, cluster,\n  link = \"logit\", link.phi = \"log\", control = betareg.control(),\n  ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model of type y ~ x or y ~ x | z, specifying the variables influencing mean and precision of y, respectively. For details see betareg.\n\n\n\n\npartition\n\n\nsymbolic description of the partitioning variables, e.g., ~ p1 + p2. The argument partition can be omitted if formula is a three-part formula of type y ~ x | z | p1 + p2.\n\n\n\n\ndata, subset, na.action, weights, offset, cluster\n\n\narguments controlling data/model processing passed to mob.\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “identity”, “log”, “sqrt” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\ncontrol\n\n\na list of control arguments for the beta regression specified via betareg.control.\n\n\n\n\n…\n\n\nfurther control arguments for the recursive partitioning passed to mob_control.\n\n\n\nBeta regression trees are an application of model-based recursive partitioning (implemented in mob, see Zeileis et al. 2008) to beta regression (implemented in betareg, see Cribari-Neto and Zeileis 2010). See also Grün at al. (2012) for more details.\nVarious methods are provided for “betatree” objects, most of them inherit their behavior from “mob” objects (e.g., print, summary, coef, etc.). The plot method employs the node_bivplot panel-generating function.\n\nbetatree() returns an object of S3 class “betatree” which inherits from “modelparty”.\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nZeileis A, Hothorn T, Hornik K (2008). Model-Based Recursive Partitioning. Journal of Computational and Graphical Statistics, 17(2), 492–514.\n\nbetareg, betareg.fit, mob\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\nsuppressWarnings(RNGversion(\"3.5.0\"))\n\n## data with two groups of dyslexic and non-dyslexic children\ndata(\"ReadingSkills\", package = \"betareg\")\n## additional random noise (not associated with reading scores)\nset.seed(1071)\nReadingSkills$x1 &lt;- rnorm(nrow(ReadingSkills))\nReadingSkills$x2 &lt;- runif(nrow(ReadingSkills))\nReadingSkills$x3 &lt;- factor(rnorm(nrow(ReadingSkills)) &gt; 0)\n\n## fit beta regression tree: in each node\n##   - accurcay's mean and precision depends on iq\n##   - partitioning is done by dyslexia and the noise variables x1, x2, x3\n## only dyslexia is correctly selected for splitting\nbt &lt;- betatree(accuracy ~ iq | iq, ~ dyslexia + x1 + x2 + x3,\n  data = ReadingSkills, minsize = 10)\nplot(bt)\n\n\n\n\n\n\n## inspect result\ncoef(bt)\n\n  (Intercept)       iq (phi)_(Intercept) (phi)_iq\n2      1.6565  1.46571             1.273    2.048\n3      0.3809 -0.08623             4.808    0.826\n\nif(require(\"strucchange\")) sctest(bt)\n\n$`1`\n           dyslexia     x1     x2     x3\nstatistic 2.269e+01 8.5251 5.5699 1.0568\np.value   5.848e-04 0.9095 0.9987 0.9999\n\n$`2`\n          dyslexia     x1     x2     x3\nstatistic        0 6.4116 4.5170 4.2308\np.value         NA 0.8412 0.9752 0.7566\n\n$`3`\nNULL\n\n## IGNORE_RDIFF_BEGIN\nsummary(bt, node = 2)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.495 -0.437  0.210  0.953  1.090 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.657      0.286    5.78  7.3e-09 ***\niq             1.466      0.248    5.92  3.2e-09 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.273      0.307    4.15  3.4e-05 ***\niq             2.048      0.331    6.19  5.9e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 39.4 on 4 Df\nPseudo R-squared: 0.149\nNumber of iterations: 17 (BFGS) + 2 (Fisher scoring) \n\nsummary(bt, node = 3)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.426 -0.631 -0.067  0.778  1.555 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.3809     0.0486    7.83  4.8e-15 ***\niq           -0.0862     0.0549   -1.57     0.12    \n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    4.808      0.414   11.61   &lt;2e-16 ***\niq             0.826      0.395    2.09    0.036 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 27.3 on 4 Df\nPseudo R-squared: 0.0391\nNumber of iterations: 16 (BFGS) + 2 (Fisher scoring) \n\n## IGNORE_RDIFF_END\n\n## add a numerical variable with relevant information for splitting\nReadingSkills$x4 &lt;- rnorm(nrow(ReadingSkills), c(-1.5, 1.5)[ReadingSkills$dyslexia])\n\nbt2 &lt;- betatree(accuracy ~ iq | iq, ~ x1 + x2 + x3 + x4,\n  data = ReadingSkills, minsize = 10)\nplot(bt2)\n\n\n\n\n\n\n## inspect result\ncoef(bt2)\n\n  (Intercept)      iq (phi)_(Intercept) (phi)_iq\n2      1.7060 1.47402             1.293   2.0841\n3      0.5048 0.03391             3.131  -0.7684\n\nif(require(\"strucchange\")) sctest(bt2)\n\n$`1`\n              x1     x2     x3       x4\nstatistic 8.5251 5.5699 1.0568 19.94405\np.value   0.9095 0.9987 0.9999  0.03485\n\n$`2`\n              x1     x2     x3     x4\nstatistic 8.9467 3.5888 3.5677 4.7049\np.value   0.5964 0.9985 0.9197 0.9848\n\n$`3`\n              x1     x2     x3     x4\nstatistic 5.5413 1.2373 4.8649 4.9921\np.value   0.6595 0.9997 0.7619 0.7432\n\n## IGNORE_RDIFF_BEGIN\nsummary(bt2, node = 2)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.583 -0.393  0.177  0.923  1.054 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.706      0.292    5.85  4.9e-09 ***\niq             1.474      0.248    5.95  2.7e-09 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.293      0.312    4.14  3.4e-05 ***\niq             2.084      0.333    6.25  4.0e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 38.6 on 4 Df\nPseudo R-squared: 0.163\nNumber of iterations: 17 (BFGS) + 1 (Fisher scoring) \n\nsummary(bt2, node = 3)\n\n\nCall:\nbetatree(formula = accuracy ~ iq | iq, data = ReadingSkills)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.070 -0.584 -0.156  0.639  2.188 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.5048     0.1245    4.05    5e-05 ***\niq            0.0339     0.0998    0.34     0.73    \n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    3.131      0.370    8.45   &lt;2e-16 ***\niq            -0.768      0.359   -2.14    0.032 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 22.4 on 4 Df\nPseudo R-squared: 0.0378\nNumber of iterations: 16 (BFGS) + 1 (Fisher scoring) \n\n## IGNORE_RDIFF_END",
    "crumbs": [
      "Beta regression extensions",
      "betatree"
    ]
  },
  {
    "objectID": "man/dxbeta.html",
    "href": "man/dxbeta.html",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the extended-support beta distribution (in regression parameterization) on [0, 1].\n\n\n\ndxbeta(x, mu, phi, nu = 0, log = FALSE)\n\npxbeta(q, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE)\n\nqxbeta(p, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE)\n\nrxbeta(n, mu, phi, nu = 0)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Exceedence parameter for the support of the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nIn order to obtain an extended-support beta distribution on [0, 1] an additional exceedence parameter nu is introduced. If nu &gt; 0, this scales the underlying beta distribution to the interval [-nu, 1 + nu] where the tails are subsequently censored to the unit interval [0, 1] with point masses on the boundaries 0 and 1. Thus, nu controls how likely boundary observations are and for nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\n\n\ndxbeta gives the density, pxbeta gives the distribution function, qxbeta gives the quantile function, and rxbeta generates random deviates.\n\n\n\ndbetar, XBeta",
    "crumbs": [
      "Distributions",
      "dxbeta"
    ]
  },
  {
    "objectID": "man/dxbeta.html#the-extended-support-beta-distribution",
    "href": "man/dxbeta.html#the-extended-support-beta-distribution",
    "title": "betareg",
    "section": "",
    "text": "Density, distribution function, quantile function, and random generation for the extended-support beta distribution (in regression parameterization) on [0, 1].\n\n\n\ndxbeta(x, mu, phi, nu = 0, log = FALSE)\n\npxbeta(q, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE)\n\nqxbeta(p, mu, phi, nu = 0, lower.tail = TRUE, log.p = FALSE)\n\nrxbeta(n, mu, phi, nu = 0)\n\n\n\n\n\n\n\nx, q\n\n\nnumeric. Vector of quantiles.\n\n\n\n\np\n\n\nnumeric. Vector of probabilities.\n\n\n\n\nn\n\n\nnumeric. Number of observations. If length(n) &gt; 1, the length is taken to be the number required.\n\n\n\n\nmu\n\n\nnumeric. The mean of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nphi\n\n\nnumeric. The precision parameter of the underlying beta distribution on [-nu, 1 + nu].\n\n\n\n\nnu\n\n\nnumeric. Exceedence parameter for the support of the underlying beta distribution on [-nu, 1 + nu] that is censored to [0, 1].\n\n\n\n\nlog, log.p\n\n\nlogical. If TRUE, probabilities p are given as log(p).\n\n\n\n\nlower.tail\n\n\nlogical. If TRUE (default), probabilities are P[X &lt;= x] otherwise, P[X &gt; x].\n\n\n\n\n\n\nIn order to obtain an extended-support beta distribution on [0, 1] an additional exceedence parameter nu is introduced. If nu &gt; 0, this scales the underlying beta distribution to the interval [-nu, 1 + nu] where the tails are subsequently censored to the unit interval [0, 1] with point masses on the boundaries 0 and 1. Thus, nu controls how likely boundary observations are and for nu = 0 (the default), the distribution reduces to the classic beta distribution (in regression parameterization) without boundary observations.\n\n\n\ndxbeta gives the density, pxbeta gives the distribution function, qxbeta gives the quantile function, and rxbeta generates random deviates.\n\n\n\ndbetar, XBeta",
    "crumbs": [
      "Distributions",
      "dxbeta"
    ]
  },
  {
    "objectID": "man/betareg.html",
    "href": "man/betareg.html",
    "title": "betareg",
    "section": "",
    "text": "Fit beta regression models for rates and proportions via maximum likelihood using a parametrization with mean (depending through a link function on the covariates) and precision parameter (called phi).\n\nbetareg(formula, data, subset, na.action, weights, offset,\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\", \"loglog\"),\n  link.phi = NULL, type = c(\"ML\", \"BC\", \"BR\"), dist = NULL, nu = NULL,\n  control = betareg.control(...), model = TRUE,\n  y = TRUE, x = FALSE, ...)\n\nbetareg.fit(x, y, z = NULL, weights = NULL, offset = NULL,\n  link = \"logit\", link.phi = \"log\", type = \"ML\", control = betareg.control(),\n  dist = NULL, nu = NULL)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, either of type y ~ x (mean submodel, constant precision) or y ~ x | z (submodels for both mean and precision); for details see below.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of case weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor for the mean. In betareg.fit, offset may also be a list of two offsets for the mean and precision equation, respectively.\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “identity”, “log”, “sqrt” are supported. The default is “log” unless formula is of type y ~ x where the default is “identity” (for backward compatibility). Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\ntype\n\n\ncharacter specification of the type of estimator. Currently, maximum likelihood (“ML”), ML with bias correction (“BC”), and ML with bias reduction (“BR”) are supported.\n\n\n\n\ndist\n\n\ncharacter specification of the response distribution. Usually, this does not have to be set by the user because by default the classical “beta” distribution is used when all observations for the dependent variable are in (0, 1). In the presence of boundary observations (0 or 1, which cannot be accomodated by “beta”) the extended-support beta mixture distribution (“xbetax”) is used. Additionally, dist = “xbeta” can be used with fixed exceedence parameter nu, mostly for testing and debugging purposes.\n\n\n\n\nnu\n\n\nnumeric. The fixed value of the expected exceedence parameter nu in case the extended-support beta mixture distribution is used. By default, nu does not need to be specified and is estimated if needed. So setting nu is mostly for profiling and debugging.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via betareg.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned. For betareg.fit, x should be a numeric regressor matrix and y should be the numeric response vector (with values in (0,1)).\n\n\n\n\nz\n\n\nnumeric matrix. Regressor matrix for the precision model, defaulting to an intercept only.\n\n\n\n\n…\n\n\narguments passed to betareg.control.\n\n\n\nBeta regression as suggested by Ferrari and Cribari-Neto (2004) and extended by Simas, Barreto-Souza, and Rocha (2010) is implemented in betareg. It is useful in situations where the dependent variable is continuous and restricted to the unit interval (0, 1), e.g., resulting from rates or proportions. It is modeled to be beta-distributed with parametrization using mean and precision parameter (called mu and phi, respectively). The mean mu is linked, as in generalized linear models (GLMs), to the explanatory variables through a link function and a linear predictor. Additionally, the precision parameter phi can be linked to another (potentially overlapping) set of regressors through a second link function, resulting in a model with variable dispersion (see Cribari-Neto and Zeileis 2010). Estimation is performed by default using maximum likelihood (ML) via optim with analytical gradients and starting values from an auxiliary linear regression of the transformed response. Subsequently, the optim result may be enhanced by an additional Fisher scoring iteration using analytical gradients and expected information. Alternative estimation methods are bias-corrected (BC) or bias-reduced (BR) maximum likelihood (see Grün, Kosmidis, and Zeileis 2012). For ML and BC the Fisher scoring is just a refinement to move the gradients even closer to zero and can be disabled by setting fsmaxit = 0 in the control arguments. For BR the Fisher scoring is needed to solve the bias-adjusted estimating equations.\nIn the beta regression as introduced by Ferrari and Cribari-Neto (2004), the mean of the response is linked to a linear predictor described by y ~ x1 + x2 using a link function while the precision parameter phi is assumed to be constant. Simas et al. (2009) suggest to extend this model by linking phi to an additional set of regressors (z1 + z2, say): In betareg this can be specified in a formula of type y ~ x1 + x2 | z1 + z2 where the regressors in the two parts can be overlapping. In the precision model (for phi), the link function link.phi is used. The default is a “log” link unless no precision model is specified. In the latter case (i.e., when the formula is of type y ~ x1 + x2), the “identity” link is used by default for backward compatibility.\nKosmidis and Zeileis (2024) introduce a generalization of the classic beta regression model with extended support [0, 1]. Specifically, the extended-support beta distribution (“xbeta”) leverages an underlying symmetric four-parameter beta distribution with exceedence parameter nu to obtain support [-nu, 1 + nu] that is subsequently censored to [0, 1] in order to obtain point masses at the boundary values 0 and 1. The extended-support beta mixture distribution (“xbetax”) is a continuous mixture of extended-support beta distributions where the exceedence parameter follows an exponential distribution with mean nu (rather than a fixed value of nu). The latter “xbetax” specification is used by default in case of boundary observations at 0 and/or 1. The “xbeta” specification with fixed nu is mostly for testing and debugging purposes.\nA set of standard extractor functions for fitted model objects is available for objects of class “betareg”, including methods to the generic functions print, summary, plot, coef, vcov, logLik, residuals, predict, terms, model.frame, model.matrix, cooks.distance and hatvalues (see influence.measures), gleverage (new generic), estfun and bread (from the sandwich package), and coeftest (from the lmtest package).\nSee predict.betareg, residuals.betareg, plot.betareg, and summary.betareg for more details on all methods.\nThe main parameters of interest are the coefficients in the linear predictor of the mean model. The additional parameters in the precision model (phi) can either be treated as full model parameters (default) or as nuisance parameters. In the latter case the estimation does not change, only the reported information in output from print, summary, or coef (among others) will be different. See also betareg.control.\nThe implemented algorithms for bias correction/reduction follow Kosmidis and Firth (2010). Technical note: In case, either bias correction or reduction is requested, the second derivative of the inverse link function is required for link and link.phi. If the two links are specified by their names (as done by default in betareg), then the “link-glm” objects are enhanced automatically by the required additional d2mu.deta function. However, if a “link-glm” object is supplied directly by the user, it needs to have the d2mu.deta function or, for backward compatibility, dmu.deta.\nThe original version of the package was written by Alexandre B. Simas and Andrea V. Rocha (up to version 1.2). Starting from version 2.0-0 the code was rewritten by Achim Zeileis.\n\nbetareg returns an object of class “betareg”, i.e., a list with components as follows. For classic beta regressions (dist = “beta”) several elements are lists with the names “mean” and “precision” for the information from the respective submodels. For extended-support beta regressions (dist = “xbetax” or “xbeta”), the corresponding names are “mu” and “phi” because they are not exactly the mean and precision due to the censoring in the response variable.\nbetareg.fit returns an unclassed list with components up to converged.\n\n\n\ncoefficients\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the coefficients from the respective submodels and for extended-support beta regressions an additional element “nu”,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\noutput from the optim call for maximizing the log-likelihood(s),\n\n\n\n\nmethod\n\n\nthe method argument passed to the optim call,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call,\n\n\n\n\nweights\n\n\nthe weights used (if any),\n\n\n\n\noffset\n\n\na list of offset vectors used (if any),\n\n\n\n\nn\n\n\nnumber of observations,\n\n\n\n\nnobs\n\n\nnumber of observations with non-zero weights,\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom in the null model (constant mean and dispersion), i.e., n - 2,\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom in the fitted model,\n\n\n\n\nphi\n\n\nlogical indicating whether the precision (phi) coefficients will be treated as full model parameters or nuisance parameters in subsequent calls to print, summary, coef etc.,\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all parameters in the model,\n\n\n\n\npseudo.r.squared\n\n\npseudo R-squared value (squared correlation of linear predictor and link-transformed response),\n\n\n\n\nlink\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the link objects for the respective submodels,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nterms\n\n\na list with elements “mean” (or “mu”), “precision” (or “phi”) and “full” containing the terms objects for the respective models,\n\n\n\n\nlevels\n\n\na list with elements “mean” (or “mu”), “precision” (or “phi”) and “full” containing the levels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the contrasts corresponding to levels from the respective models,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response proportion vector (if y = TRUE),\n\n\n\n\nx\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the model matrices from the respective models (if x = TRUE).\n\n\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nKosmidis I, Firth D (2010). A Generic Algorithm for Reducing Bias in Parametric Estimation. Electronic Journal of Statistics, 4, 1097–1112.\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\nSimas AB, Barreto-Souza W, Rocha AV (2010). Improved Estimators for a General Class of Beta Regression Models. Computational Statistics & Data Analysis, 54(2), 348–366.\n\nsummary.betareg, predict.betareg, residuals.betareg, Formula\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\n## Section 4 from Ferrari and Cribari-Neto (2004)\ndata(\"GasolineYield\", package = \"betareg\")\ndata(\"FoodExpenditure\", package = \"betareg\")\n\n## Table 1\ngy &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\nsummary(gy)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.140 -0.570  0.120  0.704  1.751 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\nbatch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\nbatch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\nbatch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\nbatch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\nbatch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\nbatch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\nbatch7       0.543692   0.109127    4.98  6.3e-07 ***\nbatch8       0.495901   0.108926    4.55  5.3e-06 ***\nbatch9       0.385793   0.118593    3.25   0.0011 ** \ntemp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)      440        110       4  6.3e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 84.8 on 12 Df\nPseudo R-squared: 0.962\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring) \n\n## Table 2\nfe_lin &lt;- lm(I(food/income) ~ income + persons, data = FoodExpenditure)\nlibrary(\"lmtest\")\nbptest(fe_lin)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fe_lin\nBP = 5.9, df = 2, p-value = 0.05\n\nfe_beta &lt;- betareg(I(food/income) ~ income + persons, data = FoodExpenditure)\nsummary(fe_beta)\n\n\nCall:\nbetareg(formula = I(food/income) ~ income + persons, data = FoodExpenditure)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.533 -0.460  0.170  0.642  1.773 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.62255    0.22385   -2.78   0.0054 ** \nincome      -0.01230    0.00304   -4.05  5.1e-05 ***\npersons      0.11846    0.03534    3.35   0.0008 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    35.61       8.08    4.41    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 45.3 on 4 Df\nPseudo R-squared: 0.388\nNumber of iterations: 28 (BFGS) + 4 (Fisher scoring) \n\n## nested model comparisons via Wald and LR tests\nfe_beta2 &lt;- betareg(I(food/income) ~ income, data = FoodExpenditure)\nlrtest(fe_beta, fe_beta2)\n\nLikelihood ratio test\n\nModel 1: I(food/income) ~ income + persons\nModel 2: I(food/income) ~ income\n  #Df LogLik Df Chisq Pr(&gt;Chisq)   \n1   4   45.3                       \n2   3   40.5 -1  9.65     0.0019 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwaldtest(fe_beta, fe_beta2)\n\nWald test\n\nModel 1: I(food/income) ~ income + persons\nModel 2: I(food/income) ~ income\n  Res.Df Df Chisq Pr(&gt;Chisq)    \n1     34                        \n2     35 -1  11.2      8e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Section 3 from online supplements to Simas et al. (2010)\n## mean model as in gy above\n## precision model with regressor temp\ngy2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\n## MLE column in Table 19\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp | temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.104 -0.585 -0.143  0.690  2.520 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.923236   0.183526  -32.27  &lt; 2e-16 ***\nbatch1       1.601988   0.063856   25.09  &lt; 2e-16 ***\nbatch2       1.297266   0.099100   13.09  &lt; 2e-16 ***\nbatch3       1.565338   0.099739   15.69  &lt; 2e-16 ***\nbatch4       1.030072   0.063288   16.28  &lt; 2e-16 ***\nbatch5       1.154163   0.065643   17.58  &lt; 2e-16 ***\nbatch6       1.019445   0.066351   15.36  &lt; 2e-16 ***\nbatch7       0.622259   0.065632    9.48  &lt; 2e-16 ***\nbatch8       0.564583   0.060185    9.38  &lt; 2e-16 ***\nbatch9       0.359439   0.067141    5.35  8.6e-08 ***\ntemp         0.010359   0.000436   23.75  &lt; 2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.36409    1.22578    1.11     0.27    \ntemp         0.01457    0.00362    4.03  5.7e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   87 on 13 Df\nPseudo R-squared: 0.952\nNumber of iterations: 33 (BFGS) + 28 (Fisher scoring) \n\n## LRT row in Table 18\nlrtest(gy, gy2)\n\nLikelihood ratio test\n\nModel 1: yield ~ batch + temp\nModel 2: yield ~ batch + temp | temp\n  #Df LogLik Df Chisq Pr(&gt;Chisq)  \n1  12   84.8                      \n2  13   87.0  1  4.36      0.037 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Beta regression",
      "betareg"
    ]
  },
  {
    "objectID": "man/betareg.html#beta-regression-for-rates-and-proportions",
    "href": "man/betareg.html#beta-regression-for-rates-and-proportions",
    "title": "betareg",
    "section": "",
    "text": "Fit beta regression models for rates and proportions via maximum likelihood using a parametrization with mean (depending through a link function on the covariates) and precision parameter (called phi).\n\nbetareg(formula, data, subset, na.action, weights, offset,\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\", \"loglog\"),\n  link.phi = NULL, type = c(\"ML\", \"BC\", \"BR\"), dist = NULL, nu = NULL,\n  control = betareg.control(...), model = TRUE,\n  y = TRUE, x = FALSE, ...)\n\nbetareg.fit(x, y, z = NULL, weights = NULL, offset = NULL,\n  link = \"logit\", link.phi = \"log\", type = \"ML\", control = betareg.control(),\n  dist = NULL, nu = NULL)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, either of type y ~ x (mean submodel, constant precision) or y ~ x | z (submodels for both mean and precision); for details see below.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of case weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor for the mean. In betareg.fit, offset may also be a list of two offsets for the mean and precision equation, respectively.\n\n\n\n\nlink\n\n\ncharacter specification of the link function in the mean model (mu). Currently, “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” are supported. Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\nlink.phi\n\n\ncharacter specification of the link function in the precision model (phi). Currently, “identity”, “log”, “sqrt” are supported. The default is “log” unless formula is of type y ~ x where the default is “identity” (for backward compatibility). Alternatively, an object of class “link-glm” can be supplied.\n\n\n\n\ntype\n\n\ncharacter specification of the type of estimator. Currently, maximum likelihood (“ML”), ML with bias correction (“BC”), and ML with bias reduction (“BR”) are supported.\n\n\n\n\ndist\n\n\ncharacter specification of the response distribution. Usually, this does not have to be set by the user because by default the classical “beta” distribution is used when all observations for the dependent variable are in (0, 1). In the presence of boundary observations (0 or 1, which cannot be accomodated by “beta”) the extended-support beta mixture distribution (“xbetax”) is used. Additionally, dist = “xbeta” can be used with fixed exceedence parameter nu, mostly for testing and debugging purposes.\n\n\n\n\nnu\n\n\nnumeric. The fixed value of the expected exceedence parameter nu in case the extended-support beta mixture distribution is used. By default, nu does not need to be specified and is estimated if needed. So setting nu is mostly for profiling and debugging.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via betareg.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned. For betareg.fit, x should be a numeric regressor matrix and y should be the numeric response vector (with values in (0,1)).\n\n\n\n\nz\n\n\nnumeric matrix. Regressor matrix for the precision model, defaulting to an intercept only.\n\n\n\n\n…\n\n\narguments passed to betareg.control.\n\n\n\nBeta regression as suggested by Ferrari and Cribari-Neto (2004) and extended by Simas, Barreto-Souza, and Rocha (2010) is implemented in betareg. It is useful in situations where the dependent variable is continuous and restricted to the unit interval (0, 1), e.g., resulting from rates or proportions. It is modeled to be beta-distributed with parametrization using mean and precision parameter (called mu and phi, respectively). The mean mu is linked, as in generalized linear models (GLMs), to the explanatory variables through a link function and a linear predictor. Additionally, the precision parameter phi can be linked to another (potentially overlapping) set of regressors through a second link function, resulting in a model with variable dispersion (see Cribari-Neto and Zeileis 2010). Estimation is performed by default using maximum likelihood (ML) via optim with analytical gradients and starting values from an auxiliary linear regression of the transformed response. Subsequently, the optim result may be enhanced by an additional Fisher scoring iteration using analytical gradients and expected information. Alternative estimation methods are bias-corrected (BC) or bias-reduced (BR) maximum likelihood (see Grün, Kosmidis, and Zeileis 2012). For ML and BC the Fisher scoring is just a refinement to move the gradients even closer to zero and can be disabled by setting fsmaxit = 0 in the control arguments. For BR the Fisher scoring is needed to solve the bias-adjusted estimating equations.\nIn the beta regression as introduced by Ferrari and Cribari-Neto (2004), the mean of the response is linked to a linear predictor described by y ~ x1 + x2 using a link function while the precision parameter phi is assumed to be constant. Simas et al. (2009) suggest to extend this model by linking phi to an additional set of regressors (z1 + z2, say): In betareg this can be specified in a formula of type y ~ x1 + x2 | z1 + z2 where the regressors in the two parts can be overlapping. In the precision model (for phi), the link function link.phi is used. The default is a “log” link unless no precision model is specified. In the latter case (i.e., when the formula is of type y ~ x1 + x2), the “identity” link is used by default for backward compatibility.\nKosmidis and Zeileis (2024) introduce a generalization of the classic beta regression model with extended support [0, 1]. Specifically, the extended-support beta distribution (“xbeta”) leverages an underlying symmetric four-parameter beta distribution with exceedence parameter nu to obtain support [-nu, 1 + nu] that is subsequently censored to [0, 1] in order to obtain point masses at the boundary values 0 and 1. The extended-support beta mixture distribution (“xbetax”) is a continuous mixture of extended-support beta distributions where the exceedence parameter follows an exponential distribution with mean nu (rather than a fixed value of nu). The latter “xbetax” specification is used by default in case of boundary observations at 0 and/or 1. The “xbeta” specification with fixed nu is mostly for testing and debugging purposes.\nA set of standard extractor functions for fitted model objects is available for objects of class “betareg”, including methods to the generic functions print, summary, plot, coef, vcov, logLik, residuals, predict, terms, model.frame, model.matrix, cooks.distance and hatvalues (see influence.measures), gleverage (new generic), estfun and bread (from the sandwich package), and coeftest (from the lmtest package).\nSee predict.betareg, residuals.betareg, plot.betareg, and summary.betareg for more details on all methods.\nThe main parameters of interest are the coefficients in the linear predictor of the mean model. The additional parameters in the precision model (phi) can either be treated as full model parameters (default) or as nuisance parameters. In the latter case the estimation does not change, only the reported information in output from print, summary, or coef (among others) will be different. See also betareg.control.\nThe implemented algorithms for bias correction/reduction follow Kosmidis and Firth (2010). Technical note: In case, either bias correction or reduction is requested, the second derivative of the inverse link function is required for link and link.phi. If the two links are specified by their names (as done by default in betareg), then the “link-glm” objects are enhanced automatically by the required additional d2mu.deta function. However, if a “link-glm” object is supplied directly by the user, it needs to have the d2mu.deta function or, for backward compatibility, dmu.deta.\nThe original version of the package was written by Alexandre B. Simas and Andrea V. Rocha (up to version 1.2). Starting from version 2.0-0 the code was rewritten by Achim Zeileis.\n\nbetareg returns an object of class “betareg”, i.e., a list with components as follows. For classic beta regressions (dist = “beta”) several elements are lists with the names “mean” and “precision” for the information from the respective submodels. For extended-support beta regressions (dist = “xbetax” or “xbeta”), the corresponding names are “mu” and “phi” because they are not exactly the mean and precision due to the censoring in the response variable.\nbetareg.fit returns an unclassed list with components up to converged.\n\n\n\ncoefficients\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the coefficients from the respective submodels and for extended-support beta regressions an additional element “nu”,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\noutput from the optim call for maximizing the log-likelihood(s),\n\n\n\n\nmethod\n\n\nthe method argument passed to the optim call,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call,\n\n\n\n\nweights\n\n\nthe weights used (if any),\n\n\n\n\noffset\n\n\na list of offset vectors used (if any),\n\n\n\n\nn\n\n\nnumber of observations,\n\n\n\n\nnobs\n\n\nnumber of observations with non-zero weights,\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom in the null model (constant mean and dispersion), i.e., n - 2,\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom in the fitted model,\n\n\n\n\nphi\n\n\nlogical indicating whether the precision (phi) coefficients will be treated as full model parameters or nuisance parameters in subsequent calls to print, summary, coef etc.,\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all parameters in the model,\n\n\n\n\npseudo.r.squared\n\n\npseudo R-squared value (squared correlation of linear predictor and link-transformed response),\n\n\n\n\nlink\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the link objects for the respective submodels,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nterms\n\n\na list with elements “mean” (or “mu”), “precision” (or “phi”) and “full” containing the terms objects for the respective models,\n\n\n\n\nlevels\n\n\na list with elements “mean” (or “mu”), “precision” (or “phi”) and “full” containing the levels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the contrasts corresponding to levels from the respective models,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response proportion vector (if y = TRUE),\n\n\n\n\nx\n\n\na list with elements “mean” (or “mu”) and “precision” (or “phi”) containing the model matrices from the respective models (if x = TRUE).\n\n\n\nCribari-Neto F, Zeileis A (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. doi:10.18637/jss.v034.i02\nFerrari SLP, Cribari-Neto F (2004). Beta Regression for Modeling Rates and Proportions. Journal of Applied Statistics, 31(7), 799–815.\nGrün B, Kosmidis I, Zeileis A (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. doi:10.18637/jss.v048.i11\nKosmidis I, Firth D (2010). A Generic Algorithm for Reducing Bias in Parametric Estimation. Electronic Journal of Statistics, 4, 1097–1112.\nKosmidis I, Zeileis A (2024). Extended-Support Beta Regression for [0, 1] Responses. 2409.07233, arXiv.org E-Print Archive. doi:10.48550/arXiv.2409.07233\nSimas AB, Barreto-Souza W, Rocha AV (2010). Improved Estimators for a General Class of Beta Regression Models. Computational Statistics & Data Analysis, 54(2), 348–366.\n\nsummary.betareg, predict.betareg, residuals.betareg, Formula\n\n\nlibrary(\"betareg\")\n\noptions(digits = 4)\n\n## Section 4 from Ferrari and Cribari-Neto (2004)\ndata(\"GasolineYield\", package = \"betareg\")\ndata(\"FoodExpenditure\", package = \"betareg\")\n\n## Table 1\ngy &lt;- betareg(yield ~ batch + temp, data = GasolineYield)\nsummary(gy)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.140 -0.570  0.120  0.704  1.751 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.159571   0.182325  -33.78  &lt; 2e-16 ***\nbatch1       1.727729   0.101229   17.07  &lt; 2e-16 ***\nbatch2       1.322597   0.117902   11.22  &lt; 2e-16 ***\nbatch3       1.572310   0.116105   13.54  &lt; 2e-16 ***\nbatch4       1.059714   0.102360   10.35  &lt; 2e-16 ***\nbatch5       1.133752   0.103523   10.95  &lt; 2e-16 ***\nbatch6       1.040162   0.106036    9.81  &lt; 2e-16 ***\nbatch7       0.543692   0.109127    4.98  6.3e-07 ***\nbatch8       0.495901   0.108926    4.55  5.3e-06 ***\nbatch9       0.385793   0.118593    3.25   0.0011 ** \ntemp         0.010967   0.000413   26.58  &lt; 2e-16 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)      440        110       4  6.3e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 84.8 on 12 Df\nPseudo R-squared: 0.962\nNumber of iterations: 51 (BFGS) + 3 (Fisher scoring) \n\n## Table 2\nfe_lin &lt;- lm(I(food/income) ~ income + persons, data = FoodExpenditure)\nlibrary(\"lmtest\")\nbptest(fe_lin)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fe_lin\nBP = 5.9, df = 2, p-value = 0.05\n\nfe_beta &lt;- betareg(I(food/income) ~ income + persons, data = FoodExpenditure)\nsummary(fe_beta)\n\n\nCall:\nbetareg(formula = I(food/income) ~ income + persons, data = FoodExpenditure)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.533 -0.460  0.170  0.642  1.773 \n\nCoefficients (mean model with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.62255    0.22385   -2.78   0.0054 ** \nincome      -0.01230    0.00304   -4.05  5.1e-05 ***\npersons      0.11846    0.03534    3.35   0.0008 ***\n\nPhi coefficients (precision model with identity link):\n      Estimate Std. Error z value Pr(&gt;|z|)    \n(phi)    35.61       8.08    4.41    1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood: 45.3 on 4 Df\nPseudo R-squared: 0.388\nNumber of iterations: 28 (BFGS) + 4 (Fisher scoring) \n\n## nested model comparisons via Wald and LR tests\nfe_beta2 &lt;- betareg(I(food/income) ~ income, data = FoodExpenditure)\nlrtest(fe_beta, fe_beta2)\n\nLikelihood ratio test\n\nModel 1: I(food/income) ~ income + persons\nModel 2: I(food/income) ~ income\n  #Df LogLik Df Chisq Pr(&gt;Chisq)   \n1   4   45.3                       \n2   3   40.5 -1  9.65     0.0019 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwaldtest(fe_beta, fe_beta2)\n\nWald test\n\nModel 1: I(food/income) ~ income + persons\nModel 2: I(food/income) ~ income\n  Res.Df Df Chisq Pr(&gt;Chisq)    \n1     34                        \n2     35 -1  11.2      8e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Section 3 from online supplements to Simas et al. (2010)\n## mean model as in gy above\n## precision model with regressor temp\ngy2 &lt;- betareg(yield ~ batch + temp | temp, data = GasolineYield)\n\n## MLE column in Table 19\nsummary(gy2)\n\n\nCall:\nbetareg(formula = yield ~ batch + temp | temp, data = GasolineYield)\n\nQuantile residuals:\n   Min     1Q Median     3Q    Max \n-2.104 -0.585 -0.143  0.690  2.520 \n\nCoefficients (mean model with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.923236   0.183526  -32.27  &lt; 2e-16 ***\nbatch1       1.601988   0.063856   25.09  &lt; 2e-16 ***\nbatch2       1.297266   0.099100   13.09  &lt; 2e-16 ***\nbatch3       1.565338   0.099739   15.69  &lt; 2e-16 ***\nbatch4       1.030072   0.063288   16.28  &lt; 2e-16 ***\nbatch5       1.154163   0.065643   17.58  &lt; 2e-16 ***\nbatch6       1.019445   0.066351   15.36  &lt; 2e-16 ***\nbatch7       0.622259   0.065632    9.48  &lt; 2e-16 ***\nbatch8       0.564583   0.060185    9.38  &lt; 2e-16 ***\nbatch9       0.359439   0.067141    5.35  8.6e-08 ***\ntemp         0.010359   0.000436   23.75  &lt; 2e-16 ***\n\nPhi coefficients (precision model with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.36409    1.22578    1.11     0.27    \ntemp         0.01457    0.00362    4.03  5.7e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nType of estimator: ML (maximum likelihood)\nLog-likelihood:   87 on 13 Df\nPseudo R-squared: 0.952\nNumber of iterations: 33 (BFGS) + 28 (Fisher scoring) \n\n## LRT row in Table 18\nlrtest(gy, gy2)\n\nLikelihood ratio test\n\nModel 1: yield ~ batch + temp\nModel 2: yield ~ batch + temp | temp\n  #Df LogLik Df Chisq Pr(&gt;Chisq)  \n1  12   84.8                      \n2  13   87.0  1  4.36      0.037 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Beta regression",
      "betareg"
    ]
  }
]