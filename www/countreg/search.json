[
  {
    "objectID": "man/hurdletest.html",
    "href": "man/hurdletest.html",
    "title": "countreg",
    "section": "",
    "text": "Wald test of the null hypothesis that no zero hurdle is required in hurdle regression models for count data.\n\nhurdletest(object, ...)\n\n\n\n\n\nobject\n\n\nA fitted model object of class “hurdle” as returned by hurdle, see details for more information.\n\n\n\n\n…\n\n\narguments passed to linearHypothesis.\n\n\n\nIf the same count distribution and the same set of regressors is used in the hurdle model for both, the count component and the zero hurdle component, then a test of pairwise equality between all coefficients from the two components assesses the null hypothesis that no hurdle is needed in the model.\nThe function hurdletest is a simple convenience interface to the function linearHypothesis from the car packages that can be employed to carry out a Wald test for this hypothesis.\n\nAn object of class “anova” as returned by linearHypothesis.\n\nCameron AC, Trivedi PK (1998). Regression Analysis of Count Data. New York: Cambridge University Press.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\n\nhurdle, linearHypothesis\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\nfm &lt;- hurdle(satellites ~ ., data = cs, dist = \"negbin\", zero = \"negbin\")\nif(require(\"car\")) hurdletest(fm)\n\nWald test for hurdle models\n\nRestrictions:\ncount_((Intercept) - zero_(Intercept) = 0\ncount_width - zero_width = 0\ncount_color - zero_color = 0\n\nModel 1: restricted model\nModel 2: satellites ~ .\n\n  Res.Df Df  Chisq Pr(&gt;Chisq)    \n1    168                         \n2    165  3 50.362  6.689e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Diagnostic tests",
      "hurdletest"
    ]
  },
  {
    "objectID": "man/hurdletest.html#testing-for-the-presence-of-a-zero-hurdle",
    "href": "man/hurdletest.html#testing-for-the-presence-of-a-zero-hurdle",
    "title": "countreg",
    "section": "",
    "text": "Wald test of the null hypothesis that no zero hurdle is required in hurdle regression models for count data.\n\nhurdletest(object, ...)\n\n\n\n\n\nobject\n\n\nA fitted model object of class “hurdle” as returned by hurdle, see details for more information.\n\n\n\n\n…\n\n\narguments passed to linearHypothesis.\n\n\n\nIf the same count distribution and the same set of regressors is used in the hurdle model for both, the count component and the zero hurdle component, then a test of pairwise equality between all coefficients from the two components assesses the null hypothesis that no hurdle is needed in the model.\nThe function hurdletest is a simple convenience interface to the function linearHypothesis from the car packages that can be employed to carry out a Wald test for this hypothesis.\n\nAn object of class “anova” as returned by linearHypothesis.\n\nCameron AC, Trivedi PK (1998). Regression Analysis of Count Data. New York: Cambridge University Press.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\n\nhurdle, linearHypothesis\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\nfm &lt;- hurdle(satellites ~ ., data = cs, dist = \"negbin\", zero = \"negbin\")\nif(require(\"car\")) hurdletest(fm)\n\nWald test for hurdle models\n\nRestrictions:\ncount_((Intercept) - zero_(Intercept) = 0\ncount_width - zero_width = 0\ncount_color - zero_color = 0\n\nModel 1: restricted model\nModel 2: satellites ~ .\n\n  Res.Df Df  Chisq Pr(&gt;Chisq)    \n1    168                         \n2    165  3 50.362  6.689e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Diagnostic tests",
      "hurdletest"
    ]
  },
  {
    "objectID": "man/GSOEP.html",
    "href": "man/GSOEP.html",
    "title": "countreg",
    "section": "",
    "text": "Unbalanced panel concerning the usage of health services in Germany for the years 1984-1988, 1991 and 1994 consisting of 27,326 observations on 22 variables taken from the German Socioeconomic Panel (GSOEP). The number of visits to a doctor and the number of inpatient hospital visits are given along with several personal socioeconomic characteristics.\n\ndata(\"GSOEP\")\n\nA data frame with 27,326 observations on 22 variables.\n\n\nid\n\n\nidentification number indicating 1 to 7 observations for each of the 7,293 cases.\n\n\nfemale\n\n\nfactor indicating whether a person is female (yes or no).\n\n\nyear\n\n\nfactor giving the calendar year of the observation.\n\n\nage\n\n\nage in years.\n\n\nhsat\n\n\npersonal health satisfaction judged on a scale from 0 (low) to 10 (high). See Details.\n\n\nhanddum\n\n\ndummy variable indicating whether a person is handicapped (=1) or not (=0). See Details.\n\n\nhandper\n\n\ndegree of handicap expressed in percent from 0 to 100. See Details.\n\n\nhhninc\n\n\nmonthly household net income in 1’000 German marks.\n\n\nhhkids\n\n\nfactor indicating whether a household has kids below the age of 16 (yes or no).\n\n\neduc\n\n\nyears of schooling. See Details.\n\n\nmarried\n\n\nfactor indicating whether a person is married (yes or no).\n\n\nschool\n\n\nfactor giving the highest schooling degree. haupt for Hauptschule, real for Realschule, fach for Fachschule and abitur for Gymnasium.\n\n\nuniv\n\n\nfactor indicating whether a person has a university diploma (yes or no).\n\n\nworking\n\n\nfactor indicating whether a person is employed (yes or no).\n\n\nbluec\n\n\nfactor indicating whether a person is a blue collar worker (yes or no).\n\n\nwhitec\n\n\nfactor indicating whether a person is a white collar worker (yes or no).\n\n\nself\n\n\nfactor indicating whether a person is self employed (yes or no).\n\n\ncivil\n\n\nfactor indicating whether a person is a civil servant (yes or no).\n\n\ndocvis\n\n\nnumber of doctor visits within the last three months.\n\n\nhospvis\n\n\nnumber of hospital visits within the last calendar year.\n\n\npublic\n\n\nfactor indicating whether a person is insured in public health insurance (yes or no).\n\n\naddon\n\n\nfactor indicating whether a person is insured in add-on insurance (yes or no).\n\n\nThe data were first used by Riphahn, Wambach and Million (2003).\nAs noted by Greene (2007, 2008), there are 40 observations on hsat taking on values between 6 and 7 although the variable is supposed to be an integer. For three other variables similar problems occur: By construction, handdum should be a dummy variable but there are 18 observations taking on values between 0 and 1. Also, for handper 3,290 observations are not integer-valued. Finally, the precision of 148 values of educ is very suspicious.\nAs of 2013, a comment and a reply on data issues are available from the JAE archive. No attempts are made to fix these issues here. Data is used ‘as is’ in order to replicate Greene (2007, 2008, 2011).\n\nJournal of Applied Econometrics Data Archive.\nhttp://qed.econ.queensu.ca/jae/2003-v18.4/riphahn-wambach-million/\n\nGreene WH (2007). “Functional Form and Heterogeneity in Models for Count Data”. Foundations and Trends in Econometrics, 1(2), 113–218.\nGreene WH (2008). “Functional Forms for the Negative Binomial Model for Count Data”. Economics Letters, 99, 585–590.\nGreene WH (2011). Econometric Analysis, 7th edition. Upper Saddle River, NJ: Prentice Hall.\nRiphahn RT, Wambach A, Million A (2003). “Incentive Effects in the Demand for Health Care: A Bivariate Panel Count Data Estimation”. Journal of Applied Econometrics, 18(4), 387–405.\n\n\nlibrary(\"countreg\")\n\ndata(\"GSOEP\")\n\nif(exists(\"nbp.u\") & exists(\"nbp.o\")) {\n\n# We fit an NB1, an NB2 and an NBP model as in Greene (2007) ...\nmodel.u &lt;- docvis ~ age + I(age^2) + hsat + handdum + handper + married + educ + \n           hhninc + hhkids + self + civil + bluec + working + public + addon + year\nfm.nb1.u &lt;- nbp.u(model.u, data = subset(GSOEP, female == \"no\"), p.fix = 1)\nfm.nb2.u &lt;- nbp.u(model.u, data = subset(GSOEP, female == \"no\"), p.fix = 2)\nfm.nbp.u &lt;- nbp.u(model.u, data = subset(GSOEP, female == \"no\"))\n\n# ... and show that the former two are inferior to the last one by standard likelihood ratio tests\nlibrary(\"lmtest\")\nlrtest(fm.nb1.u, fm.nbp.u)\nlrtest(fm.nb2.u, fm.nbp.u)\n\n# model with observed heterogeneity\nmodel.o &lt;- docvis ~ age + I(age^2) + hsat + handdum + handper + married + educ + \n           hhninc + hhkids + self + civil + bluec + working + public + addon + year | hhninc + educ\n## fm.nbp.o &lt;- nbp.o(model.o, data = subset(GSOEP, female == \"no\"))\n\n# visualization of the fit\n# plot(fm.nbp.o)\n\n# things to replicate from Greene (2011):\n# Ex. 11.16, Tab 11.13, pp 451-452: nonlinear regression\n# Tab. 14.2, pp 580--581: logistic and Poisson \n# Tab. 14.10, pp 611-613: geometric regression\n# Ex. 17.7: binary Chow test (maybe not?)\n# Tab 18.14, p 850: Poisson, various negbins\n# Tab 18.17, p 860 (panel models, some of)\n# Tab 18.19, p 866: hurdle models\n\n# things to replicate from Greene (2008):\n# Tab 2: Poisson, various negbins, presumably same as book\n\n}",
    "crumbs": [
      "Data sets",
      "GSOEP"
    ]
  },
  {
    "objectID": "man/GSOEP.html#health-services-gsoep",
    "href": "man/GSOEP.html#health-services-gsoep",
    "title": "countreg",
    "section": "",
    "text": "Unbalanced panel concerning the usage of health services in Germany for the years 1984-1988, 1991 and 1994 consisting of 27,326 observations on 22 variables taken from the German Socioeconomic Panel (GSOEP). The number of visits to a doctor and the number of inpatient hospital visits are given along with several personal socioeconomic characteristics.\n\ndata(\"GSOEP\")\n\nA data frame with 27,326 observations on 22 variables.\n\n\nid\n\n\nidentification number indicating 1 to 7 observations for each of the 7,293 cases.\n\n\nfemale\n\n\nfactor indicating whether a person is female (yes or no).\n\n\nyear\n\n\nfactor giving the calendar year of the observation.\n\n\nage\n\n\nage in years.\n\n\nhsat\n\n\npersonal health satisfaction judged on a scale from 0 (low) to 10 (high). See Details.\n\n\nhanddum\n\n\ndummy variable indicating whether a person is handicapped (=1) or not (=0). See Details.\n\n\nhandper\n\n\ndegree of handicap expressed in percent from 0 to 100. See Details.\n\n\nhhninc\n\n\nmonthly household net income in 1’000 German marks.\n\n\nhhkids\n\n\nfactor indicating whether a household has kids below the age of 16 (yes or no).\n\n\neduc\n\n\nyears of schooling. See Details.\n\n\nmarried\n\n\nfactor indicating whether a person is married (yes or no).\n\n\nschool\n\n\nfactor giving the highest schooling degree. haupt for Hauptschule, real for Realschule, fach for Fachschule and abitur for Gymnasium.\n\n\nuniv\n\n\nfactor indicating whether a person has a university diploma (yes or no).\n\n\nworking\n\n\nfactor indicating whether a person is employed (yes or no).\n\n\nbluec\n\n\nfactor indicating whether a person is a blue collar worker (yes or no).\n\n\nwhitec\n\n\nfactor indicating whether a person is a white collar worker (yes or no).\n\n\nself\n\n\nfactor indicating whether a person is self employed (yes or no).\n\n\ncivil\n\n\nfactor indicating whether a person is a civil servant (yes or no).\n\n\ndocvis\n\n\nnumber of doctor visits within the last three months.\n\n\nhospvis\n\n\nnumber of hospital visits within the last calendar year.\n\n\npublic\n\n\nfactor indicating whether a person is insured in public health insurance (yes or no).\n\n\naddon\n\n\nfactor indicating whether a person is insured in add-on insurance (yes or no).\n\n\nThe data were first used by Riphahn, Wambach and Million (2003).\nAs noted by Greene (2007, 2008), there are 40 observations on hsat taking on values between 6 and 7 although the variable is supposed to be an integer. For three other variables similar problems occur: By construction, handdum should be a dummy variable but there are 18 observations taking on values between 0 and 1. Also, for handper 3,290 observations are not integer-valued. Finally, the precision of 148 values of educ is very suspicious.\nAs of 2013, a comment and a reply on data issues are available from the JAE archive. No attempts are made to fix these issues here. Data is used ‘as is’ in order to replicate Greene (2007, 2008, 2011).\n\nJournal of Applied Econometrics Data Archive.\nhttp://qed.econ.queensu.ca/jae/2003-v18.4/riphahn-wambach-million/\n\nGreene WH (2007). “Functional Form and Heterogeneity in Models for Count Data”. Foundations and Trends in Econometrics, 1(2), 113–218.\nGreene WH (2008). “Functional Forms for the Negative Binomial Model for Count Data”. Economics Letters, 99, 585–590.\nGreene WH (2011). Econometric Analysis, 7th edition. Upper Saddle River, NJ: Prentice Hall.\nRiphahn RT, Wambach A, Million A (2003). “Incentive Effects in the Demand for Health Care: A Bivariate Panel Count Data Estimation”. Journal of Applied Econometrics, 18(4), 387–405.\n\n\nlibrary(\"countreg\")\n\ndata(\"GSOEP\")\n\nif(exists(\"nbp.u\") & exists(\"nbp.o\")) {\n\n# We fit an NB1, an NB2 and an NBP model as in Greene (2007) ...\nmodel.u &lt;- docvis ~ age + I(age^2) + hsat + handdum + handper + married + educ + \n           hhninc + hhkids + self + civil + bluec + working + public + addon + year\nfm.nb1.u &lt;- nbp.u(model.u, data = subset(GSOEP, female == \"no\"), p.fix = 1)\nfm.nb2.u &lt;- nbp.u(model.u, data = subset(GSOEP, female == \"no\"), p.fix = 2)\nfm.nbp.u &lt;- nbp.u(model.u, data = subset(GSOEP, female == \"no\"))\n\n# ... and show that the former two are inferior to the last one by standard likelihood ratio tests\nlibrary(\"lmtest\")\nlrtest(fm.nb1.u, fm.nbp.u)\nlrtest(fm.nb2.u, fm.nbp.u)\n\n# model with observed heterogeneity\nmodel.o &lt;- docvis ~ age + I(age^2) + hsat + handdum + handper + married + educ + \n           hhninc + hhkids + self + civil + bluec + working + public + addon + year | hhninc + educ\n## fm.nbp.o &lt;- nbp.o(model.o, data = subset(GSOEP, female == \"no\"))\n\n# visualization of the fit\n# plot(fm.nbp.o)\n\n# things to replicate from Greene (2011):\n# Ex. 11.16, Tab 11.13, pp 451-452: nonlinear regression\n# Tab. 14.2, pp 580--581: logistic and Poisson \n# Tab. 14.10, pp 611-613: geometric regression\n# Ex. 17.7: binary Chow test (maybe not?)\n# Tab 18.14, p 850: Poisson, various negbins\n# Tab 18.17, p 860 (panel models, some of)\n# Tab 18.19, p 866: hurdle models\n\n# things to replicate from Greene (2008):\n# Tab 2: Poisson, various negbins, presumably same as book\n\n}",
    "crumbs": [
      "Data sets",
      "GSOEP"
    ]
  },
  {
    "objectID": "man/zerotrunc.html",
    "href": "man/zerotrunc.html",
    "title": "countreg",
    "section": "",
    "text": "Fit zero-truncated regression models for count data via maximum likelihood.\n\nzerotrunc(formula, data, subset, na.action, weights, offset,\n  dist = c(\"poisson\", \"negbin\", \"geometric\"), theta = Inf,\n  control = zerotrunc.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor.\n\n\n\n\ndist\n\n\ncharacter specification of the count distribution family.\n\n\n\n\ntheta\n\n\nnumeric. Alternative (and more flexible) specification of the count distribution family. Some values correspond to dist values: theta = Inf (“poisson”), theta = 1 (“geometric”), theta = NULL (“negbin”). But every non-negative value for theta is allowed. When theta is given, dist must not be specified and vice versa.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via zerotrunc.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\n…\n\n\narguments passed to zerotrunc.control in the default setup.\n\n\n\nAll zero-truncated count data models in zerotrunc are obtained from the corresponding untruncated distribution using a log-link between the mean of the untruncated distribution and the linear predictor. All parameters are estimated by maximum likelihood using optim, with control options set in zerotrunc.control. Starting values can be supplied, otherwise they are estimated by glm.fit (the default). Standard errors are derived numerically using the Hessian matrix returned by optim. See zerotrunc.control for details.\nThe returned fitted model object is of class “zerotrunc” and is similar to fitted “glm” objects.\nA set of standard extractor functions for fitted model objects is available for objects of class “zerotrunc”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.frame, model.matrix. See predict.zerotrunc for more details on all methods.\n\nAn object of class “zerotrunc”, i.e., a list with components including\n\n\n\ncoefficients\n\n\nestimated coefficients,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list with the output from the optim call for minimizing the negative log-likelihood,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call(s),\n\n\n\n\nweights\n\n\nthe case weights used (if any),\n\n\n\n\noffset\n\n\nthe offset vector used (if any),\n\n\n\n\nn\n\n\nnumber of observations,\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model,\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\nterms objects for the model,\n\n\n\n\ntheta\n\n\n(estimated) \\(\\theta\\) parameter of the negative binomial model,\n\n\n\n\nSE.logtheta\n\n\nstandard error for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of the coefficients in the model (derived from the Hessian of the optim output),\n\n\n\n\ndist\n\n\ncharacter describing the distribution used,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\ncontrasts corresponding to levels from the model,\n\n\n\n\nmodel\n\n\nthe model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\nmodel matrix (if x = TRUE).\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nzerotrunc.control, glm, glm.fit, glm.nb, zeroinfl, hurdle\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\ncs &lt;- subset(cs, subset = satellites &gt; 0)\n\n## poisson\nzt_p &lt;- zerotrunc(satellites ~ ., data = cs)\n## or equivalently\nzt_p &lt;- zerotrunc(satellites ~ ., data = cs, theta = Inf)\nsummary(zt_p)\n\n\nCall:\nzerotrunc(formula = satellites ~ ., data = cs, theta = Inf)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.5409 -0.9350 -0.2051  0.6278  3.7722 \n\nCoefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007166   0.066627   0.108    0.914\n\nNumber of iterations in BFGS optimization: 6 \nLog-likelihood: -267.5 on 3 Df\n\n## negbin\nzt_nb &lt;- zerotrunc(satellites ~ ., data = cs, dist = \"negbin\")\n## or equivalently\nzt_nb &lt;- zerotrunc(satellites ~ ., data = cs, theta = NULL)\nsummary(zt_nb)\n\n\nCall:\nzerotrunc(formula = satellites ~ ., data = cs, theta = NULL)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.1636 -0.7158 -0.1520  0.4498  2.4215 \n\nCoefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.427224   0.941131   0.454    0.650    \nwidth       0.037890   0.032751   1.157    0.247    \ncolor       0.006985   0.091081   0.077    0.939    \nLog(theta)  1.527243   0.352937   4.327 1.51e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTheta = 4.6055\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -255.8 on 4 Df",
    "crumbs": [
      "Zero-truncated models",
      "zerotrunc"
    ]
  },
  {
    "objectID": "man/zerotrunc.html#zero-truncated-count-data-regression",
    "href": "man/zerotrunc.html#zero-truncated-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Fit zero-truncated regression models for count data via maximum likelihood.\n\nzerotrunc(formula, data, subset, na.action, weights, offset,\n  dist = c(\"poisson\", \"negbin\", \"geometric\"), theta = Inf,\n  control = zerotrunc.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor.\n\n\n\n\ndist\n\n\ncharacter specification of the count distribution family.\n\n\n\n\ntheta\n\n\nnumeric. Alternative (and more flexible) specification of the count distribution family. Some values correspond to dist values: theta = Inf (“poisson”), theta = 1 (“geometric”), theta = NULL (“negbin”). But every non-negative value for theta is allowed. When theta is given, dist must not be specified and vice versa.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via zerotrunc.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\n…\n\n\narguments passed to zerotrunc.control in the default setup.\n\n\n\nAll zero-truncated count data models in zerotrunc are obtained from the corresponding untruncated distribution using a log-link between the mean of the untruncated distribution and the linear predictor. All parameters are estimated by maximum likelihood using optim, with control options set in zerotrunc.control. Starting values can be supplied, otherwise they are estimated by glm.fit (the default). Standard errors are derived numerically using the Hessian matrix returned by optim. See zerotrunc.control for details.\nThe returned fitted model object is of class “zerotrunc” and is similar to fitted “glm” objects.\nA set of standard extractor functions for fitted model objects is available for objects of class “zerotrunc”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.frame, model.matrix. See predict.zerotrunc for more details on all methods.\n\nAn object of class “zerotrunc”, i.e., a list with components including\n\n\n\ncoefficients\n\n\nestimated coefficients,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list with the output from the optim call for minimizing the negative log-likelihood,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call(s),\n\n\n\n\nweights\n\n\nthe case weights used (if any),\n\n\n\n\noffset\n\n\nthe offset vector used (if any),\n\n\n\n\nn\n\n\nnumber of observations,\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model,\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\nterms objects for the model,\n\n\n\n\ntheta\n\n\n(estimated) \\(\\theta\\) parameter of the negative binomial model,\n\n\n\n\nSE.logtheta\n\n\nstandard error for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of the coefficients in the model (derived from the Hessian of the optim output),\n\n\n\n\ndist\n\n\ncharacter describing the distribution used,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\ncontrasts corresponding to levels from the model,\n\n\n\n\nmodel\n\n\nthe model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\nmodel matrix (if x = TRUE).\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nzerotrunc.control, glm, glm.fit, glm.nb, zeroinfl, hurdle\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\ncs &lt;- subset(cs, subset = satellites &gt; 0)\n\n## poisson\nzt_p &lt;- zerotrunc(satellites ~ ., data = cs)\n## or equivalently\nzt_p &lt;- zerotrunc(satellites ~ ., data = cs, theta = Inf)\nsummary(zt_p)\n\n\nCall:\nzerotrunc(formula = satellites ~ ., data = cs, theta = Inf)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.5409 -0.9350 -0.2051  0.6278  3.7722 \n\nCoefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007166   0.066627   0.108    0.914\n\nNumber of iterations in BFGS optimization: 6 \nLog-likelihood: -267.5 on 3 Df\n\n## negbin\nzt_nb &lt;- zerotrunc(satellites ~ ., data = cs, dist = \"negbin\")\n## or equivalently\nzt_nb &lt;- zerotrunc(satellites ~ ., data = cs, theta = NULL)\nsummary(zt_nb)\n\n\nCall:\nzerotrunc(formula = satellites ~ ., data = cs, theta = NULL)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.1636 -0.7158 -0.1520  0.4498  2.4215 \n\nCoefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.427224   0.941131   0.454    0.650    \nwidth       0.037890   0.032751   1.157    0.247    \ncolor       0.006985   0.091081   0.077    0.939    \nLog(theta)  1.527243   0.352937   4.327 1.51e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTheta = 4.6055\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -255.8 on 4 Df",
    "crumbs": [
      "Zero-truncated models",
      "zerotrunc"
    ]
  },
  {
    "objectID": "man/nbreg.html",
    "href": "man/nbreg.html",
    "title": "countreg",
    "section": "",
    "text": "Fit negative binomial regression models for count data via maximum likelihood\n\nnbreg(formula, data, subset, na.action, weights, offset, theta = NULL,\n  dist = \"NB2\", link = \"log\", link.theta = \"log\", control = nbreg.control(...),\n  model = TRUE, y = TRUE, x = FALSE, z = FALSE, hessA = TRUE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, see details.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor. See below for more information on offsets.\n\n\n\n\ntheta\n\n\nnumeric. Optional. If specified, then the dispersion parameter is not estimated.\n\n\n\n\ndist\n\n\ncharacter specification of the NB type. Either “NB2” or “NB1”. Lowercase versions “nb2” and “nb1” are equivalent.\n\n\n\n\nlink\n\n\ncharacter specification of the link function for the mean. Currently, only “log” is supported.\n\n\n\n\nlink.theta\n\n\ncharacter specification of the link function for the dispersion parameter. Currently, only “log” is supported.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via nbreg.control.\n\n\n\n\nmodel, y, x, z\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\nhessA\n\n\nlogical. If TRUE, then the analytical Hessian is used to compute the covariance matrix of the estimator.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nThe Negative Binomial Distribution is often used to model count data with overdispersion. Cameron and Trivedi (2013) offer two parametrization, negative binomial type 2 (NB2) and type 1 (NB1). NB2 is parametrized as follows\n\n\n\\(f(y | \\mu, \\theta) = \\dfrac{\\Gamma(\\theta + y)}{\\Gamma(\\theta) y!}  \\left(\\dfrac{\\theta}{\\theta + \\mu}\\right)^\\theta  \\left(\\dfrac{\\mu}{\\theta + \\mu}\\right)^y.\\)\nFor NB1 replace \\(\\theta\\) with \\(\\mu\\theta\\) on the RHS.\nThis function further allows us to model the dispersion parameter with covariates via a two-part formula. If a formula of type y ~ x1 + x2 is supplied, then the regressors are employed in the mean and the dispersion parameter is estimated as a constant, i.e. a standard NB2 or NB1 is estimated. This is equivalent to y ~ x1 + x2 | 1. If a formula of type y ~ x1 + x2 + x3 | z1 + z2 is given, then the mean \\(mu\\) is modeled using x1 + x2 and the dispersion parameter \\(\\theta\\) with z1 + z2. If dist = “NB2”, then the function estimates the NBH model.\nOffsets can be specified in both the mean and dispersion parameter \\(\\theta\\): y ~ x1 + x2 + offset(x3) | z1 + offset(z2), where x3 is used as an offset (i.e., with coefficient fixed to 1) in the mean \\(mu\\) and z2 analogously in \\(\\theta\\). By the rule stated above y ~ x1 + offset(x2) is equivalent to y ~ x1 + offset(x2) | 1. Instead of using the offset() wrapper within the formula, the offset argument can also be employed which sets an offset only for \\(mu\\). Thus, formula = y ~ x1 and offset = x2 is equivalent to formula = y ~ x1 + offset(x2) | 1.\nAll parameters are estimated by maximum likelihood using optim, with control options set in nbreg.control. Starting values can be supplied or are estimated by a Poisson regression in glm.fit (the default, starting values of coefficients in \\(\\theta\\) are set to zero to ensure compatibility with NB1). Standard errors are derived analytically or numerically using the Hessian matrix returned byoptim. See nbreg.control for details.\nThe returned fitted model object is of class “nbreg” and is similar to fitted “glm” objects.\nA set of standard extractor functions for fitted model objects is available for objects of class “nbreg”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.matrix. See predict.nbreg for more details on all methods.\n\nAn object of class “nbreg”, i.e., a list with components including\n\n\n\ncoefficients\n\n\na vector containing the coefficients from the mean,\n\n\n\n\ncoefficients.theta\n\n\na vector containing the coefficients from the dispersion parameter theta,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list with the output from the optim call for maximizing the log-likelihood,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call,\n\n\n\n\nweights\n\n\nthe case weights used,\n\n\n\n\noffset\n\n\na list with elements “mu” and “theta” containing the offset vectors (if any) from the respective parameters,\n\n\n\n\nn\n\n\nnumber of observations (with weights &gt; 0),\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model,\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\na list with elements “mu”, “theta” and “full” containing the terms objects for the respective parameters,\n\n\n\n\nSE.logtheta\n\n\nstandard error for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all coefficients in the model (derived from the analytical Hessian (hessA = TRUE) or from the Hessian of the optim output (hessA = FALSE)),\n\n\n\n\ndist\n\n\ncharacter string describing the type of NB distribution used,\n\n\n\n\nlink\n\n\ncharacter string describing the link of the mean,\n\n\n\n\nlink.theta\n\n\ncharacter string describing the link of the dispersion parameter theta,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “mu” and “theta” containing the contrasts corresponding to levels from the respective parts,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\nthe model matrix for the mean (if x = TRUE),\n\n\n\n\nz\n\n\nthe model matrix for the mean (if z = TRUE),\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\n\nnbreg.control, glm, glm.fit, glm.nb,\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## NB2\nfm_nb2 &lt;- nbreg(satellites ~ width + color, data = CrabSatellites)\n\n## NB1\nfm_nb1 &lt;- nbreg(satellites ~ width + color, data = CrabSatellites, dist = \"NB1\")\n\n## NBH\nfm_nbh &lt;- nbreg(satellites ~ width + color | weight, data = CrabSatellites)\n\n## NB1 with variable theta\nfm_nb1h &lt;- nbreg(satellites ~ width + color | weight, data = CrabSatellites,\n                dist = \"NB1\")\n\n## Example not run:\n## data\n# data(\"GSOEP\", package = \"countreg\")\n# gsoep &lt;- subset(GSOEP, year == \"1984\")\n\n## NB2\n# fm_nb2 &lt;- nbreg(docvis ~ educ + public + addon,\n#                 data = gsoep)\n                \n## NB1\n# fm_nb1 &lt;- nbreg(docvis ~ educ + public + addon,\n#                 data = gsoep, dist = \"NB1\")                \n\n## NBH\n# fm_nbh &lt;- nbreg(docvis ~ educ + public + addon | married + public,\n#                 data = gsoep)\n                \n## NB1 with variable theta\n# fm_nb1h &lt;- nbreg(docvis ~ educ + public + addon | married + public,\n#                 data = gsoep, dist = \"NB1\")",
    "crumbs": [
      "Negative binomial regression",
      "nbreg"
    ]
  },
  {
    "objectID": "man/nbreg.html#negative-binomial-count-data-regression",
    "href": "man/nbreg.html#negative-binomial-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Fit negative binomial regression models for count data via maximum likelihood\n\nnbreg(formula, data, subset, na.action, weights, offset, theta = NULL,\n  dist = \"NB2\", link = \"log\", link.theta = \"log\", control = nbreg.control(...),\n  model = TRUE, y = TRUE, x = FALSE, z = FALSE, hessA = TRUE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, see details.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor. See below for more information on offsets.\n\n\n\n\ntheta\n\n\nnumeric. Optional. If specified, then the dispersion parameter is not estimated.\n\n\n\n\ndist\n\n\ncharacter specification of the NB type. Either “NB2” or “NB1”. Lowercase versions “nb2” and “nb1” are equivalent.\n\n\n\n\nlink\n\n\ncharacter specification of the link function for the mean. Currently, only “log” is supported.\n\n\n\n\nlink.theta\n\n\ncharacter specification of the link function for the dispersion parameter. Currently, only “log” is supported.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via nbreg.control.\n\n\n\n\nmodel, y, x, z\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\nhessA\n\n\nlogical. If TRUE, then the analytical Hessian is used to compute the covariance matrix of the estimator.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nThe Negative Binomial Distribution is often used to model count data with overdispersion. Cameron and Trivedi (2013) offer two parametrization, negative binomial type 2 (NB2) and type 1 (NB1). NB2 is parametrized as follows\n\n\n\\(f(y | \\mu, \\theta) = \\dfrac{\\Gamma(\\theta + y)}{\\Gamma(\\theta) y!}  \\left(\\dfrac{\\theta}{\\theta + \\mu}\\right)^\\theta  \\left(\\dfrac{\\mu}{\\theta + \\mu}\\right)^y.\\)\nFor NB1 replace \\(\\theta\\) with \\(\\mu\\theta\\) on the RHS.\nThis function further allows us to model the dispersion parameter with covariates via a two-part formula. If a formula of type y ~ x1 + x2 is supplied, then the regressors are employed in the mean and the dispersion parameter is estimated as a constant, i.e. a standard NB2 or NB1 is estimated. This is equivalent to y ~ x1 + x2 | 1. If a formula of type y ~ x1 + x2 + x3 | z1 + z2 is given, then the mean \\(mu\\) is modeled using x1 + x2 and the dispersion parameter \\(\\theta\\) with z1 + z2. If dist = “NB2”, then the function estimates the NBH model.\nOffsets can be specified in both the mean and dispersion parameter \\(\\theta\\): y ~ x1 + x2 + offset(x3) | z1 + offset(z2), where x3 is used as an offset (i.e., with coefficient fixed to 1) in the mean \\(mu\\) and z2 analogously in \\(\\theta\\). By the rule stated above y ~ x1 + offset(x2) is equivalent to y ~ x1 + offset(x2) | 1. Instead of using the offset() wrapper within the formula, the offset argument can also be employed which sets an offset only for \\(mu\\). Thus, formula = y ~ x1 and offset = x2 is equivalent to formula = y ~ x1 + offset(x2) | 1.\nAll parameters are estimated by maximum likelihood using optim, with control options set in nbreg.control. Starting values can be supplied or are estimated by a Poisson regression in glm.fit (the default, starting values of coefficients in \\(\\theta\\) are set to zero to ensure compatibility with NB1). Standard errors are derived analytically or numerically using the Hessian matrix returned byoptim. See nbreg.control for details.\nThe returned fitted model object is of class “nbreg” and is similar to fitted “glm” objects.\nA set of standard extractor functions for fitted model objects is available for objects of class “nbreg”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.matrix. See predict.nbreg for more details on all methods.\n\nAn object of class “nbreg”, i.e., a list with components including\n\n\n\ncoefficients\n\n\na vector containing the coefficients from the mean,\n\n\n\n\ncoefficients.theta\n\n\na vector containing the coefficients from the dispersion parameter theta,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list with the output from the optim call for maximizing the log-likelihood,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call,\n\n\n\n\nweights\n\n\nthe case weights used,\n\n\n\n\noffset\n\n\na list with elements “mu” and “theta” containing the offset vectors (if any) from the respective parameters,\n\n\n\n\nn\n\n\nnumber of observations (with weights &gt; 0),\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model,\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\na list with elements “mu”, “theta” and “full” containing the terms objects for the respective parameters,\n\n\n\n\nSE.logtheta\n\n\nstandard error for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all coefficients in the model (derived from the analytical Hessian (hessA = TRUE) or from the Hessian of the optim output (hessA = FALSE)),\n\n\n\n\ndist\n\n\ncharacter string describing the type of NB distribution used,\n\n\n\n\nlink\n\n\ncharacter string describing the link of the mean,\n\n\n\n\nlink.theta\n\n\ncharacter string describing the link of the dispersion parameter theta,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “mu” and “theta” containing the contrasts corresponding to levels from the respective parts,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\nthe model matrix for the mean (if x = TRUE),\n\n\n\n\nz\n\n\nthe model matrix for the mean (if z = TRUE),\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\n\nnbreg.control, glm, glm.fit, glm.nb,\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## NB2\nfm_nb2 &lt;- nbreg(satellites ~ width + color, data = CrabSatellites)\n\n## NB1\nfm_nb1 &lt;- nbreg(satellites ~ width + color, data = CrabSatellites, dist = \"NB1\")\n\n## NBH\nfm_nbh &lt;- nbreg(satellites ~ width + color | weight, data = CrabSatellites)\n\n## NB1 with variable theta\nfm_nb1h &lt;- nbreg(satellites ~ width + color | weight, data = CrabSatellites,\n                dist = \"NB1\")\n\n## Example not run:\n## data\n# data(\"GSOEP\", package = \"countreg\")\n# gsoep &lt;- subset(GSOEP, year == \"1984\")\n\n## NB2\n# fm_nb2 &lt;- nbreg(docvis ~ educ + public + addon,\n#                 data = gsoep)\n                \n## NB1\n# fm_nb1 &lt;- nbreg(docvis ~ educ + public + addon,\n#                 data = gsoep, dist = \"NB1\")                \n\n## NBH\n# fm_nbh &lt;- nbreg(docvis ~ educ + public + addon | married + public,\n#                 data = gsoep)\n                \n## NB1 with variable theta\n# fm_nb1h &lt;- nbreg(docvis ~ educ + public + addon | married + public,\n#                 data = gsoep, dist = \"NB1\")",
    "crumbs": [
      "Negative binomial regression",
      "nbreg"
    ]
  },
  {
    "objectID": "man/hpois.html",
    "href": "man/hpois.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and, variance for the (zero-)hurdle Poisson distribution with parameters mu (= mean of the underlying Poisson distribution) and hurdle crossing probability pi (i.e., 1 - pi is the probability for observed zeros).\n\n\n\nshpois(x, lambda, pi, parameter = c(\"lambda\", \"pi\"), drop = TRUE)\nhhpois(x, lambda, pi, parameter = c(\"lambda\", \"pi\"), drop = TRUE)\nmean_hpois(lambda, pi, drop = TRUE)\nvar_hpois(lambda, pi, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nlambda\n\n\nvector of non-negative means of the underlying Poisson distribution.\n\n\n\n\npi\n\n\nvector of hurdle crossing probabilities (i.e., 1 - pi is the probability for observed zeros).\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “lambda” and/or “pi” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe underlying Poisson distribution has density\n\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\). The hurdle density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\) and \\(g(0) = 1 - \\pi\\), respectively.\n\n\n\nshpois gives the score function (= derivative of the log-density with respect to lambda and/or pi). hhpois gives the hessian (= 2nd derivative of the log-density with respect to lambda and/or pi). mean_hpois and var_hpois give the mean and the variance, respectively.\n\n\n\ndhpois, dpois, hurdle",
    "crumbs": [
      "Distribution extensions",
      "Hurdle Poisson"
    ]
  },
  {
    "objectID": "man/hpois.html#extension-of-the-hurdle-poisson-distribution",
    "href": "man/hpois.html#extension-of-the-hurdle-poisson-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and, variance for the (zero-)hurdle Poisson distribution with parameters mu (= mean of the underlying Poisson distribution) and hurdle crossing probability pi (i.e., 1 - pi is the probability for observed zeros).\n\n\n\nshpois(x, lambda, pi, parameter = c(\"lambda\", \"pi\"), drop = TRUE)\nhhpois(x, lambda, pi, parameter = c(\"lambda\", \"pi\"), drop = TRUE)\nmean_hpois(lambda, pi, drop = TRUE)\nvar_hpois(lambda, pi, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nlambda\n\n\nvector of non-negative means of the underlying Poisson distribution.\n\n\n\n\npi\n\n\nvector of hurdle crossing probabilities (i.e., 1 - pi is the probability for observed zeros).\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “lambda” and/or “pi” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe underlying Poisson distribution has density\n\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\). The hurdle density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\) and \\(g(0) = 1 - \\pi\\), respectively.\n\n\n\nshpois gives the score function (= derivative of the log-density with respect to lambda and/or pi). hhpois gives the hessian (= 2nd derivative of the log-density with respect to lambda and/or pi). mean_hpois and var_hpois give the mean and the variance, respectively.\n\n\n\ndhpois, dpois, hurdle",
    "crumbs": [
      "Distribution extensions",
      "Hurdle Poisson"
    ]
  },
  {
    "objectID": "man/zeroinfl.control.html",
    "href": "man/zeroinfl.control.html",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of zero-inflated regression models using zeroinfl.\n\nzeroinfl.control(method = \"BFGS\", maxit = 10000, trace = FALSE,\n  EM = FALSE, start = NULL, hessian = TRUE, ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\ntrace\n\n\nlogical or integer controlling whether tracing information on the progress of the optimization should be produced (passed to optim).\n\n\n\n\nEM\n\n\nlogical. Should starting values be estimated by the EM (expectation maximization) algorithm? See details.\n\n\n\n\nstart\n\n\nan optional list with elements “count” and “zero” (and potentially “theta”) containing the coefficients for the corresponding component.\n\n\n\n\nhessian\n\n\nlogical. Should the Hessian be computed to derive an estimate of the variance-covariance matrix? If FALSE, the variance-covariance matrix contains only NAs.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in zeroinfl are estimated by maximum likelihood using optim with control options set in zeroinfl.control. Most arguments are passed on directly to optim, only trace is also used within zeroinfl and EM/start control the choice of starting values for calling optim.\nStarting values can be supplied, estimated by the EM (expectation maximization) algorithm, or by glm.fit (the default). Standard errors are derived numerically using the Hessian matrix returned by optim. To supply starting values, start should be a list with elements “count” and “zero” and potentially “theta” (for negative binomial components only) containing the starting values for the coefficients of the corresponding component of the model.\n\nA list with the arguments specified.\n\nzeroinfl\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nfm1 &lt;- zeroinfl(satellites ~ width + as.numeric(color), data = CrabSatellites)\n\n## use EM algorithm for start values\nfm2 &lt;- zeroinfl(satellites ~ width + as.numeric(color), data = CrabSatellites, EM = TRUE)\n\n## user-supplied start values\nfm3 &lt;- zeroinfl(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  start = list(count = c(0.5, 0, 0), zero = c(10, -0.5, 0.5)))",
    "crumbs": [
      "Zero-inflated models",
      "zeroinfl.control"
    ]
  },
  {
    "objectID": "man/zeroinfl.control.html#control-parameters-for-zero-inflated-count-data-regression",
    "href": "man/zeroinfl.control.html#control-parameters-for-zero-inflated-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of zero-inflated regression models using zeroinfl.\n\nzeroinfl.control(method = \"BFGS\", maxit = 10000, trace = FALSE,\n  EM = FALSE, start = NULL, hessian = TRUE, ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\ntrace\n\n\nlogical or integer controlling whether tracing information on the progress of the optimization should be produced (passed to optim).\n\n\n\n\nEM\n\n\nlogical. Should starting values be estimated by the EM (expectation maximization) algorithm? See details.\n\n\n\n\nstart\n\n\nan optional list with elements “count” and “zero” (and potentially “theta”) containing the coefficients for the corresponding component.\n\n\n\n\nhessian\n\n\nlogical. Should the Hessian be computed to derive an estimate of the variance-covariance matrix? If FALSE, the variance-covariance matrix contains only NAs.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in zeroinfl are estimated by maximum likelihood using optim with control options set in zeroinfl.control. Most arguments are passed on directly to optim, only trace is also used within zeroinfl and EM/start control the choice of starting values for calling optim.\nStarting values can be supplied, estimated by the EM (expectation maximization) algorithm, or by glm.fit (the default). Standard errors are derived numerically using the Hessian matrix returned by optim. To supply starting values, start should be a list with elements “count” and “zero” and potentially “theta” (for negative binomial components only) containing the starting values for the coefficients of the corresponding component of the model.\n\nA list with the arguments specified.\n\nzeroinfl\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nfm1 &lt;- zeroinfl(satellites ~ width + as.numeric(color), data = CrabSatellites)\n\n## use EM algorithm for start values\nfm2 &lt;- zeroinfl(satellites ~ width + as.numeric(color), data = CrabSatellites, EM = TRUE)\n\n## user-supplied start values\nfm3 &lt;- zeroinfl(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  start = list(count = c(0.5, 0, 0), zero = c(10, -0.5, 0.5)))",
    "crumbs": [
      "Zero-inflated models",
      "zeroinfl.control"
    ]
  },
  {
    "objectID": "man/predict.hurdle.html",
    "href": "man/predict.hurdle.html",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted hurdle regression model objects of class “hurdle”.\n\n## S3 method for class 'hurdle'\npredict(object, newdata,\n  type = c(\"mean\", \"variance\", \"quantile\", \"probability\", \"density\", \"loglikelihood\", \"parameters\", \"distribution\"),\n  model = c(\"full\", \"count\", \"zero\", \"truncated\"),\n  na.action = na.pass, at = NULL, drop = TRUE, ...)\n## S3 method for class 'hurdle'\nresiduals(object, type = c(\"pearson\", \"response\"), ...)\n\n## S3 method for class 'hurdle'\ncoef(object, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'hurdle'\nvcov(object, model = c(\"full\", \"count\", \"zero\"), ...)\n\n## S3 method for class 'hurdle'\nterms(x, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'hurdle'\nmodel.matrix(object, model = c(\"count\", \"zero\"), ...)\n\n\n\n\n\nobject, x\n\n\nan object of class “hurdle” as returned by hurdle.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model the terms or model matrix should be extracted.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nat\n\n\noptionally, if type = “prob”, a numeric vector at which the probabilities are evaluated. By default 0:max(y) is used where y is the original observed response.\n\n\n\n\ndrop\n\n\nlogical. Should predictions be returned in a data frame or (if possible) dropped to a vector (default).\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “hurdle”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix (estimated from the Hessian in the numerical optimization of the log-likelihood). As usual, the summary method returns an object of class “summary.hurdle” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nThe methods for coef and vcov by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting the model argument, the estimates for the corresponding model component can be extracted.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts), the predicted mean from the count component (without zero hurdle) and the predicted ratio of probabilities for observing a non-zero count. The latter is the ratio of probabilities for a non-zero implied by the zero hurdle component and a non-zero count in the non-truncated count distribution. See also Appendix C in Zeileis et al. (2008).\nThe residuals method can compute raw residuals (observed - fitted) and Pearson residuals (raw residuals scaled by square root of variance function).\nThe terms and model.matrix extractors can be used to extract the relevant information for either component of the model.\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nhurdle\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm &lt;- hurdle(satellites ~ 1 | width + color, data = CrabSatellites)\n\nplot(residuals(fm) ~ fitted(fm))\n\n\n\n\n\n\ncoef(fm)\n\ncount_(Intercept)  zero_(Intercept)        zero_width      zero_color.L \n       1.50384532      -11.75551773        0.46795599       -0.95837247 \n     zero_color.Q      zero_color.C \n      -0.58926921       -0.09867217 \n\ncoef(fm, model = \"zero\")\n\n (Intercept)        width      color.L      color.Q      color.C \n-11.75551773   0.46795599  -0.95837247  -0.58926921  -0.09867217 \n\nsummary(fm)\n\n\nCall:\nhurdle(formula = satellites ~ 1 | width + color, data = CrabSatellites)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.6805 -0.8220 -0.3307  0.5894  4.4785 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.50385    0.04567   32.93   &lt;2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -11.75552    2.74141  -4.288 1.80e-05 ***\nwidth         0.46796    0.10553   4.434 9.23e-06 ***\ncolor.L      -0.95837    0.58033  -1.651   0.0986 .  \ncolor.Q      -0.58927    0.47472  -1.241   0.2145    \ncolor.C      -0.09867    0.33738  -0.292   0.7699    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -362.5 on 6 Df\n\nlogLik(fm)\n\n'log Lik.' -362.4638 (df=6)",
    "crumbs": [
      "Hurdle models",
      "predict.hurdle"
    ]
  },
  {
    "objectID": "man/predict.hurdle.html#methods-for-hurdle-objects",
    "href": "man/predict.hurdle.html#methods-for-hurdle-objects",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted hurdle regression model objects of class “hurdle”.\n\n## S3 method for class 'hurdle'\npredict(object, newdata,\n  type = c(\"mean\", \"variance\", \"quantile\", \"probability\", \"density\", \"loglikelihood\", \"parameters\", \"distribution\"),\n  model = c(\"full\", \"count\", \"zero\", \"truncated\"),\n  na.action = na.pass, at = NULL, drop = TRUE, ...)\n## S3 method for class 'hurdle'\nresiduals(object, type = c(\"pearson\", \"response\"), ...)\n\n## S3 method for class 'hurdle'\ncoef(object, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'hurdle'\nvcov(object, model = c(\"full\", \"count\", \"zero\"), ...)\n\n## S3 method for class 'hurdle'\nterms(x, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'hurdle'\nmodel.matrix(object, model = c(\"count\", \"zero\"), ...)\n\n\n\n\n\nobject, x\n\n\nan object of class “hurdle” as returned by hurdle.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model the terms or model matrix should be extracted.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nat\n\n\noptionally, if type = “prob”, a numeric vector at which the probabilities are evaluated. By default 0:max(y) is used where y is the original observed response.\n\n\n\n\ndrop\n\n\nlogical. Should predictions be returned in a data frame or (if possible) dropped to a vector (default).\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “hurdle”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix (estimated from the Hessian in the numerical optimization of the log-likelihood). As usual, the summary method returns an object of class “summary.hurdle” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nThe methods for coef and vcov by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting the model argument, the estimates for the corresponding model component can be extracted.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts), the predicted mean from the count component (without zero hurdle) and the predicted ratio of probabilities for observing a non-zero count. The latter is the ratio of probabilities for a non-zero implied by the zero hurdle component and a non-zero count in the non-truncated count distribution. See also Appendix C in Zeileis et al. (2008).\nThe residuals method can compute raw residuals (observed - fitted) and Pearson residuals (raw residuals scaled by square root of variance function).\nThe terms and model.matrix extractors can be used to extract the relevant information for either component of the model.\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nhurdle\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm &lt;- hurdle(satellites ~ 1 | width + color, data = CrabSatellites)\n\nplot(residuals(fm) ~ fitted(fm))\n\n\n\n\n\n\ncoef(fm)\n\ncount_(Intercept)  zero_(Intercept)        zero_width      zero_color.L \n       1.50384532      -11.75551773        0.46795599       -0.95837247 \n     zero_color.Q      zero_color.C \n      -0.58926921       -0.09867217 \n\ncoef(fm, model = \"zero\")\n\n (Intercept)        width      color.L      color.Q      color.C \n-11.75551773   0.46795599  -0.95837247  -0.58926921  -0.09867217 \n\nsummary(fm)\n\n\nCall:\nhurdle(formula = satellites ~ 1 | width + color, data = CrabSatellites)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.6805 -0.8220 -0.3307  0.5894  4.4785 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.50385    0.04567   32.93   &lt;2e-16 ***\nZero hurdle model coefficients (binomial with logit link):\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -11.75552    2.74141  -4.288 1.80e-05 ***\nwidth         0.46796    0.10553   4.434 9.23e-06 ***\ncolor.L      -0.95837    0.58033  -1.651   0.0986 .  \ncolor.Q      -0.58927    0.47472  -1.241   0.2145    \ncolor.C      -0.09867    0.33738  -0.292   0.7699    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 7 \nLog-likelihood: -362.5 on 6 Df\n\nlogLik(fm)\n\n'log Lik.' -362.4638 (df=6)",
    "crumbs": [
      "Hurdle models",
      "predict.hurdle"
    ]
  },
  {
    "objectID": "man/ztnbinom.html",
    "href": "man/ztnbinom.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the zero-truncated negative binomial distribution with parameters mu (= mean of the untruncated distribution) and dispersion parameter theta (or equivalently size).\n\n\n\nsztnbinom(x, mu, theta, size, parameter = c(\"mu\", \"theta\", \"size\"), drop = TRUE)\nhztnbinom(x, mu, theta, size, parameter = c(\"mu\", \"theta\"), drop = TRUE)\nmean_ztnbinom(mu, theta, size, drop = TRUE)\nvar_ztnbinom(mu, theta, size, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nmu\n\n\nvector of non-negative means of the untruncated negative binomial distribution.\n\n\n\n\ntheta, size\n\n\nvector of strictly positive dispersion parameters (shape parameter of the gamma mixing distribution). Only one of theta or size must be specified.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “theta”/“size” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe untruncated negative binomial distribution has density\n\nf(x) = \nfor \\(x = 0, 1, 2, \\ldots\\). The zero-truncated density is then simply obtained as\n\n\\(g(x) = \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\).\n\n\n\nsztnbinom gives the score function (= derivative of the log-density with respect to mu and/or theta). hztnbinom gives the hessian (= 2nd derivative of the log-density with respect to mu and/or theta). mean_ztnbinom and var_ztnbinom give the mean and the variance, respectively.\n\n\n\ndztnbinom, dnbinom, zerotrunc",
    "crumbs": [
      "Distribution extensions",
      "Zero-truncated negative binomial"
    ]
  },
  {
    "objectID": "man/ztnbinom.html#extension-of-the-zero-truncated-negative-binomial-distribution",
    "href": "man/ztnbinom.html#extension-of-the-zero-truncated-negative-binomial-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the zero-truncated negative binomial distribution with parameters mu (= mean of the untruncated distribution) and dispersion parameter theta (or equivalently size).\n\n\n\nsztnbinom(x, mu, theta, size, parameter = c(\"mu\", \"theta\", \"size\"), drop = TRUE)\nhztnbinom(x, mu, theta, size, parameter = c(\"mu\", \"theta\"), drop = TRUE)\nmean_ztnbinom(mu, theta, size, drop = TRUE)\nvar_ztnbinom(mu, theta, size, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nmu\n\n\nvector of non-negative means of the untruncated negative binomial distribution.\n\n\n\n\ntheta, size\n\n\nvector of strictly positive dispersion parameters (shape parameter of the gamma mixing distribution). Only one of theta or size must be specified.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “theta”/“size” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe untruncated negative binomial distribution has density\n\nf(x) = \nfor \\(x = 0, 1, 2, \\ldots\\). The zero-truncated density is then simply obtained as\n\n\\(g(x) = \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\).\n\n\n\nsztnbinom gives the score function (= derivative of the log-density with respect to mu and/or theta). hztnbinom gives the hessian (= 2nd derivative of the log-density with respect to mu and/or theta). mean_ztnbinom and var_ztnbinom give the mean and the variance, respectively.\n\n\n\ndztnbinom, dnbinom, zerotrunc",
    "crumbs": [
      "Distribution extensions",
      "Zero-truncated negative binomial"
    ]
  },
  {
    "objectID": "man/hurdle.control.html",
    "href": "man/hurdle.control.html",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of hurdle regression models using hurdle.\n\nhurdle.control(method = \"BFGS\", maxit = 10000, trace = FALSE,\n  separate = TRUE, start = NULL, hessian = TRUE, ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\ntrace\n\n\nlogical or integer controlling whether tracing information on\nthe progress of the optimization should be produced (passed to optim).\n\n\n\n\nseparate\n\n\nlogical. Should the estimation of the parameters in the truncated count component and hurdle zero component be carried out separately? See details.\n\n\n\n\nstart\n\n\nan optional list with elements “count” and “zero” (and potentially “theta”) containing the coefficients for the corresponding component.\n\n\n\n\nhessian\n\n\nlogical. Should the Hessian be computed to derive an estimate of the variance-covariance matrix? If FALSE, the variance-covariance matrix contains only NAs.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in hurdle are estimated by maximum likelihood using optim with control options set in hurdle.control. Most arguments are passed on directly to optim, only trace is also used within hurdle and separate/start control how optim is called.\nStarting values can be supplied via start or estimated by glm.fit (default).\nIf separate = TRUE (default) the likelihoods of the truncated count component and the hurdle zero component will be maximized separately, otherwise the joint likelihood is set up and maximized. In case of separate = FALSE and both dist == “negbin” and zero.dist == “negbin” the theta parameter is restricted to be identical across both negative binomial distributions.\nStandard errors are derived numerically using the Hessian matrix returned by optim. To supply starting values, start should be a list with elements “count” and “zero” and potentially “theta” (a named vector, for models with negative binomial components only) containing the starting values for the coefficients of the corresponding component of the model.\n\nA list with the arguments specified.\n\nhurdle\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nfm1 &lt;- hurdle(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  dist = \"negbin\", zero = \"negbin\")\n\n## user-supplied start values and other options\nfm2 &lt;- hurdle(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  dist = \"negbin\",\n  zero = \"negbin\",\n  trace = TRUE,\n  separate = FALSE,\n  start = list(count = c(0.5, 0, 0),\n           zero = c(-10, 0.5, -0.5),\n           theta = c(count = 1, zero = 1)))\n\nHurdle Count Model\ncount model: negbin with log link\nzero hurdle model: negbin with log link\ndependent variable:\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 \n62 16  9 19 19 15 13  4  6  3  3  1  1  0  1  1 \ncalling optim() for joint count and zero hurlde estimation:\ninitial  value 408.872181 \niter  10 value 350.643112\nfinal  value 350.367259 \nconverged\ndone",
    "crumbs": [
      "Hurdle models",
      "hurdle.control"
    ]
  },
  {
    "objectID": "man/hurdle.control.html#control-parameters-for-hurdle-count-data-regression",
    "href": "man/hurdle.control.html#control-parameters-for-hurdle-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of hurdle regression models using hurdle.\n\nhurdle.control(method = \"BFGS\", maxit = 10000, trace = FALSE,\n  separate = TRUE, start = NULL, hessian = TRUE, ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\ntrace\n\n\nlogical or integer controlling whether tracing information on\nthe progress of the optimization should be produced (passed to optim).\n\n\n\n\nseparate\n\n\nlogical. Should the estimation of the parameters in the truncated count component and hurdle zero component be carried out separately? See details.\n\n\n\n\nstart\n\n\nan optional list with elements “count” and “zero” (and potentially “theta”) containing the coefficients for the corresponding component.\n\n\n\n\nhessian\n\n\nlogical. Should the Hessian be computed to derive an estimate of the variance-covariance matrix? If FALSE, the variance-covariance matrix contains only NAs.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in hurdle are estimated by maximum likelihood using optim with control options set in hurdle.control. Most arguments are passed on directly to optim, only trace is also used within hurdle and separate/start control how optim is called.\nStarting values can be supplied via start or estimated by glm.fit (default).\nIf separate = TRUE (default) the likelihoods of the truncated count component and the hurdle zero component will be maximized separately, otherwise the joint likelihood is set up and maximized. In case of separate = FALSE and both dist == “negbin” and zero.dist == “negbin” the theta parameter is restricted to be identical across both negative binomial distributions.\nStandard errors are derived numerically using the Hessian matrix returned by optim. To supply starting values, start should be a list with elements “count” and “zero” and potentially “theta” (a named vector, for models with negative binomial components only) containing the starting values for the coefficients of the corresponding component of the model.\n\nA list with the arguments specified.\n\nhurdle\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nfm1 &lt;- hurdle(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  dist = \"negbin\", zero = \"negbin\")\n\n## user-supplied start values and other options\nfm2 &lt;- hurdle(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  dist = \"negbin\",\n  zero = \"negbin\",\n  trace = TRUE,\n  separate = FALSE,\n  start = list(count = c(0.5, 0, 0),\n           zero = c(-10, 0.5, -0.5),\n           theta = c(count = 1, zero = 1)))\n\nHurdle Count Model\ncount model: negbin with log link\nzero hurdle model: negbin with log link\ndependent variable:\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 \n62 16  9 19 19 15 13  4  6  3  3  1  1  0  1  1 \ncalling optim() for joint count and zero hurlde estimation:\ninitial  value 408.872181 \niter  10 value 350.643112\nfinal  value 350.367259 \nconverged\ndone",
    "crumbs": [
      "Hurdle models",
      "hurdle.control"
    ]
  },
  {
    "objectID": "man/zipois.html",
    "href": "man/zipois.html",
    "title": "countreg",
    "section": "",
    "text": "Score function for the zero-inflated Poisson distribution with parameters lambda (= mean of the uninflated distribution) and inflation probability pi (for structural zeros).\n\n\n\nszipois(x, lambda, pi, parameter = c(\"lambda\", \"pi\"), drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (non-negative integer) quantiles.\n\n\n\n\nlambda\n\n\nvector of non-negative means of the uninflated Poisson distribution.\n\n\n\n\npi\n\n\nvector of zero inflation probabilities for structural zeros.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “size” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe uninflated Poisson distribution has density\n\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\). The zero-inflated density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot I_{\\{0\\}}(x) + (1 - \\pi) \\cdot f(x)\\)\nwhere \\(I\\) is the indicator function (for the point mass at zero).\n\n\n\nszipois gives the score function (= derivative of the log-density with respect to lambda and/or pi).\n\n\n\ndzipois, dpois, zeroinfl",
    "crumbs": [
      "Distribution extensions",
      "Zero-inflated Poisson"
    ]
  },
  {
    "objectID": "man/zipois.html#extension-of-the-zero-inflated-poisson-distribution",
    "href": "man/zipois.html#extension-of-the-zero-inflated-poisson-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function for the zero-inflated Poisson distribution with parameters lambda (= mean of the uninflated distribution) and inflation probability pi (for structural zeros).\n\n\n\nszipois(x, lambda, pi, parameter = c(\"lambda\", \"pi\"), drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (non-negative integer) quantiles.\n\n\n\n\nlambda\n\n\nvector of non-negative means of the uninflated Poisson distribution.\n\n\n\n\npi\n\n\nvector of zero inflation probabilities for structural zeros.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “size” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe uninflated Poisson distribution has density\n\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\). The zero-inflated density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot I_{\\{0\\}}(x) + (1 - \\pi) \\cdot f(x)\\)\nwhere \\(I\\) is the indicator function (for the point mass at zero).\n\n\n\nszipois gives the score function (= derivative of the log-density with respect to lambda and/or pi).\n\n\n\ndzipois, dpois, zeroinfl",
    "crumbs": [
      "Distribution extensions",
      "Zero-inflated Poisson"
    ]
  },
  {
    "objectID": "man/TakeoverBids.html",
    "href": "man/TakeoverBids.html",
    "title": "countreg",
    "section": "",
    "text": "Firms that were targets of takeover bids during the period 1978–1985.\n\ndata(\"TakeoverBids\")\n\nA data frame containing 126 observations on 9 variables.\n\n\nbids\n\n\nNumber of takeover bids (after the initial bid received by the target firm).\n\n\nlegalrest\n\n\nfactor. Equals “yes” if target management responded by lawsuit.\n\n\nrealrest\n\n\nfactor. Equals “yes” if target management proposed changes in asset structure.\n\n\nfinrest\n\n\nfactor. Equals “yes” if target management proposed changes in ownership structure.\n\n\nwhiteknight\n\n\nfactor. Equals “yes” if target management invited friendly third-party bid.\n\n\nbidpremium\n\n\nBid price divided by price 14 working days before bid.\n\n\ninsthold\n\n\nPercentage of stock held by institutions.\n\n\nsize\n\n\nTotal book value of assets (in billions of USD).\n\n\nregulation\n\n\nfactor. Equals “yes” if intervention by federal regulators.\n\n\nThe data were originally used by Jaggia and Thosar (1993), where further details on the variables may be found.\n\nJournal of Applied Econometrics Data Archive for Cameron and Johansson (1997).\nhttp://qed.econ.queensu.ca/jae/1997-v12.3/cameron-johansson/\n\nCameron AC, Johansson P (1997). “Count Data Regression Using Series Expansion: With Applications”, Journal of Applied Econometrics, 12(3), 203–224.\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed.  Cambridge: Cambridge University Press.\nJaggia S, Thosar S (1993). “Multiple Bids as a Consequence of Target Management Resistance: A Count Data Approach”, Review of Quantitative Finance and Accounting, 3, 447–457.\n\n\nlibrary(\"countreg\")\n\ndata(\"TakeoverBids\", package = \"countreg\")\n\n## Poisson model:\n## Jaggia and Thosar (1993), Table 3\n## Cameron and Johansson (1997), Table IV\ntb_p &lt;- glm(bids ~ . + I(size^2), data = TakeoverBids, family = poisson)\nsummary(tb_p)\n\n\nCall:\nglm(formula = bids ~ . + I(size^2), family = poisson, data = TakeoverBids)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)     0.986060   0.533920   1.847  0.06477 . \nlegalrestyes    0.260146   0.150959   1.723  0.08484 . \nrealrestyes    -0.195660   0.192631  -1.016  0.30976   \nfinrestyes      0.074030   0.216522   0.342  0.73242   \nwhiteknightyes  0.481382   0.158870   3.030  0.00245 **\nbidpremium     -0.677696   0.376737  -1.799  0.07204 . \ninsthold       -0.361991   0.424329  -0.853  0.39361   \nsize            0.178503   0.060022   2.974  0.00294 **\nregulationyes  -0.029439   0.160568  -0.183  0.85453   \nI(size^2)      -0.007569   0.003122  -2.425  0.01532 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 121.862  on 125  degrees of freedom\nResidual deviance:  88.615  on 116  degrees of freedom\nAIC: 389.9\n\nNumber of Fisher Scoring iterations: 5\n\nlogLik(tb_p)\n\n'log Lik.' -184.9483 (df=10)\n\n## dispersion tests\n## Cameron and Trivedi (2013, p. 185)\nAER::dispersiontest(tb_p, alternative = \"less\", trafo = 2)\n\n\n    Underdispersion test\n\ndata:  tb_p\nz = -1.1863, p-value = 0.1177\nalternative hypothesis: true alpha is less than 0\nsample estimates:\n      alpha \n-0.06829684 \n\nAER::dispersiontest(tb_p, alternative = \"less\", trafo = 1)\n\n\n    Underdispersion test\n\ndata:  tb_p\nz = -3.0281, p-value = 0.001231\nalternative hypothesis: true alpha is less than 0\nsample estimates:\n     alpha \n-0.3175595 \n\n## visualization of underdispersion\nif(require(\"topmodels\")) {\nrootogram(tb_p)\nqqrplot(tb_p, range = c(0.05, 0.95))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n## Parts of Cameron and Trivedi (2013), Table 5.4\nsummary(residuals(tb_p, type = \"response\"))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.22537 -0.71963 -0.07505  0.00000  0.37182  5.57238 \n\nsummary(residuals(tb_p, type = \"pearson\"))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1.606458 -0.521439 -0.068443  0.001563  0.297046  3.026831 \n\nsummary(residuals(tb_p, type = \"deviance\"))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.27187 -0.55501 -0.06922 -0.08986  0.28720  2.39771 \n\n## hurdle Poisson model mitigates underdispersion\ntb_hp &lt;- hurdle(bids ~ . + I(size^2), data = TakeoverBids, dist = \"poisson\")\nAIC(tb_p, tb_hp)\n\n      df      AIC\ntb_p  10 389.8967\ntb_hp 20 358.9549\n\nif(require(\"topmodels\")) {\nrootogram(tb_hp)\nqqrplot(tb_hp, range = c(0.05, 0.95))\n}",
    "crumbs": [
      "Data sets",
      "TakeoverBids"
    ]
  },
  {
    "objectID": "man/TakeoverBids.html#takeover-bids-data",
    "href": "man/TakeoverBids.html#takeover-bids-data",
    "title": "countreg",
    "section": "",
    "text": "Firms that were targets of takeover bids during the period 1978–1985.\n\ndata(\"TakeoverBids\")\n\nA data frame containing 126 observations on 9 variables.\n\n\nbids\n\n\nNumber of takeover bids (after the initial bid received by the target firm).\n\n\nlegalrest\n\n\nfactor. Equals “yes” if target management responded by lawsuit.\n\n\nrealrest\n\n\nfactor. Equals “yes” if target management proposed changes in asset structure.\n\n\nfinrest\n\n\nfactor. Equals “yes” if target management proposed changes in ownership structure.\n\n\nwhiteknight\n\n\nfactor. Equals “yes” if target management invited friendly third-party bid.\n\n\nbidpremium\n\n\nBid price divided by price 14 working days before bid.\n\n\ninsthold\n\n\nPercentage of stock held by institutions.\n\n\nsize\n\n\nTotal book value of assets (in billions of USD).\n\n\nregulation\n\n\nfactor. Equals “yes” if intervention by federal regulators.\n\n\nThe data were originally used by Jaggia and Thosar (1993), where further details on the variables may be found.\n\nJournal of Applied Econometrics Data Archive for Cameron and Johansson (1997).\nhttp://qed.econ.queensu.ca/jae/1997-v12.3/cameron-johansson/\n\nCameron AC, Johansson P (1997). “Count Data Regression Using Series Expansion: With Applications”, Journal of Applied Econometrics, 12(3), 203–224.\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed.  Cambridge: Cambridge University Press.\nJaggia S, Thosar S (1993). “Multiple Bids as a Consequence of Target Management Resistance: A Count Data Approach”, Review of Quantitative Finance and Accounting, 3, 447–457.\n\n\nlibrary(\"countreg\")\n\ndata(\"TakeoverBids\", package = \"countreg\")\n\n## Poisson model:\n## Jaggia and Thosar (1993), Table 3\n## Cameron and Johansson (1997), Table IV\ntb_p &lt;- glm(bids ~ . + I(size^2), data = TakeoverBids, family = poisson)\nsummary(tb_p)\n\n\nCall:\nglm(formula = bids ~ . + I(size^2), family = poisson, data = TakeoverBids)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)     0.986060   0.533920   1.847  0.06477 . \nlegalrestyes    0.260146   0.150959   1.723  0.08484 . \nrealrestyes    -0.195660   0.192631  -1.016  0.30976   \nfinrestyes      0.074030   0.216522   0.342  0.73242   \nwhiteknightyes  0.481382   0.158870   3.030  0.00245 **\nbidpremium     -0.677696   0.376737  -1.799  0.07204 . \ninsthold       -0.361991   0.424329  -0.853  0.39361   \nsize            0.178503   0.060022   2.974  0.00294 **\nregulationyes  -0.029439   0.160568  -0.183  0.85453   \nI(size^2)      -0.007569   0.003122  -2.425  0.01532 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 121.862  on 125  degrees of freedom\nResidual deviance:  88.615  on 116  degrees of freedom\nAIC: 389.9\n\nNumber of Fisher Scoring iterations: 5\n\nlogLik(tb_p)\n\n'log Lik.' -184.9483 (df=10)\n\n## dispersion tests\n## Cameron and Trivedi (2013, p. 185)\nAER::dispersiontest(tb_p, alternative = \"less\", trafo = 2)\n\n\n    Underdispersion test\n\ndata:  tb_p\nz = -1.1863, p-value = 0.1177\nalternative hypothesis: true alpha is less than 0\nsample estimates:\n      alpha \n-0.06829684 \n\nAER::dispersiontest(tb_p, alternative = \"less\", trafo = 1)\n\n\n    Underdispersion test\n\ndata:  tb_p\nz = -3.0281, p-value = 0.001231\nalternative hypothesis: true alpha is less than 0\nsample estimates:\n     alpha \n-0.3175595 \n\n## visualization of underdispersion\nif(require(\"topmodels\")) {\nrootogram(tb_p)\nqqrplot(tb_p, range = c(0.05, 0.95))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n## Parts of Cameron and Trivedi (2013), Table 5.4\nsummary(residuals(tb_p, type = \"response\"))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-3.22537 -0.71963 -0.07505  0.00000  0.37182  5.57238 \n\nsummary(residuals(tb_p, type = \"pearson\"))\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1.606458 -0.521439 -0.068443  0.001563  0.297046  3.026831 \n\nsummary(residuals(tb_p, type = \"deviance\"))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.27187 -0.55501 -0.06922 -0.08986  0.28720  2.39771 \n\n## hurdle Poisson model mitigates underdispersion\ntb_hp &lt;- hurdle(bids ~ . + I(size^2), data = TakeoverBids, dist = \"poisson\")\nAIC(tb_p, tb_hp)\n\n      df      AIC\ntb_p  10 389.8967\ntb_hp 20 358.9549\n\nif(require(\"topmodels\")) {\nrootogram(tb_hp)\nqqrplot(tb_hp, range = c(0.05, 0.95))\n}",
    "crumbs": [
      "Data sets",
      "TakeoverBids"
    ]
  },
  {
    "objectID": "man/disptest.html",
    "href": "man/disptest.html",
    "title": "countreg",
    "section": "",
    "text": "Tests the null hypothesis of equidispersion in Poisson GLMs against the alternative of overdispersion and/or underdispersion.\n\ndisptest(object, \n  type = c(\"lrtNB2\", \"scoreNB2\", \"scoreNB2adj\", \"scoreNB1\", \"scoreNB1adj\", \"scoreKatz\"), \n  trafo = NULL, alternative = c(\"greater\", \"two.sided\", \"less\"))\n\n\n\n\n\nobject\n\n\na fitted Poisson GLM of class “glm” as fitted by glm with family poisson.\n\n\n\n\ntype\n\n\ntype of test, one of lrtNB2, scoreNB2, scoreNB2adj, scoreNB1, scoreNB1adj, scoreKatz. See details.\n\n\n\n\ntrafo\n\n\na specification of the alternative (see also details), can be numeric or a (positive) function or NULL (the default).\n\n\n\n\nalternative\n\n\na character string specifying the alternative hypothesis: “greater” corresponds to overdispersion, “less” to underdispersion and “two.sided” to either one.\n\n\n\nThe standard Poisson GLM models the (conditional) mean \\(\\mathsf{E}[y] = \\mu\\) which is assumed to be equal to the variance \\(\\mathsf{VAR}[y] = \\mu\\). disptest assesses the hypothesis that this assumption holds (equidispersion) against the alternative that the variance is of the form:\n\n\n\\(\\mathsf{VAR}[y] \\quad = \\quad \\mu \\; + \\; \\alpha \\cdot \\mathrm{trafo}(\\mu).\\)\nOverdispersion corresponds to \\(\\alpha &gt; 0\\) and underdispersion to \\(\\alpha &lt; 0\\). The coefficient \\(\\alpha\\) can be estimated by an auxiliary OLS regression and tested with the corresponding t (or z) statistic which is asymptotically standard normal under the null hypothesis.\nCommon specifications of the transformation function \\(\\mathrm{trafo}\\) are \\(\\mathrm{trafo}(\\mu) = \\mu^2\\) or \\(\\mathrm{trafo}(\\mu) = \\mu\\). The former corresponds to a negative binomial (NB) model with quadratic variance function (called NB2 by Cameron and Trivedi, 2005), the latter to a NB model with linear variance function (called NB1 by Cameron and Trivedi, 2005) or quasi-Poisson model with dispersion parameter, i.e.,\n\n\n\\(\\mathsf{VAR}[y] \\quad = \\quad (1 + \\alpha) \\cdot \\mu = \\mathrm{dispersion} \\cdot \\mu.\\)\nBy default, for trafo = NULL, the latter dispersion formulation is used in dispersiontest. Otherwise, if trafo is specified, the test is formulated in terms of the parameter \\(\\alpha\\). The transformation trafo can either be specified as a function or an integer corresponding to the function function(x) x^trafo, such that trafo = 1 and trafo = 2 yield the linear and quadratic formulations respectively.\nType “lrtNB2” is the LRT comparing the classical Poisson and negative binomial regression models. Note that this test has a non-standard null distribution here, since the negative binomial shape parameter (called theta in glm.nb) is on the boundary of the parameter space under the null hypothesis. Hence the asymptotic distribution of the LRT is that of the arithmetic mean of a point mass at zero and a \\(\\chi^2_1\\) distribution, implying that the \\(p\\)-value is half that of the classical case.\nType “scoreNB2” corresponds to the statistic \\(T_1\\) in Dean and Lawless (1989), type “scoreNB2adj” is their \\(T_a\\). “scoreNB2” also appears in Lee (1986). Type “scoreNB1” corresponds to the statistic \\(P_C\\) in Dean (1992), type “scoreNB1adj” is her \\(P'_C\\). Type “scoreKatz” is the score test against Katz alternatives derived by Lee (1986), these distributions permit overdispersion as well as underdispersion. The score tests against NB1 and NB2 alternatives are also the score tests against Generalized Poisson type 1 and type 2 alternatives (Yang, Hardin, and Addy, 2009).\n\nAn object of class “htest”.\n\nCameron AC, Trivedi PK (1990). “Regression-based Tests for Overdispersion in the Poisson Model”. Journal of Econometrics, 46, 347–364.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed.  Cambridge: Cambridge University Press.\nDean CB (1992). “Testing for Overdispersion in Poisson and Binomial Regression Models”. Journal of the American Statistical Association, 87, 451–457.\nDean C, Lawless JF (1989). “Tests for Detecting Overdispersion in Poisson Regression Models”. Journal of the American Statistical Association, 84, 467–472.\nJaggia S, Thosar S (1993). “Multiple Bids as a Consequence of Target Management Resistance: A Count Data Approach”. Review of Quantitative Finance and Accounting, 3, 447–457.\nLee LF (1986). “Specification Test for Poisson Regression Models”. International Economic Review, 27, 689–706.\nYang Z, Hardin JW, Addy CL (2009). “A Note on Dean’s Overdispersion Test”. Journal of Statistical Planning and Inference, 139 (10), 3675–3678.\n\nglm, poisson, glm.nb\n\n\nlibrary(\"countreg\")\n\n## Data with overdispersion\ndata(\"RecreationDemand\", package = \"AER\")\nrd_p &lt;- glm(trips ~ ., data = RecreationDemand, family = poisson)\n\n## Cameron and Trivedi (2013), p. 248\ndisptest(rd_p, type = \"lrtNB2\", alternative = \"greater\")\n\n\n    Overdispersion test\n\ndata:  rd_p\nz = 1407.7, p-value &lt; 2.2e-16\nalternative hypothesis: greater\n\n## Data with underdispersion\ndata(\"TakeoverBids\", package = \"countreg\")\ntb_p &lt;- glm(bids ~ . + I(size^2), data = TakeoverBids, family = poisson)\n\n## Jaggia and Thosar (1993), Table 3\n## testing overdispersion\ndisptest(tb_p, type = \"scoreNB2\", alternative = \"greater\")\n\n\n    Overdispersion test\n\ndata:  tb_p\nz = -1.0173, p-value = 0.8455\nalternative hypothesis: greater\n\ndisptest(tb_p, type = \"scoreNB2adj\", alternative = \"greater\")\n\n\n    Overdispersion test\n\ndata:  tb_p\nz = -0.29924, p-value = 0.6176\nalternative hypothesis: greater\n\n## testing underdispersion\ndisptest(tb_p, type = \"scoreKatz\", alternative = \"two.sided\")\n\n\n    Dispersion test\n\ndata:  tb_p\nz = -2.5206, p-value = 0.01172\nalternative hypothesis: two.sided",
    "crumbs": [
      "Diagnostic tests",
      "disptest"
    ]
  },
  {
    "objectID": "man/disptest.html#dispersion-tests",
    "href": "man/disptest.html#dispersion-tests",
    "title": "countreg",
    "section": "",
    "text": "Tests the null hypothesis of equidispersion in Poisson GLMs against the alternative of overdispersion and/or underdispersion.\n\ndisptest(object, \n  type = c(\"lrtNB2\", \"scoreNB2\", \"scoreNB2adj\", \"scoreNB1\", \"scoreNB1adj\", \"scoreKatz\"), \n  trafo = NULL, alternative = c(\"greater\", \"two.sided\", \"less\"))\n\n\n\n\n\nobject\n\n\na fitted Poisson GLM of class “glm” as fitted by glm with family poisson.\n\n\n\n\ntype\n\n\ntype of test, one of lrtNB2, scoreNB2, scoreNB2adj, scoreNB1, scoreNB1adj, scoreKatz. See details.\n\n\n\n\ntrafo\n\n\na specification of the alternative (see also details), can be numeric or a (positive) function or NULL (the default).\n\n\n\n\nalternative\n\n\na character string specifying the alternative hypothesis: “greater” corresponds to overdispersion, “less” to underdispersion and “two.sided” to either one.\n\n\n\nThe standard Poisson GLM models the (conditional) mean \\(\\mathsf{E}[y] = \\mu\\) which is assumed to be equal to the variance \\(\\mathsf{VAR}[y] = \\mu\\). disptest assesses the hypothesis that this assumption holds (equidispersion) against the alternative that the variance is of the form:\n\n\n\\(\\mathsf{VAR}[y] \\quad = \\quad \\mu \\; + \\; \\alpha \\cdot \\mathrm{trafo}(\\mu).\\)\nOverdispersion corresponds to \\(\\alpha &gt; 0\\) and underdispersion to \\(\\alpha &lt; 0\\). The coefficient \\(\\alpha\\) can be estimated by an auxiliary OLS regression and tested with the corresponding t (or z) statistic which is asymptotically standard normal under the null hypothesis.\nCommon specifications of the transformation function \\(\\mathrm{trafo}\\) are \\(\\mathrm{trafo}(\\mu) = \\mu^2\\) or \\(\\mathrm{trafo}(\\mu) = \\mu\\). The former corresponds to a negative binomial (NB) model with quadratic variance function (called NB2 by Cameron and Trivedi, 2005), the latter to a NB model with linear variance function (called NB1 by Cameron and Trivedi, 2005) or quasi-Poisson model with dispersion parameter, i.e.,\n\n\n\\(\\mathsf{VAR}[y] \\quad = \\quad (1 + \\alpha) \\cdot \\mu = \\mathrm{dispersion} \\cdot \\mu.\\)\nBy default, for trafo = NULL, the latter dispersion formulation is used in dispersiontest. Otherwise, if trafo is specified, the test is formulated in terms of the parameter \\(\\alpha\\). The transformation trafo can either be specified as a function or an integer corresponding to the function function(x) x^trafo, such that trafo = 1 and trafo = 2 yield the linear and quadratic formulations respectively.\nType “lrtNB2” is the LRT comparing the classical Poisson and negative binomial regression models. Note that this test has a non-standard null distribution here, since the negative binomial shape parameter (called theta in glm.nb) is on the boundary of the parameter space under the null hypothesis. Hence the asymptotic distribution of the LRT is that of the arithmetic mean of a point mass at zero and a \\(\\chi^2_1\\) distribution, implying that the \\(p\\)-value is half that of the classical case.\nType “scoreNB2” corresponds to the statistic \\(T_1\\) in Dean and Lawless (1989), type “scoreNB2adj” is their \\(T_a\\). “scoreNB2” also appears in Lee (1986). Type “scoreNB1” corresponds to the statistic \\(P_C\\) in Dean (1992), type “scoreNB1adj” is her \\(P'_C\\). Type “scoreKatz” is the score test against Katz alternatives derived by Lee (1986), these distributions permit overdispersion as well as underdispersion. The score tests against NB1 and NB2 alternatives are also the score tests against Generalized Poisson type 1 and type 2 alternatives (Yang, Hardin, and Addy, 2009).\n\nAn object of class “htest”.\n\nCameron AC, Trivedi PK (1990). “Regression-based Tests for Overdispersion in the Poisson Model”. Journal of Econometrics, 46, 347–364.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed.  Cambridge: Cambridge University Press.\nDean CB (1992). “Testing for Overdispersion in Poisson and Binomial Regression Models”. Journal of the American Statistical Association, 87, 451–457.\nDean C, Lawless JF (1989). “Tests for Detecting Overdispersion in Poisson Regression Models”. Journal of the American Statistical Association, 84, 467–472.\nJaggia S, Thosar S (1993). “Multiple Bids as a Consequence of Target Management Resistance: A Count Data Approach”. Review of Quantitative Finance and Accounting, 3, 447–457.\nLee LF (1986). “Specification Test for Poisson Regression Models”. International Economic Review, 27, 689–706.\nYang Z, Hardin JW, Addy CL (2009). “A Note on Dean’s Overdispersion Test”. Journal of Statistical Planning and Inference, 139 (10), 3675–3678.\n\nglm, poisson, glm.nb\n\n\nlibrary(\"countreg\")\n\n## Data with overdispersion\ndata(\"RecreationDemand\", package = \"AER\")\nrd_p &lt;- glm(trips ~ ., data = RecreationDemand, family = poisson)\n\n## Cameron and Trivedi (2013), p. 248\ndisptest(rd_p, type = \"lrtNB2\", alternative = \"greater\")\n\n\n    Overdispersion test\n\ndata:  rd_p\nz = 1407.7, p-value &lt; 2.2e-16\nalternative hypothesis: greater\n\n## Data with underdispersion\ndata(\"TakeoverBids\", package = \"countreg\")\ntb_p &lt;- glm(bids ~ . + I(size^2), data = TakeoverBids, family = poisson)\n\n## Jaggia and Thosar (1993), Table 3\n## testing overdispersion\ndisptest(tb_p, type = \"scoreNB2\", alternative = \"greater\")\n\n\n    Overdispersion test\n\ndata:  tb_p\nz = -1.0173, p-value = 0.8455\nalternative hypothesis: greater\n\ndisptest(tb_p, type = \"scoreNB2adj\", alternative = \"greater\")\n\n\n    Overdispersion test\n\ndata:  tb_p\nz = -0.29924, p-value = 0.6176\nalternative hypothesis: greater\n\n## testing underdispersion\ndisptest(tb_p, type = \"scoreKatz\", alternative = \"two.sided\")\n\n\n    Dispersion test\n\ndata:  tb_p\nz = -2.5206, p-value = 0.01172\nalternative hypothesis: two.sided",
    "crumbs": [
      "Diagnostic tests",
      "disptest"
    ]
  },
  {
    "objectID": "man/zinbinom.html",
    "href": "man/zinbinom.html",
    "title": "countreg",
    "section": "",
    "text": "Score function for the zero-inflated negative binomial distribution with parameters mu (= mean of the uninflated distribution), dispersion parameter theta (or equivalently size), and inflation probability pi (for structural zeros).\n\n\n\nszinbinom(x, mu, theta, size, pi, parameter = c(\"mu\", \"theta\", \"pi\"), drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (non-negative integer) quantiles.\n\n\n\n\nmu\n\n\nvector of non-negative means of the uninflated negative binomial distribution.\n\n\n\n\ntheta, size\n\n\nvector of strictly positive dispersion parameters (shape parameter of the gamma mixing distribution). Only one of theta or size must be specified.\n\n\n\n\npi\n\n\nvector of zero inflation probabilities for structural zeros.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “theta” and/or “pi” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe uninflated negative binomial distribution has density\n\nf(x) = \nfor \\(x = 0, 1, 2, \\ldots\\). The zero-inflated density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot I_{\\{0\\}}(x) + (1 - \\pi) \\cdot f(x)\\)\nwhere \\(I\\) is the indicator function (for the point mass at zero).\n\n\n\nszinbinom gives the score function (= derivative of the log-density with respect to mu and/or theta and/or pi).\n\n\n\ndzinbinom, dnbinom, zeroinfl",
    "crumbs": [
      "Distribution extensions",
      "Zero-inflated negative binomial"
    ]
  },
  {
    "objectID": "man/zinbinom.html#extension-of-the-zero-inflated-negative-binomial-distribution",
    "href": "man/zinbinom.html#extension-of-the-zero-inflated-negative-binomial-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function for the zero-inflated negative binomial distribution with parameters mu (= mean of the uninflated distribution), dispersion parameter theta (or equivalently size), and inflation probability pi (for structural zeros).\n\n\n\nszinbinom(x, mu, theta, size, pi, parameter = c(\"mu\", \"theta\", \"pi\"), drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (non-negative integer) quantiles.\n\n\n\n\nmu\n\n\nvector of non-negative means of the uninflated negative binomial distribution.\n\n\n\n\ntheta, size\n\n\nvector of strictly positive dispersion parameters (shape parameter of the gamma mixing distribution). Only one of theta or size must be specified.\n\n\n\n\npi\n\n\nvector of zero inflation probabilities for structural zeros.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “theta” and/or “pi” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe uninflated negative binomial distribution has density\n\nf(x) = \nfor \\(x = 0, 1, 2, \\ldots\\). The zero-inflated density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot I_{\\{0\\}}(x) + (1 - \\pi) \\cdot f(x)\\)\nwhere \\(I\\) is the indicator function (for the point mass at zero).\n\n\n\nszinbinom gives the score function (= derivative of the log-density with respect to mu and/or theta and/or pi).\n\n\n\ndzinbinom, dnbinom, zeroinfl",
    "crumbs": [
      "Distribution extensions",
      "Zero-inflated negative binomial"
    ]
  },
  {
    "objectID": "man/FLXMRnegbin.html",
    "href": "man/FLXMRnegbin.html",
    "title": "countreg",
    "section": "",
    "text": "FlexMix driver for fitting of negative binomial regression models.\n\nFLXMRnegbin(formula = . ~ ., theta = NULL, offset = NULL,\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\n\n\n\n\nformula\n\n\nformula. This is interpreted relative to the formula specified in the call to flexmix using update.formula. Default is to use the original flexmix model formula.\n\n\n\n\ntheta\n\n\nnumeric or NULL. Value of the theta parameter of the negative binomial model. If NULL, theta is estimated along with the regression coefficients.\n\n\n\n\noffset\n\n\nnumeric. Optional offset vector for the linear predictor.\n\n\n\n\ncontrol\n\n\nlist with control parameters passed to optim.\n\n\n\nThe driver function FLXMRnegbin enables estimation of finite mixtures of negative binomial regression models via flexmix or stepFlexmix. The driver is modeled after FLXMRglm and supports both fixed and unknown theta. In the M-step for fixed theta, glm.fit is employed along with the negative.binomial family. If the theta is unknown and has be estimated along with the regression coefficients, direct optimization using optim with analytical gradients is employed.\n\nAn object of class FLXMRglm.\n\nflexmix, stepFlexmix, FLXMRglm, negative.binomial\n\n\nlibrary(\"countreg\")\n\n## artificial data from a two-component mixture of geometric regressions\nset.seed(1)\nd &lt;- data.frame(x = runif(500, -1, 1))\nd$cluster &lt;- rep(1:2, each = 250)\nd$y &lt;- rnbinom(500, mu = exp(c(1, -1)[d$cluster] + c(0, 3)[d$cluster] * d$x), size = 1)\n\nif(require(\"flexmix\")) {\n## fit mixture models with known correct theta and unknown theta\nfm1 &lt;- flexmix(y ~ x, data = d, k = 2, model = FLXMRnegbin(theta = 1))\nfm0 &lt;- flexmix(y ~ x, data = d, k = 2, model = FLXMRnegbin())\n\n## parameter recovery\nparameters(fm1)\nparameters(fm0)\n\n## refit to obtain joint summary\nsummary(refit(fm1, gradient = NULL))\nsummary(refit(fm0, gradient = NULL))\n\n## refitting both components manually for rootograms\nrf1 &lt;- lapply(1:2, function(i)\n  nbreg(y ~ x, data = d, theta = 1, weights = posterior(fm1)[,i]))\nrf0 &lt;- lapply(1:2, function(i)\n  nbreg(y ~ x, data = d, weights = posterior(fm0)[,i]))\n\n## Rootograms\nif(require(\"topmodels\")) {\npar(mfrow = c(1, 2))\n\nr11 &lt;- rootogram(rf1[[1]])\nr12 &lt;- rootogram(rf1[[2]])\n\nr01 &lt;- rootogram(rf0[[1]])\nr02 &lt;- rootogram(rf0[[2]])\n\nrootogram(glm.nb(y ~ x, data = d))\nplot(r01)\nplot(r02)\n}\n}\n\n## two-component mixture model fro NMES1988 physician office visits\n## (fitting takes some time...)\nif(require(\"flexmix\") & require(\"AER\")) {\n\n## data from AER\ndata(\"NMES1988\", package = \"AER\")\nnmes &lt;- NMES1988[, c(1, 7:8, 13, 15, 18:19)] \n\n## single-component model\nnmes_nb &lt;- glm.nb(visits ~ ., data = nmes)\n\n## two-component model\nset.seed(1090)\nnmes_fnb &lt;- stepFlexmix(visits ~ ., data = nmes, k = 2, model = FLXMRnegbin())\n\n## refit to obtain summary with estimate of joint covariance matrix\nsummary(refit(nmes_fnb, gradient = NULL))\n\n## refit individual models manually for rootograms\nnmes_fnb_rf &lt;- lapply(1:2, function(i)\n  nbreg(visits ~ ., data = nmes, weights = posterior(nmes_fnb)[,i]))\n\npar(mfrow = c(1, 3))\nrootogram(nmes_nb, main = \"Negative Binomial\", xlim = c(0, 50), ylim = c(-1, 25))\nrootogram(nmes_fnb_rf[[1]], main = \"Mixture Negative Binomial (Component 1)\",\n  xlim = c(0, 50), ylim = c(-1, 25))\nrootogram(nmes_fnb_rf[[2]], main = \"Mixture Negative Binomial (Component 2)\",\n  xlim = c(0, 50), ylim = c(-1, 25))\n}",
    "crumbs": [
      "Finite mixtures, boosting, GLMs",
      "FLXMRnegbin"
    ]
  },
  {
    "objectID": "man/FLXMRnegbin.html#flexmix-interface-to-negative-binomial-regression-models",
    "href": "man/FLXMRnegbin.html#flexmix-interface-to-negative-binomial-regression-models",
    "title": "countreg",
    "section": "",
    "text": "FlexMix driver for fitting of negative binomial regression models.\n\nFLXMRnegbin(formula = . ~ ., theta = NULL, offset = NULL,\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\n\n\n\n\nformula\n\n\nformula. This is interpreted relative to the formula specified in the call to flexmix using update.formula. Default is to use the original flexmix model formula.\n\n\n\n\ntheta\n\n\nnumeric or NULL. Value of the theta parameter of the negative binomial model. If NULL, theta is estimated along with the regression coefficients.\n\n\n\n\noffset\n\n\nnumeric. Optional offset vector for the linear predictor.\n\n\n\n\ncontrol\n\n\nlist with control parameters passed to optim.\n\n\n\nThe driver function FLXMRnegbin enables estimation of finite mixtures of negative binomial regression models via flexmix or stepFlexmix. The driver is modeled after FLXMRglm and supports both fixed and unknown theta. In the M-step for fixed theta, glm.fit is employed along with the negative.binomial family. If the theta is unknown and has be estimated along with the regression coefficients, direct optimization using optim with analytical gradients is employed.\n\nAn object of class FLXMRglm.\n\nflexmix, stepFlexmix, FLXMRglm, negative.binomial\n\n\nlibrary(\"countreg\")\n\n## artificial data from a two-component mixture of geometric regressions\nset.seed(1)\nd &lt;- data.frame(x = runif(500, -1, 1))\nd$cluster &lt;- rep(1:2, each = 250)\nd$y &lt;- rnbinom(500, mu = exp(c(1, -1)[d$cluster] + c(0, 3)[d$cluster] * d$x), size = 1)\n\nif(require(\"flexmix\")) {\n## fit mixture models with known correct theta and unknown theta\nfm1 &lt;- flexmix(y ~ x, data = d, k = 2, model = FLXMRnegbin(theta = 1))\nfm0 &lt;- flexmix(y ~ x, data = d, k = 2, model = FLXMRnegbin())\n\n## parameter recovery\nparameters(fm1)\nparameters(fm0)\n\n## refit to obtain joint summary\nsummary(refit(fm1, gradient = NULL))\nsummary(refit(fm0, gradient = NULL))\n\n## refitting both components manually for rootograms\nrf1 &lt;- lapply(1:2, function(i)\n  nbreg(y ~ x, data = d, theta = 1, weights = posterior(fm1)[,i]))\nrf0 &lt;- lapply(1:2, function(i)\n  nbreg(y ~ x, data = d, weights = posterior(fm0)[,i]))\n\n## Rootograms\nif(require(\"topmodels\")) {\npar(mfrow = c(1, 2))\n\nr11 &lt;- rootogram(rf1[[1]])\nr12 &lt;- rootogram(rf1[[2]])\n\nr01 &lt;- rootogram(rf0[[1]])\nr02 &lt;- rootogram(rf0[[2]])\n\nrootogram(glm.nb(y ~ x, data = d))\nplot(r01)\nplot(r02)\n}\n}\n\n## two-component mixture model fro NMES1988 physician office visits\n## (fitting takes some time...)\nif(require(\"flexmix\") & require(\"AER\")) {\n\n## data from AER\ndata(\"NMES1988\", package = \"AER\")\nnmes &lt;- NMES1988[, c(1, 7:8, 13, 15, 18:19)] \n\n## single-component model\nnmes_nb &lt;- glm.nb(visits ~ ., data = nmes)\n\n## two-component model\nset.seed(1090)\nnmes_fnb &lt;- stepFlexmix(visits ~ ., data = nmes, k = 2, model = FLXMRnegbin())\n\n## refit to obtain summary with estimate of joint covariance matrix\nsummary(refit(nmes_fnb, gradient = NULL))\n\n## refit individual models manually for rootograms\nnmes_fnb_rf &lt;- lapply(1:2, function(i)\n  nbreg(visits ~ ., data = nmes, weights = posterior(nmes_fnb)[,i]))\n\npar(mfrow = c(1, 3))\nrootogram(nmes_nb, main = \"Negative Binomial\", xlim = c(0, 50), ylim = c(-1, 25))\nrootogram(nmes_fnb_rf[[1]], main = \"Mixture Negative Binomial (Component 1)\",\n  xlim = c(0, 50), ylim = c(-1, 25))\nrootogram(nmes_fnb_rf[[2]], main = \"Mixture Negative Binomial (Component 2)\",\n  xlim = c(0, 50), ylim = c(-1, 25))\n}",
    "crumbs": [
      "Finite mixtures, boosting, GLMs",
      "FLXMRnegbin"
    ]
  },
  {
    "objectID": "man/predict.zerotrunc.html",
    "href": "man/predict.zerotrunc.html",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted zero-truncated count regression model objects of class “zerotrunc”.\n\n## S3 method for class 'zerotrunc'\npredict(object, newdata,\n  type = c(\"response\", \"prob\", \"count\", \"zero\"), na.action = na.pass, ...)\n## S3 method for class 'zerotrunc'\nresiduals(object, type = c(\"deviance\", \"pearson\", \"response\"), ...)\n\n\n\n\n\nobject\n\n\nan object of class “zerotrunc” as returned by zerotrunc.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “zerotrunc”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix (estimated from the Hessian in the numerical optimization of the log-likelihood). As usual, the summary method returns an object of class “summary.zerotrunc” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts), the predicted mean from the count component (without zero truncation) and the predicted probability for observing a non-zero count (in the un-truncated model). The residuals method can compute raw residuals (observed - fitted), Pearson residuals (raw residuals scaled by square root of variance function), and deviance residuals (contributions to the centered log-likelihood).\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nzerotrunc\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm &lt;- zerotrunc(satellites ~ width + color, data = CrabSatellites, subset = satellites &gt; 0)\n\nplot(residuals(fm, type = \"deviance\") ~ fitted(fm))\n\n\n\n\n\n\nplot(residuals(fm, type = \"pearson\") ~ fitted(fm))\n\n\n\n\n\n\ncoef(fm)\n\n(Intercept)       width     color.L     color.Q     color.C \n 0.52666751  0.03971107  0.10777950  0.39487848  0.16936889 \n\nsummary(fm)\n\n\nCall:\nzerotrunc(formula = satellites ~ width + color, data = CrabSatellites, \n    subset = satellites &gt; 0)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.5416 -1.0496 -0.1063  0.6271  3.7814 \n\nCoefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.52667    0.60017   0.878  0.38020   \nwidth        0.03971    0.02228   1.783  0.07466 . \ncolor.L      0.10778    0.14402   0.748  0.45425   \ncolor.Q      0.39488    0.12110   3.261  0.00111 **\ncolor.C      0.16937    0.09365   1.808  0.07054 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNumber of iterations in BFGS optimization: 8 \nLog-likelihood: -261.9 on 5 Df\n\nlogLik(fm)\n\n'log Lik.' -261.9177 (df=5)\n\nAIC(fm)\n\n[1] 533.8354",
    "crumbs": [
      "Zero-truncated models",
      "predict.zerotrunc"
    ]
  },
  {
    "objectID": "man/predict.zerotrunc.html#methods-for-zerotrunc-objects",
    "href": "man/predict.zerotrunc.html#methods-for-zerotrunc-objects",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted zero-truncated count regression model objects of class “zerotrunc”.\n\n## S3 method for class 'zerotrunc'\npredict(object, newdata,\n  type = c(\"response\", \"prob\", \"count\", \"zero\"), na.action = na.pass, ...)\n## S3 method for class 'zerotrunc'\nresiduals(object, type = c(\"deviance\", \"pearson\", \"response\"), ...)\n\n\n\n\n\nobject\n\n\nan object of class “zerotrunc” as returned by zerotrunc.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “zerotrunc”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix (estimated from the Hessian in the numerical optimization of the log-likelihood). As usual, the summary method returns an object of class “summary.zerotrunc” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts), the predicted mean from the count component (without zero truncation) and the predicted probability for observing a non-zero count (in the un-truncated model). The residuals method can compute raw residuals (observed - fitted), Pearson residuals (raw residuals scaled by square root of variance function), and deviance residuals (contributions to the centered log-likelihood).\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nzerotrunc\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm &lt;- zerotrunc(satellites ~ width + color, data = CrabSatellites, subset = satellites &gt; 0)\n\nplot(residuals(fm, type = \"deviance\") ~ fitted(fm))\n\n\n\n\n\n\nplot(residuals(fm, type = \"pearson\") ~ fitted(fm))\n\n\n\n\n\n\ncoef(fm)\n\n(Intercept)       width     color.L     color.Q     color.C \n 0.52666751  0.03971107  0.10777950  0.39487848  0.16936889 \n\nsummary(fm)\n\n\nCall:\nzerotrunc(formula = satellites ~ width + color, data = CrabSatellites, \n    subset = satellites &gt; 0)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.5416 -1.0496 -0.1063  0.6271  3.7814 \n\nCoefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.52667    0.60017   0.878  0.38020   \nwidth        0.03971    0.02228   1.783  0.07466 . \ncolor.L      0.10778    0.14402   0.748  0.45425   \ncolor.Q      0.39488    0.12110   3.261  0.00111 **\ncolor.C      0.16937    0.09365   1.808  0.07054 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNumber of iterations in BFGS optimization: 8 \nLog-likelihood: -261.9 on 5 Df\n\nlogLik(fm)\n\n'log Lik.' -261.9177 (df=5)\n\nAIC(fm)\n\n[1] 533.8354",
    "crumbs": [
      "Zero-truncated models",
      "predict.zerotrunc"
    ]
  },
  {
    "objectID": "man/nbinom.html",
    "href": "man/nbinom.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the negative binomial distribution with parameters mu and size.\n\nsnbinom(x, mu, size, parameter = c(\"mu\", \"size\"), drop = TRUE)\nhnbinom(x, mu, size, parameter = c(\"mu\", \"size\"), drop = TRUE)\nmean_nbinom(mu, size, drop = TRUE)\nvar_nbinom(mu, size, drop = TRUE)\n\n\n\n\n\nx\n\n\nvector of quantiles.\n\n\n\n\nmu\n\n\nmean of distribution.\n\n\n\n\nsize\n\n\ndispersion parameter. Must be strictly positive.\n\n\n\n\nparameter\n\n\ncharacter. Derivatives are computed wrt this paramter.\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\nThe negative binomial with mu and size (or theta) has density\n\n\n f(y | , ) = , y {0, 1, 2, } \nDerivatives of the log-likelihood \\(\\ell\\) wrt \\(\\mu\\):\n\n\n = - \n\n\n = - + \nDerivatives wrt \\(\\theta\\):\n\n\n = _0(y + ) - _0() + () + 1 - (+ ) - \n\n\n = _1(y + ) - _1() + - + \n\\(\\psi_0\\) and \\(\\psi_1\\) denote the digamma and trigamma function, respectively.\nThe derivative wrt \\(\\mu\\) and \\(\\theta\\):\n\n\n = = \n\nsnbinom gives the score function, i.e., the 1st derivative of the log-density wrt mu or theta and hnbinom gives the hessian, i.e., the 2nd derivative of the log-density wrt mu and/or theta. mean and var give the mean and variance, respectively.\n\nNo parameter prob—as in dnbinom, pnbinom, qnbinom and rnbinom—is implemented in the functions snbinom and hnbinom.\n\nNegBinomial encompassing dnbinom, pnbinom, qnbinom and rnbinom.\n\n\nlibrary(\"countreg\")\n\n## Simulate some data\nset.seed(123)\ny &lt;- rnbinom(1000, size = 2, mu = 2)\n\n## Plot log-likelihood function\npar(mfrow = c(1, 3))\nll &lt;- function(x) {sum(dnbinom(y, size = x, mu = 2, log = TRUE))}\ncurve(sapply(x, ll), 1, 4, xlab = expression(theta), ylab = \"\",\n      main = \"Log-likelihood\")\nabline(v = 2, lty = 3)\n\n## Plot score function\ncurve(sapply(x, function(x) sum(snbinom(y, size = x, mu = 2, parameter = \"size\"))),\n      1, 4, xlab = expression(theta), ylab = \"\", main = \"Score\")\nabline(h = 0, lty = 3)\nabline(v = 2, lty = 3)\n\n## Plot hessian\ncurve(sapply(x, function(x) sum(hnbinom(y, size = x, mu = 2, parameter = \"size\"))),\n      1, 4, xlab = expression(theta), ylab = \"\", main = \"Hessian\")\nabline(v = 2, lty = 3)",
    "crumbs": [
      "Distribution extensions",
      "Negative binomial"
    ]
  },
  {
    "objectID": "man/nbinom.html#extension-of-the-negative-binomial-distribution",
    "href": "man/nbinom.html#extension-of-the-negative-binomial-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the negative binomial distribution with parameters mu and size.\n\nsnbinom(x, mu, size, parameter = c(\"mu\", \"size\"), drop = TRUE)\nhnbinom(x, mu, size, parameter = c(\"mu\", \"size\"), drop = TRUE)\nmean_nbinom(mu, size, drop = TRUE)\nvar_nbinom(mu, size, drop = TRUE)\n\n\n\n\n\nx\n\n\nvector of quantiles.\n\n\n\n\nmu\n\n\nmean of distribution.\n\n\n\n\nsize\n\n\ndispersion parameter. Must be strictly positive.\n\n\n\n\nparameter\n\n\ncharacter. Derivatives are computed wrt this paramter.\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\nThe negative binomial with mu and size (or theta) has density\n\n\n f(y | , ) = , y {0, 1, 2, } \nDerivatives of the log-likelihood \\(\\ell\\) wrt \\(\\mu\\):\n\n\n = - \n\n\n = - + \nDerivatives wrt \\(\\theta\\):\n\n\n = _0(y + ) - _0() + () + 1 - (+ ) - \n\n\n = _1(y + ) - _1() + - + \n\\(\\psi_0\\) and \\(\\psi_1\\) denote the digamma and trigamma function, respectively.\nThe derivative wrt \\(\\mu\\) and \\(\\theta\\):\n\n\n = = \n\nsnbinom gives the score function, i.e., the 1st derivative of the log-density wrt mu or theta and hnbinom gives the hessian, i.e., the 2nd derivative of the log-density wrt mu and/or theta. mean and var give the mean and variance, respectively.\n\nNo parameter prob—as in dnbinom, pnbinom, qnbinom and rnbinom—is implemented in the functions snbinom and hnbinom.\n\nNegBinomial encompassing dnbinom, pnbinom, qnbinom and rnbinom.\n\n\nlibrary(\"countreg\")\n\n## Simulate some data\nset.seed(123)\ny &lt;- rnbinom(1000, size = 2, mu = 2)\n\n## Plot log-likelihood function\npar(mfrow = c(1, 3))\nll &lt;- function(x) {sum(dnbinom(y, size = x, mu = 2, log = TRUE))}\ncurve(sapply(x, ll), 1, 4, xlab = expression(theta), ylab = \"\",\n      main = \"Log-likelihood\")\nabline(v = 2, lty = 3)\n\n## Plot score function\ncurve(sapply(x, function(x) sum(snbinom(y, size = x, mu = 2, parameter = \"size\"))),\n      1, 4, xlab = expression(theta), ylab = \"\", main = \"Score\")\nabline(h = 0, lty = 3)\nabline(v = 2, lty = 3)\n\n## Plot hessian\ncurve(sapply(x, function(x) sum(hnbinom(y, size = x, mu = 2, parameter = \"size\"))),\n      1, 4, xlab = expression(theta), ylab = \"\", main = \"Hessian\")\nabline(v = 2, lty = 3)",
    "crumbs": [
      "Distribution extensions",
      "Negative binomial"
    ]
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "countreg 0.3-0",
    "section": "",
    "text": "countreg 0.3-0\n\nMajor revision of the package, refactoring some crucial infrastructure. Rootograms and other visualizations are in topmodels (currently on R-Forge), see https://topmodels.R-Forge.R-project.org/articles/topmodels.html. Distribution functions (d/p/q/r) are in distributions3 (on CRAN), see https://www.zeileis.org/news/user2022/.\nAlso, data sets SerumPotassium and VolcanoHeights are now in topmodels.\nImproved predict() method with more type of predictions, including moments (type = \"mean\" or \"variance\"), \"quantile\", \"density\", cumulative distribution function (\"probability\") etc. By default, the prediction is computed for the \"full\" outcome model (e.g., hurdle or zero-inflation) but can also be just the \"count\" or \"zero\" component or the distribution \"truncated\" at zero.\n\n\n\ncountreg 0.2-1\n\nBug fix in dhpois() and dhnbinom() when x is a vector and pi is a scalar (reported by Andrea Gilardi).\nTry to auto-detect columns in regressor matrices that are aliased, i.e., whose coefficients cannot be estimated. This is based on pivot and rank from qr() and can detect, e.g., constant or linearly dependent columns.\nCatch errors when inverting the Hessian from optim, returning an NA matrix of correct dimension instead.\nConditionally register autoplot.rootogram() method.\n\n\n\ncountreg 0.2-0\n\nNew d/p/q/r/s functions for all combinations of Poisson vs. negative binomial (NB) and zero-truncated vs. zero-inflated vs. hurdle. The parameter names are all greek letters: lambda for Poisson mean, mu for NB mean, theta for NB dispersion, pi for zero-modification probability (either inflation or hurdle crossing). The theta parameter may also be called size for consistency with base R’s nbinom family.\nNew generic function pit() for extracting the probability integral transform F(y) where F is the predicted cumulative density function and y the observed response.\nQuantile residuals are now computed based on the pit() extractor.\nNew graphics function pithist() for PIT histograms based on pit().\nNew data set OralHealthNL accompanying Hofstetter et al. (2016): “Modeling Caries Experience: Advantages of the Use of the Hurdle Model”, Caries Research, 50(6), 517-526. doi:10.1159/000448197\n\n\n\ncountreg 0.1-5\n\nNew d/p/q/r/s functions for the zero-truncated Poisson distribution where the parameter can either be specified in terms of the zero-truncated mean or the untruncated mean lambda.\nNew ztpoisson() family object for the estimation of zero-truncated Poisson regression model via glm().\n\n\n\ncountreg 0.1-4\n\nNew qresiduals() generic for computing (randomized) quantile residuals along with methods for various objects. (This is somewhat more flexible than statmod::qresiduals() which is not generic.)\nA Q-Q plot based on quantile residuals is available in the new function qqrplot().\n\n\n\ncountreg 0.1-3\n\nThe hurdle() function now allows for the restriction of the theta parameter for the negative binomial distribution across the censored zero and truncated count components. The restriction is applied if dist = \"negbin\", zero.dist = \"negbin\", and separate = FALSE.\nBug fix in the computation of the standard error of log(theta) in zeroinfl() when with weighted data. The weights were used for the estimation of log(theta) but the standard error was not scaled accordingly.\n\n\n\ncountreg 0.1-2\n\nThe style and scale are now added as attributes to rootogram objects so that they can be re-used in the c() and + methods.\n\n\n\ncountreg 0.1-1\n\nNew data sets SerumPotassium, TakeoverBids, VolcanoHeights.\nNew function disptest() providing several (score) tests for over/underdispersion.\nNew rootogram() function with a wide range methods for creating rootograms based on various fitted model objects (fitdistr, glm, hurdle, zeroinfl, zerotrunc, …)\nFLXMRnegbin() driver for estimating mixtures of negative binomial models using flexmix.\nMBnegbin(), MBztnegbin(), MBztpoisson() and MBbinomial() families for estimating boosted components of hurdle models using mboost.\n\n\n\ncountreg 0.1-0\n\nPorted zeroinfl() and hurdle() along with corresponding vignette from pscl to countreg.\nAdded zerotrunc() for estimating zero-truncated count regressions."
  },
  {
    "objectID": "vignettes/zero-augmented.html",
    "href": "vignettes/zero-augmented.html",
    "title": "Zero-augmented models",
    "section": "",
    "text": "\\[\n\\newcommand{\\E}{\\mathsf{E}}\n\\newcommand{\\VAR}{\\mathsf{VAR}}\n\\newcommand{\\COV}{\\mathsf{COV}}\n\\newcommand{\\Prob}{\\mathsf{P}}\n\\]"
  },
  {
    "objectID": "vignettes/zero-augmented.html#hurdle-models",
    "href": "vignettes/zero-augmented.html#hurdle-models",
    "title": "Zero-augmented models",
    "section": "\n1 Hurdle models",
    "text": "1 Hurdle models\nIn addition to over-dispersion, many empirical count data sets exhibit more zero observations than would be allowed for by the Poisson model. One model class capable of capturing both properties is the hurdle model, originally proposed by Mullahy (1986) in the econometrics literature (see Cameron and Trivedi 1998; Cameron and Trivedi 2005 for an overview). They are two-component models: A truncated count component, such as Poisson, geometric or negative binomial, is employed for positive counts, and a hurdle component models zero vs. larger counts. For the latter, either a binomial model or a censored count distribution can be employed.\nMore formally, the hurdle model combines a count data model \\(f_\\mathrm{count}(y; x, \\beta)\\) (that is left-truncated at \\(y = 1\\)) and a zero hurdle model \\(f_\\mathrm{zero}(y; z, \\gamma)\\) (right-censored at \\(y = 1\\)): \\[\nf_\\mathrm{hurdle}(y; x, z, \\beta, \\gamma) =\n  \\left\\{\n  \\begin{array}{ll}\n  f_\\mathrm{zero}(0; z, \\gamma) & \\mbox{if } y = 0, \\\\\n  (1 - f_\\mathrm{zero}(0; z, \\gamma)) \\cdot\n  f_\\mathrm{count}(y; x, \\beta)/(1 - f_\\mathrm{count}(0; x, \\beta)) & \\mbox{if } y &gt; 0\n  \\end{array}\n  \\right.\n\\tag{1}\\] The model parameters \\(\\beta\\), \\(\\gamma\\), and potentially one or two additional dispersion parameters \\(\\theta\\) (if \\(f_\\mathrm{count}\\) or \\(f_\\mathrm{zero}\\) or both are negative binomial densities) are estimated by ML, where the specification of the likelihood has the advantage that the count and the hurdle component can be maximized separately. The corresponding mean regression relationship is given by \\[\n\\log(\\mu_i) \\quad = \\quad x_i^\\top \\beta +\n                    \\log(1 - f_\\mathrm{zero}(0; z_i, \\gamma)) -\n            \\log(1 - f_\\mathrm{count}(0; x_i, \\beta)),\n\\tag{2}\\] again using the canonical log link. For interpreting the zero model as a hurdle, a binomial GLM is probably the most intuitive specification1. Another useful interpretation arises if the same regressors \\(x_i = z_i\\) are used in the same count model in both components \\(f_\\mathrm{count} = f_\\mathrm{zero}\\): A test of the hypothesis \\(\\beta = \\gamma\\) then tests whether the hurdle is needed or not.\nIn R, hurdle count data models can be fitted with the hurdle() function from the countreg package (Zeileis and Kleiber 2013). Both its fitting function and the returned model objects of class hurdle are modelled after the corresponding GLM functionality in R. The arguments of hurdle() are given by\nhurdle(formula, data, subset, na.action, weights, offset,\n  dist = \"poisson\", zero.dist = \"binomial\", link = \"logit\",\n  control = hurdle.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\nwhere the first line contains the standard model-frame specifications, the second and third lines have the arguments specific to hurdle models and the arguments in the last line control some components of the return value.\nIf a formula of type y ~ x1 + x2 is supplied, it not only describes the count regression relationship of \\(y_i\\) and \\(x_i\\) but also implies that the same set of regressors is used for the zero hurdle component \\(z_i = x_i\\). This is could be made more explicit by equivalently writing the formula as y ~ x1 + x2 | x1 + x2. Of course, a different set of regressors could be specified for the zero hurdle component, e.g., y ~ x1 + x2 | z1 + z2 + z3, giving the count data model y ~ x1 + x2 conditional on (|) the zero hurdle model y ~ z1 + z2 + z3.\nThe model likelihood can be specified by the dist, zero.dist and link arguments. The count data distribution dist is \"poisson\" by default (it can also be set to \"negbin\" or \"geometric\"), for which the canonical log link is always used. The distribution for the zero hurdle model can be specified via zero.dist. The default is a binomial model with link (defaulting to \"logit\", but all link functions of the binomial() family are also supported), alternatively a right-censored count distribution (Poisson, negative binomial or geometric, all with log link) could be specified.\nML estimation of all parameters employing analytical gradients is carried out using R’s optim() with control options set in hurdle.control(). Starting values can be user-supplied, otherwise they are estimated by glm.fit() (the default). The covariance matrix estimate is derived numerically using the Hessian matrix returned by optim(). See Section 4 for further technical details.\nThe returned fitted-model object of class hurdle is a list similar to glm objects. Some of its elements—such as coefficients or terms—are lists with a zero and count component, respectively. For details see Section 4.\nA set of standard extractor functions for fitted model objects is available for objects of class hurdle, including the usual summary() method that provides partial Wald tests for all coefficients. No anova() method is provided, but the general coeftest(), waldtest() from lmtest, and linearHypothesis() from car can be used for Wald tests and lrtest() from lmtest for LR tests of nested models. The function hurdletest() is a convenience interface to linearHypothesis() for testing for the presence of a hurdle (which is only applicable if the same regressors and the same count distribution are used in both components)."
  },
  {
    "objectID": "vignettes/zero-augmented.html#zero-inflated-models",
    "href": "vignettes/zero-augmented.html#zero-inflated-models",
    "title": "Zero-augmented models",
    "section": "\n2 Zero-inflated models",
    "text": "2 Zero-inflated models\nZero-inflated models (Mullahy 1986; Lambert 1992) are another model class capable of dealing with excess zero counts (see Cameron and Trivedi 1998; 2005 for an overview). They are two-component mixture models combining a point mass at zero with a count distribution such as Poisson, geometric or negative binomial. Thus, there are two sources of zeros: zeros may come from both the point mass and from the count component. For modeling the unobserved state (zero vs. count), a binary model is used: in the simplest case only with an intercept but potentially containing regressors.\nFormally, the zero-inflated density is a mixture of a point mass at zero \\(I_{\\{0\\}}(y)\\) and a count distribution \\(f_\\mathrm{count}(y; x, \\beta)\\). The probability of observing a zero count is inflated with probability \\(\\pi = f_\\mathrm{zero}(0; z, \\gamma)\\): \\[\nf_\\mathrm{zeroinfl}(y; x, z, \\beta, \\gamma) \\quad = \\quad\n  f_\\mathrm{zero}(0; z, \\gamma) \\cdot I_{\\{0\\}}(y) \\; + \\;\n  (1 - f_\\mathrm{zero}(0; z, \\gamma)) \\cdot f_\\mathrm{count}(y; x, \\beta),\n\\tag{3}\\] where \\(I(\\cdot)\\) is the indicator function and the unobserved probability \\(\\pi\\) of belonging to the point mass component is modelled by a binomial GLM \\(\\pi = g^{-1}(z^\\top \\gamma)\\). The corresponding regression equation for the mean is \\[\n\\mu_i \\quad = \\quad \\pi_i \\cdot 0 \\; + \\; (1 - \\pi_i) \\cdot \\exp(x_i^\\top \\beta),\n\\tag{4}\\] using the canonical log link. The vector of regressors in the zero-inflation model \\(z_i\\) and the regressors in the count component \\(x_i\\) need not to be distinct—in the simplest case, \\(z_i = 1\\) is just an intercept. The default link function \\(g(\\pi)\\) in binomial GLMs is the logit link, but other links such as the probit are also available. The full set of parameters of \\(\\beta\\), \\(\\gamma\\), and potentially the dispersion parameter \\(\\theta\\) (if a negative binomial count model is used) can be estimated by ML. Inference is typically performed for \\(\\beta\\) and \\(\\gamma\\), while \\(\\theta\\) is treated as a nuisance parameter even if a negative binomial model is used.\nIn R, zero-inflated count data models can be fitted with the zeroinfl() function from the countreg package. Both the fitting function interface and the returned model objects of class zeroinfl are almost identical to the corresponding hurdle() functionality and again modelled after the corresponding GLM functionality in R. The arguments of zeroinfl() are given by\nzeroinfl(formula, data, subset, na.action, weights, offset,\n  dist = \"poisson\", link = \"logit\", control = zeroinfl.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\nwhere all arguments have almost the same meaning as for hurdle(). The main difference is that there is no zero.dist argument: a binomial model is always used for distribution in the zero-inflation component.\nAgain, ML estimates of all parameters are obtained from optim(), with control options set in zeroinfl.control() and employing analytical gradients. Starting values can be user-supplied, estimated by the expectation maximization (EM) algorithm, or by glm.fit() (the default). The covariance matrix estimate is derived numerically using the Hessian matrix returned by optim(). Using EM estimation for deriving starting values is typically slower but can be numerically more stable. It already maximizes the likelihood, but a single optim() iteration is used for determining the covariance matrix estimate. See Section 5 for further technical details.\nThe returned fitted model object is of class zeroinfl whose structure is virtually identical to that of hurdle models. As above, a set of standard extractor functions for fitted model objects is available for objects of class zeroinfl, including the usual summary() method that provides partial Wald tests for all coefficients. Again, no anova() method is provided, but the general functions coeftest() and waldtest() from lmtest, as well as linearHypothesis() from car can be used for Wald tests, and lrtest() from lmtest for LR tests of nested models."
  },
  {
    "objectID": "vignettes/zero-augmented.html#sec-illustrations",
    "href": "vignettes/zero-augmented.html#sec-illustrations",
    "title": "Zero-augmented models",
    "section": "\n3 Illustrations",
    "text": "3 Illustrations\nIn the following, we illustrate hurdle and zero-inflated models by applying them to the cross-sectional data set used in Deb and Trivedi (1997). It is based on the US National Medical Expenditure Survey (NMES) for 1987/88 and is available from the data archive of the Journal of Applied Econometrics at https://journaldata.zbw.eu/dataset/demand-for-medical-care-by-the-elderly-a-finite-mixture-approach. It was prepared for the R package AER accompanying Kleiber and Zeileis (2008) and is also available as DebTrivedi.rda in the Journal of Statistical Software together with Zeileis (2006). The same data set is used to illustrate some basic count data models in the article “Basics”, which contains a more detailed description and a basic exploratory analysis of the data.\nThe objective is to model the number of physician office visits visits using the health status variables health (self-perceived health status), chronic (number of chronic conditions), as well as the socio-economic variables gender, school (number of years of education), and insurance (private insurance indicator) as regressors. For convenience, we select the variables used from the full data set:\n\ndata(\"NMES1988\", package = \"AER\")\ndt &lt;- NMES1988[, c(1, 7, 8, 13, 15, 18)]\n\nAt the end, we will provide a brief comparison between the zero-adjusted models and the basic count data models illustrated in “Basics”.\n\n3.1 Hurdle regression\nThe exploratory analysis in “Basics” conveyed the impression that there might be more zero observations than explained by the basic count data distributions, hence a negative binomial hurdle model is fitted via\n\nfm_hurdle0 &lt;- hurdle(visits ~ ., data = dt, dist = \"negbin\")\n\nThis uses the same type of count data model as in the preceeding section but it is now truncated for visits &lt; 1 and has an additional hurdle component modeling zero vs. count observations. By default, the hurdle component is a binomial GLM with logit link which contains all regressors used in the count model. The associated coefficient estimates and partial Wald tests for both model components are displayed via\n\nsummary(fm_hurdle0)\n\n\nCall:\nhurdle(formula = visits ~ ., data = dt, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.1362 -0.7069 -0.2790  0.3066 17.1420 \n\nCount model coefficients (truncated negbin with log link):\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.194286   0.060182  19.844  &lt; 2e-16 ***\nhealthpoor       0.382645   0.048855   7.832 4.79e-15 ***\nhealthexcellent -0.368340   0.067493  -5.457 4.83e-08 ***\nchronic          0.147537   0.012654  11.659  &lt; 2e-16 ***\ngendermale      -0.056464   0.033144  -1.704  0.08846 .  \nschool           0.021187   0.004632   4.574 4.79e-06 ***\ninsuranceyes     0.130454   0.043511   2.998  0.00272 ** \nLog(theta)       0.269981   0.043070   6.268 3.65e-10 ***\nZero hurdle model coefficients (binomial with logit link):\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      0.07459    0.13919   0.536   0.5920    \nhealthpoor       0.04995    0.15969   0.313   0.7544    \nhealthexcellent -0.30883    0.14256  -2.166   0.0303 *  \nchronic          0.55842    0.04505  12.395  &lt; 2e-16 ***\ngendermale      -0.40145    0.08737  -4.595 4.33e-06 ***\nschool           0.05821    0.01196   4.867 1.13e-06 ***\ninsuranceyes     0.74670    0.10059   7.424 1.14e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 1.3099\nNumber of iterations in BFGS optimization: 14 \nLog-likelihood: -1.215e+04 on 15 Df\n\n\nThe coefficients in the count component resemble those from the previous models, but the increase in the log-likelihood (see also Table 1) conveys that the model has improved by including the hurdle component. However, it might be possible to omit the health variable from the hurdle model. To test this hypothesis, the reduced model is fitted via\n\nfm_hurdle &lt;- hurdle(visits ~ . | chronic + insurance + school + gender,\n  data = dt, dist = \"negbin\")\n\nand can then be compared to the full model in a Wald test\n\nwaldtest(fm_hurdle0, fm_hurdle)\n\nWald test\n\nModel 1: visits ~ .\nModel 2: visits ~ . | chronic + insurance + school + gender\n  Res.Df Df  Chisq Pr(&gt;Chisq)  \n1   4391                       \n2   4393 -2 4.8785    0.08723 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nor an LR test\n\nlrtest(fm_hurdle0, fm_hurdle)\n\nwhich leads to virtually identical results.\n\n3.2 Zero-inflated regression\nA different way of augmenting the negative binomial count model fm_nbin with additional probability weight for zero counts is a zero-inflated negative binomial (ZINB) regression. The default model is fitted via\n\nfm_zinb0 &lt;- zeroinfl(visits ~ ., data = dt, dist = \"negbin\")\n\nAs for the hurdle model above, all regressors from the count model are also used in the zero-inflation model. Again, we can modify the regressors in the zero-inflation part, e.g., by fitting a second model\n\nfm_zinb &lt;- zeroinfl(visits ~ . | chronic + insurance + school + gender,\n  data = dt, dist = \"negbin\")\n\nthat has the same variables in the zero-inflation part as the hurdle component in fm_hurdle. By omitting the health variable, the fit does not change significantly which can again be brought out by a Wald test\n\nwaldtest(fm_zinb0, fm_zinb)\n\nWald test\n\nModel 1: visits ~ .\nModel 2: visits ~ . | chronic + insurance + school + gender\n  Res.Df Df  Chisq Pr(&gt;Chisq)\n1   4391                     \n2   4393 -2 0.0937     0.9542\n\n\nor an LR test lrtest(fm_zinb0, fm_zinb) that produces virtually identical results. The chosen fitted model can again be inspected via\n\nsummary(fm_zinb)\n\nSee Table 1 for a more concise summary.\n\n\n\nTable 1: Summary of fitted count regression models for NMES data (including the models from “Basics”): coefficient estimates from count model, zero-inflation model (both with standard errors in parantheses) , number of estimated parameters, maximized log-likelihood, AIC, BIC and expected number of zeros (sum of fitted densities evaluated at zero). The observed number of zeros is 683 in 4406 observations.\n\n\n \n\n  \n    \n\ntinytable_xqun60yecvj99h3qv9dg\n\n\n      \n\n \n                ML-Pois\n                Adj-Pois\n                Quasi-Pois\n                NB\n                NB-Hurdle\n                ZINB\n              \n\n\n(Intercept)                \n                  1.035     \n                  1.035     \n                  1.035  \n                  0.940     \n                  1.194     \n                  1.198     \n                \n\n                           \n                  (0.024)   \n                  (0.065)   \n                  (0.063)\n                  (0.055)   \n                  (0.060)   \n                  (0.057)   \n                \n\nhealthpoor                 \n                  0.318     \n                  0.318     \n                  0.318  \n                  0.368     \n                  0.383     \n                  0.349     \n                \n\n                           \n                  (0.017)   \n                  (0.056)   \n                  (0.046)\n                  (0.049)   \n                  (0.049)   \n                  (0.046)   \n                \n\nhealthexcellent            \n                  -0.379    \n                  -0.379    \n                  -0.379 \n                  -0.374    \n                  -0.368    \n                  -0.354    \n                \n\n                           \n                  (0.030)   \n                  (0.078)   \n                  (0.080)\n                  (0.062)   \n                  (0.067)   \n                  (0.061)   \n                \n\nchronic                    \n                  0.169     \n                  0.169     \n                  0.169  \n                  0.196     \n                  0.148     \n                  0.150     \n                \n\n                           \n                  (0.004)   \n                  (0.012)   \n                  (0.012)\n                  (0.012)   \n                  (0.013)   \n                  (0.012)   \n                \n\ngendermale                 \n                  -0.108    \n                  -0.108    \n                  -0.108 \n                  -0.115    \n                  -0.056    \n                  -0.072    \n                \n\n                           \n                  (0.013)   \n                  (0.036)   \n                  (0.034)\n                  (0.032)   \n                  (0.033)   \n                  (0.032)   \n                \n\nschool                     \n                  0.026     \n                  0.026     \n                  0.026  \n                  0.027     \n                  0.021     \n                  0.022     \n                \n\n                           \n                  (0.002)   \n                  (0.005)   \n                  (0.005)\n                  (0.004)   \n                  (0.005)   \n                  (0.004)   \n                \n\ninsuranceyes               \n                  0.216     \n                  0.216     \n                  0.216  \n                  0.250     \n                  0.130     \n                  0.151     \n                \n\n                           \n                  (0.017)   \n                  (0.043)   \n                  (0.045)\n                  (0.040)   \n                  (0.044)   \n                  (0.042)   \n                \n\n(Intercept)           \n                            \n                            \n                         \n                            \n                  0.054     \n                  -0.098    \n                \n\n                           \n                            \n                            \n                         \n                            \n                  (0.137)   \n                  (0.274)   \n                \n\nchronic               \n                            \n                            \n                         \n                            \n                  0.577     \n                  -1.302    \n                \n\n                           \n                            \n                            \n                         \n                            \n                  (0.043)   \n                  (0.191)   \n                \n\ninsuranceyes          \n                            \n                            \n                         \n                            \n                  0.742     \n                  -1.193    \n                \n\n                           \n                            \n                            \n                         \n                            \n                  (0.100)   \n                  (0.228)   \n                \n\nschool                \n                            \n                            \n                         \n                            \n                  0.056     \n                  -0.087    \n                \n\n                           \n                            \n                            \n                         \n                            \n                  (0.012)   \n                  (0.027)   \n                \n\ngendermale            \n                            \n                            \n                         \n                            \n                  -0.406    \n                  0.583     \n                \n\n                           \n                            \n                            \n                         \n                            \n                  (0.087)   \n                  (0.204)   \n                \n\nAIC                        \n                  36597.0   \n                  36597.0   \n                         \n                  24469.9   \n                  24331.1   \n                  24336.9   \n                \n\nBIC                        \n                  36641.7   \n                  36641.7   \n                         \n                  24521.0   \n                  24414.2   \n                  24419.9   \n                \n\nLog.Lik.                   \n                  -18291.494\n                  -18291.494\n                         \n                  -12226.953\n                  -12152.560\n                  -12155.431\n                \n\nno. of parameters          \n                  7         \n                  7         \n                  8      \n                  8         \n                  13        \n                  13        \n                \n\n$\\sum_{i} \\hat{f}_{i}(0)$\n                  46        \n                            \n                         \n                  618       \n                  683       \n                  712       \n                \n\n\n\n\n    \n\n\n\n\n\n\n3.3 Comparison\nHaving fitted hurdle and zero-inflated models to the demand for medical care in the NMES data, it is, of course, of interest to understand what these models have in common and what their differences are, especially in comparison to the basic count data models described in “Basics”. In this section, we show how to compute the components of Table 1 and provide some further comments and interpretations.\nBefore we start the analysis, we refit the models from “Basics”:\n\nfm_pois &lt;- glm(visits ~ ., data = dt, family = poisson)\nfm_qpois &lt;- glm(visits ~ ., data = dt, family = quasipoisson)\nfm_nbin &lt;- glm.nb(visits ~ ., data = dt)\n\nAs a first comparison, it is of natural interest to inspect the estimated regression coefficients in the count data model\n\nfm &lt;- list(\"ML-Pois\" = fm_pois, \"Quasi-Pois\" = fm_qpois, \"NB\" = fm_nbin,\n  \"Hurdle-NB\" = fm_hurdle, \"ZINB\" = fm_zinb)\nsapply(fm, function(x) coef(x)[1:7])\n\nThe result (see Table 1) shows that there are some small differences, especially between the GLMs and the zero-augmented models. However, the zero-augmented models have to be interpreted slightly differently: While the GLMs all have the same mean function (\\(g(\\mu_i) = x_i^\\top \\beta\\), see “Basics”), the zero-augmentation also enters the mean function, see (4) and (2). Nevertheless, the overall impression is that the estimated mean functions are rather similar. Moreover, the associated estimated standard errors are very similar as well (see Table 1):\n\ncbind(\"ML-Pois\" = sqrt(diag(vcov(fm_pois))),\n  \"Adj-Pois\" = sqrt(diag(sandwich(fm_pois))),\n  sapply(fm[-1], function(x) sqrt(diag(vcov(x)))[1:7]))\n\nThe only exception are the model-based standard errors for the Poisson model, when treated as a fully specified model, which is obviously not appropriate for this data set.\nIn summary, the models are not too different with respect to their fitted mean functions. The differences become obvious if not only the mean but the full likelihood is considered:\n\nrbind(logLik = sapply(fm, function(x) round(logLik(x), digits = 0)),\n  Df = sapply(fm, function(x) attr(logLik(x), \"df\")))\n\n       ML-Pois Quasi-Pois     NB Hurdle-NB   ZINB\nlogLik  -18291         NA -12227    -12153 -12155\nDf           7          8      8        13     13\n\n\nThe ML Poisson model is clearly inferior to all other fits. The quasi-Poisson model and the sandwich-adjusted Poisson model are not associated with a fitted likelihood. The negative binomial already improves the fit dramatically but can in turn be improved by the hurdle and zero-inflated models which give almost identical fits. This also reflects that the over-dispersion in the data is captured better by the negative-binomial-based models than the plain Poisson model. Additionally, it is of interest how the zero counts are captured by the various models. Therefore, the observed zero counts are compared to the expected number of zero counts for the likelihood-based models:\n\nround(c(\"Obs\" = sum(dt$visits &lt; 1),\n  \"ML-Pois\" = sum(dpois(0, fitted(fm_pois))),\n  \"NB\" = sum(dnbinom(0, mu = fitted(fm_nbin), size = fm_nbin$theta)),\n  \"NB-Hurdle\" = sum(predict(fm_hurdle, type = \"density\", at = 0)),\n  \"ZINB\" = sum(predict(fm_zinb, type = \"density\", at = 0))))\n\n      Obs   ML-Pois        NB NB-Hurdle      ZINB \n      683        46       618       683       712 \n\n\nThus, the ML Poisson model is again not appropriate whereas the negative-binomial-based models are much better in modeling the zero counts. By construction, the expected number of zero counts in the hurdle model matches the observed number.\nIn summary, the hurdle and zero-inflation models lead to the best results (in terms of likelihood) on this data set. Above, their mean function for the count component was already shown to be very similar, below we take a look at the fitted zero components:\n\nt(sapply(fm[4:5], function(x) round(x$coefficients$zero, digits = 3)))\n\n          (Intercept) chronic insuranceyes school gendermale\nHurdle-NB       0.054   0.577        0.742  0.056     -0.406\nZINB           -0.098  -1.302       -1.193 -0.087      0.583\n\n\nThis shows that the absolute values are rather different—which is not surprising as they pertain to slightly different ways of modeling zero counts—but the signs of the coefficients match, i.e., are just inversed. For the hurdle model, the zero hurdle component describes the probability of observing a positive count whereas, for the ZINB model, the zero-inflation component predicts the probability of observing a zero count from the point mass component. Overall, both models lead to the same qualitative results and very similar model fits. Perhaps the hurdle model is slightly preferable because it has the nicer interpretation: there is one process that controls whether a patient sees a physician or not, and a second process that determines how many office visits are made."
  },
  {
    "objectID": "vignettes/zero-augmented.html#sec-hurdle",
    "href": "vignettes/zero-augmented.html#sec-hurdle",
    "title": "Zero-augmented models",
    "section": "\n4 Technical details for hurdle models",
    "text": "4 Technical details for hurdle models\nThe fitting of hurdle models via ML in hurdle() is controlled by the arguments in the hurdle.control() wrapper function:\nhurdle.control(method = \"BFGS\", maxit = 10000, trace = FALSE,\n  separate = TRUE, start = NULL, ...)\nThis modifies some default arguments passed on to the optimizer optim(), such as method, maxit and trace. The latter is also used within hurdle() and can be set to produce more verbose output concerning the fitting process. The argument separate controls whether the two components of the model are optimized separately (the default) or not. This is possible because there are no mixed sources for the zeros in the data (unlike in zero-inflation models). The argument start controls the choice of starting values for calling optim(), all remaining arguments passed through ... are directly passed on to optim().\nBy default, starting values are estimated by calling glm.fit() for both components of the model separately, once for the counts and once for zero vs. non-zero counts. If starting values are supplied, start needs to be set to a named list with the parameters for the $count and $zero part of the model (and potentially a $theta dispersion parameter if a negative binomial distribution is used).\nThe fitted model object of class hurdle is similar to glm objects and contains sufficient information on all aspects of the fitting process. In particular, the estimated parameters and associated covariances are included as well as the result from the optim() call. Furthermore, the call, formula, terms structure etc. is contained, potentially also the model frame, dependent variable and regressor matrices.\nFollowing glm.nb(), the \\(\\theta\\) parameter of the negative binomial distribution is treated as a nuisance parameter. Thus, the $coefficients component of the fitted model object just contains estimates of \\(\\beta\\) and \\(\\gamma\\) while the estimate of \\(\\theta\\) and its standard deviation (on a log scale) are kept in extra list elements $theta and $SE.logtheta."
  },
  {
    "objectID": "vignettes/zero-augmented.html#sec-zeroinfl",
    "href": "vignettes/zero-augmented.html#sec-zeroinfl",
    "title": "Zero-augmented models",
    "section": "\n5 Technical details for zero-inflated models",
    "text": "5 Technical details for zero-inflated models\nBoth the interface of the zeroinfl() function as well as its fitted model objects are virtually identical to the corresponding hurdle functionality. Hence, we only provide some additional information for those aspects that differ from those discussed above. The details of the ML optimization are again provided by a zeroinfl.control() wrapper:\nzeroinfl.control(method = \"BFGS\", maxit = 10000, trace = FALSE,\n  EM = FALSE, start = NULL, ...)\nThe only new argument here is the argument EM which allows for EM estimation of the starting values. Instead of calling glm.fit() only once for both components of the model, this process can be iterated until convergence of the parameters to the ML estimates. The optimizer is still called subsequently (for a single iteration) to obtain the Hessian matrix from which the estimated covariance matrix can be computed."
  },
  {
    "objectID": "vignettes/zero-augmented.html#sec-methods",
    "href": "vignettes/zero-augmented.html#sec-methods",
    "title": "Zero-augmented models",
    "section": "\n6 Methods for fitted zero-inflated and hurdle models",
    "text": "6 Methods for fitted zero-inflated and hurdle models\nUsers typically should not need to compute on the internal structure of hurdle or zeroinfl objects because a set of standard extractor functions is provided, an overview is given in Table 2. This includes methods to the generic functions print() and summary() which print the estimated coefficients along with further information. The summary() in particular supplies partial Wald tests based on the coefficients and the covariance matrix. As usual, the summary() method returns an object of class summary.hurdle or summary.zeroinfl, respectively, containing the relevant summary statistics which can subsequently be printed using the associated print() method.\nThe methods for coef() and vcov() by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting their model argument, the estimates for a single component can be extracted. Concatenating the parameters by default and providing a matching covariance matrix estimate (that does not contain the covariances of further nuisance parameters) facilitates the application of generic inference functions such as coeftest(), waldtest(), and linearHypothesis(). All of these compute Wald tests for which coefficient estimates and associated covariances is essentially all information required and can therefore be queried in an object-oriented way with the coef() and vcov() methods.\nSimilarly, the terms() and model.matrix() extractors can be used to extract the relevant information for either component of the model. A logLik() method is provided, hence AIC() can be called to compute information criteria and lrtest() for conducting LR tests of nested models.\nThe predict() method computes predicted means (default) or probabilities (i.e., likelihood contributions) for observed or new data. Additionally, the means from the count and zero component, respectively, can be predicted. For the count component, this is the predicted count mean (without hurdle/inflation): \\(\\exp(x_i^\\top \\beta)\\). For the zero component, this is the the ratio of probabilities \\((1 - f_\\mathrm{zero}(0; z_i, \\gamma))/(1 - f_\\mathrm{count}(0; x_i, \\beta))\\) of observing non-zero counts in hurdle models. In zero-inflation models, it is the probability \\(f_\\mathrm{zero}(0; z_i, \\gamma)\\) of observing a zero from the point mass component in zero-inflated models\nPredicted means for the observed data can also be obtained by the fitted() method. Deviations between observed counts \\(y_i\\) and predicted means \\(\\hat \\mu_i\\) can be obtained by the residuals() method returning either raw residuals \\(y_i - \\hat \\mu_i\\) or the Pearson residuals (raw residuals standardized by square root of the variance function) with the latter being the default.\n\n\nTable 2: Functions and methods for objects of class zeroinfl and hurdle. The first ten rows refer to methods, the remaining rows contain generic functions whose default methods work because of the information supplied by the methods above.\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nprint()\nsimple printed display with coefficient estimates\n\n\nsummary()\nstandard regression output (coefficient estimates, standard errors, partial Wald tests); returns an object of class “summary.class” containing the relevant summary statistics (which has a print() method)\n\n\ncoef()\nextract coefficients of model (full or components), a single vector of all coefficients by default\n\n\nvcov()\nassociated covariance matrix (with matching names)\n\n\npredict()\npredictions (means or probabilities) for new data\n\n\nfitted()\nfitted means for observed data\n\n\nresiduals()\nextract residuals (response or Pearson)\n\n\nterms()\nextract terms of model components\n\n\nmodel.matrix()\nextract model matrix of model components\n\n\nlogLik()\nextract fitted log-likelihood\n\n\ncoeftest()\npartial Wald tests of coefficients\n\n\nwaldtest()\nWald tests of nested models\n\n\nlinearHypothesis()\nWald tests of linear hypotheses\n\n\nlrtest()\nlikelihood ratio tests of nested models\n\n\nAIC()\ncompute information criteria (AIC, BIC, …)"
  },
  {
    "objectID": "vignettes/zero-augmented.html#replication-of-textbook-results",
    "href": "vignettes/zero-augmented.html#replication-of-textbook-results",
    "title": "Zero-augmented models",
    "section": "Replication of textbook results",
    "text": "Replication of textbook results\nCameron and Trivedi (1998, 204) use a somewhat extended version of the model employed above. Because not all variables in that extended model are significant, a reduced set of variables was used throughout the main text. Here, however, we use the full model to show that the tools in countreg reproduce the results of Cameron and Trivedi (1998).\nAfter omitting the responses other than visits and setting \"other\" as the reference category for region using\n\ndt2 &lt;- NMES1988[, -(2:6)]\ndt2$region &lt;- relevel(dt2$region, \"other\")\n\nwe fit a model that contains all explanatory variables, both in the count model and the zero hurdle model:\n\nfm_hurdle2 &lt;- hurdle(visits ~ ., data = dt2, dist = \"negbin\")\n\nThe resulting coefficient estimates are virtually identical to those published in Cameron and Trivedi (1998, 204). The associated Wald statistics are also very similar provided that sandwich standard errors are used (which is not stated explicitely in Cameron and Trivedi 1998).\n\ncfz &lt;- coef(fm_hurdle2, model = \"zero\")\ncfc &lt;- coef(fm_hurdle2, model = \"count\")\nse &lt;- sqrt(diag(sandwich(fm_hurdle2)))\nround(cbind(zero = cfz, zero_t = cfz/se[-seq(along = cfc)], \n  count = cfc, count_t = cfc/se[seq(along = cfc)]),\n  digits = 3)[c(3, 2, 4, 5, 7, 6, 8, 9:17, 1),]\n\n                  zero zero_t  count count_t\nhealthexcellent -0.329 -2.310 -0.378  -4.312\nhealthpoor       0.071  0.420  0.333   5.863\nchronic          0.557 10.547  0.143  10.520\nadllimited      -0.188 -1.448  0.129   2.504\nregionmidwest    0.101  0.880 -0.016  -0.344\nregionnortheast  0.129  1.033  0.104   1.974\nregionwest       0.202  1.509  0.123   2.444\nage              0.190  2.348 -0.075  -2.339\nafamyes         -0.327 -2.450  0.002   0.023\ngendermale      -0.464 -4.715  0.004   0.098\nmarriedyes       0.247  2.379 -0.092  -2.110\nschool           0.054  4.109  0.022   3.824\nincome           0.007  0.365 -0.002  -0.380\nemployedyes     -0.012 -0.085  0.030   0.401\ninsuranceyes     0.762  6.501  0.227   4.007\nmedicaidyes      0.554  3.055  0.185   2.777\n(Intercept)     -1.475 -2.283  1.631   6.017\n\nlogLik(fm_hurdle2)\n\n'log Lik.' -12110.49 (df=35)\n\n1/fm_hurdle2$theta\n\n    count \n0.7437966 \n\n\nThere are some small and very few larger deviations in the Wald statistics which are probably explicable by different approximations to the gradient of \\(\\theta\\) (or \\(1/\\theta\\) or \\(\\log(\\theta)\\)) and the usage of different non-linear optimizers (and at least ten years of software development).\nMore replication exercises are performed in the example sections of AER (Zeileis and Kleiber 2008), the software package accompanying Kleiber and Zeileis (2008)."
  },
  {
    "objectID": "vignettes/zero-augmented.html#footnotes",
    "href": "vignettes/zero-augmented.html#footnotes",
    "title": "Zero-augmented models",
    "section": "Footnotes",
    "text": "Footnotes\n\nNote that binomial logit and censored geometric models as the hurdle part both lead to the same likelihood function and thus to the same coefficient estimates (Mullahy 1986).↩︎"
  },
  {
    "objectID": "vignettes/intro.html",
    "href": "vignettes/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Modeling count variables is a common task in economics and the social sciences. The classical Poisson regression model for count data is often of limited use in these disciplines because empirical count data sets typically exhibit over-dispersion and/or an excess number of zeros. The former issue can be addressed by extending the plain Poisson regression model in various directions: e.g., using sandwich covariances or estimating an additional dispersion parameter (in a so-called quasi-Poisson model). Another more formal way is to use a negative binomial (NB) regression. All of these models belong to the family of generalized linear models (GLMs, see Nelder and Wedderburn 1972; McCullagh and Nelder 1989). However, although these models typically can capture over-dispersion rather well, they are in many applications not sufficient for modeling excess zeros. Since Mullahy (1986) and Lambert (1992) there is increased interest, both in the econometrics and statistics literature, in zero-augmented models that address this issue by a second model component capturing zero counts. Hurdle models (Mullahy 1986) combine a left-truncated count component with a right-censored hurdle component. Zero-inflation models (Lambert 1992) take a somewhat different approach: they are mixture models that combine a count component and a point mass at zero. An overview of count data models in econometrics, including hurdle and zero-inflated models, is provided in Cameron and Trivedi (1998),countreg:Cameron+Trivedi:2005.\nIn R (R Development Core Team 2008), GLMs are provided by the model fitting functions glm() (Chambers and Hastie 1992) in the stats package and glm.nb() in the MASS package (Venables and Ripley 2002) along with associated methods for diagnostics and inference. Here, we discuss the implementation of hurdle and zero-inflated models in the functions hurdle() and zeroinfl() in the countreg package (Zeileis and Kleiber 2013), available from the Comprehensive R Archive Network (CRAN) at http://CRAN.R-project.org/package=countreg. The design of both modeling functions as well as the methods operating on the associated fitted model objects follows that of the base R functionality so that the new software integrates easily into the computational toolbox for modeling count data in R.\nThe remainder of this article provides an overview of the supported count data models in countreg. More detailed discussions of the classical count data models and their zero-augmented counterparts can be found in “Basics” and “Zero-augmented models”, respectively. Both links contain applications of the respective models to a microeconomic cross-section data set on the demand for medical care."
  },
  {
    "objectID": "vignettes/intro.html#sec-software",
    "href": "vignettes/intro.html#sec-software",
    "title": "Introduction",
    "section": "Models and software",
    "text": "Models and software\n\n\n\nTable 1: Overview of discussed count regression models. All GLMs use the same log-linear mean function (\\(\\log(\\mu) = x^\\top \\beta\\)) but make different assumptions about the remaining likelihood. The zero-augmented models extend the mean function by modifying (typically, increasing) the likelihood of zero counts.\n\n\n \n\n  \n    \n\ntinytable_4eco4uh63k02oh97utkl\n\n\n      \n\nType\n                Distribution\n                Method\n                Description\n              \n\n\nGLM           \n                  Poisson\n                  ML      \n                  Poisson regression: classical GLM, estimated by maximum likelihood (ML)                                                                                                                    \n                \n\n              \n                         \n                  quasi   \n                  \"quasi-Poisson regression\": same mean function, estimated by quasi-ML (QML) or equivalently generalized estimating equations (GEE), inference adjustment via estimated dispersion parameter\n                \n\n              \n                         \n                  adjusted\n                  \"adjusted Poisson regression\": same mean function, estimated by QML/GEE, inference adjustment via sandwich covariances                                                                     \n                \n\n              \n                  NB     \n                  ML      \n                  NB regression: extended GLM, estimated by ML including additional shape parameter                                                                                                          \n                \n\nzero-augmented\n                  Poisson\n                  ML      \n                  zero-inflated Poisson (ZIP), hurdle Poisson                                                                                                                                                \n                \n\n              \n                  NB     \n                  ML      \n                  zero-inflated NB (ZINB), hurdle NB                                                                                                                                                         \n                \n\n\n\n\n    \n\n\n\n\n\nIn this section, we briefly outline the theory and its implementation in R (R Development Core Team 2008) for some basic count data regression models as well as their zero-augmented extensions (see Table 1 for an overview). The classical Poisson, geometric and negative binomial models are described in a generalized linear model (GLM) framework; they are implemented in R by the glm() function (Chambers and Hastie 1992) in the stats package and the glm.nb() function in the MASS package (Venables and Ripley 2002). The hurdle and zero-inflated extensions of these models are provided by the functions hurdle() and zeroinfl() in package countreg (Zeileis and Kleiber 2013). The original implementation of Jackman (2008) was improved by Kleiber and Zeileis (2008) for countreg to make the fitting functions and the fitted model objects more similar to their glm() and glm.nb() counterparts. The most important features of the new hurdle() and zeroinfl() functions are discussed below while some technical aspects are deferred to the appendix.\nAn alternative implementation of zero-inflated count models is available in the currently orphaned package zicounts (Mwalili 2007). Another extension of zero-inflated Poisson models is available in package ZIGP (Erhardt 2008) which allows dispersion—in addition to mean and zero-inflation level—to depend on regressors. However, the interfaces of both packages are less standard with fewer (or no) standard methods provided. Therefore, re-using generic inference tools is more cumbersome and hence these packages are not discussed here.\nTwo packages that embed zero-inflated models into more general implementations of GLMs and GAMs (generalized additive models) are gamlss (Stasinopoulos and Rigby 2007) and VGAM (Yee 2008). The latter also provides hurdle models (under the name zero-altered models). Both implementations allow specification of only one set of regressors.\nIn addition to zero-augmented models, there are many further extensions to the classical Poisson model which are not discussed here. Some important model classes include finite mixture models—implemented in R in package flexmix (Leisch 2004)—and generalized estimating equations (GEE)—provided in R by package geepack (Halekoh, Højsgaard, and Yan 2006)—and mixed-effects models—available in R in packages lme4 and nlme (see Pinheiro and Bates 2000). Further information about the models and alternative R implementations can be found in the respective references."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "To report bugs please send a simple e-mail to the package maintainer: &lt;Achim.Zeileis at R-project.org&gt;\nFor discussions we also try to follow these channels:\n\nCrossValidated with zero-inflation or negative-binomial tag\nR-help mailing list\n\n\n\n\n\nAchim Zeileis  🌐\nChristian Kleiber  🌐\nThorsten Simon \nKevin Huynh"
  },
  {
    "objectID": "contact.html#reporting-bugs",
    "href": "contact.html#reporting-bugs",
    "title": "Contact",
    "section": "",
    "text": "To report bugs please send a simple e-mail to the package maintainer: &lt;Achim.Zeileis at R-project.org&gt;\nFor discussions we also try to follow these channels:\n\nCrossValidated with zero-inflation or negative-binomial tag\nR-help mailing list"
  },
  {
    "objectID": "contact.html#authors-and-contributors",
    "href": "contact.html#authors-and-contributors",
    "title": "Contact",
    "section": "",
    "text": "Achim Zeileis  🌐\nChristian Kleiber  🌐\nThorsten Simon \nKevin Huynh"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "countreg: Count Data Regression in R",
    "section": "Overview",
    "text": "Overview\nThe core of the countreg package consists of functions for a variety of count data regression models:\n\n\nnbreg(): Negative binomial regression (type 1 and 2), including both constant and varying dispersion parameters which can depend on regressors.\n\nzeroinfl() and hurdle(): Zero-inflated and hurdle models for count data based on Poisson, geometric, negative binomial, and binomial distributions. These allow for excess zeros compared to the unadjusted underlying distributions (or also a deficit of zeros in case of the hurdle model).\n\nzerotrunc(): Zero-truncated count regression based on Poisson, geometric, and negative binomial distributions. These models can be employed for the zero-truncated count part of hurdle models.\n\nFLXMRnegbin(): Finite mixtures of negative binomial regressions via driver for the flexmix package.\n\nMBnegbin(): Model-based boosting of negative binomial regressions via driver for the mboost package.\n\nPreviously available functions for graphical goodness-of-fit assessment (rootograms, PIT histograms, Q-Q plots based on randomized quantile residulas, etc.) are now provided in the topmodels package.\nDistribution functions (d/p/q/r) for the different zero-truncated, zero-inflated, and hurdle count distributions are availabe in the distributions3 package along with corresponding distribution objects."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "countreg: Count Data Regression in R",
    "section": "Installation",
    "text": "Installation\nThe latest development version can be installed from R-universe:\ninstall.packages(\"countreg\", repos = \"https://zeileis.R-universe.dev\")"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "countreg: Count Data Regression in R",
    "section": "License",
    "text": "License\nThe package is available under the General Public License version 3 or version 2"
  },
  {
    "objectID": "index.html#illustration",
    "href": "index.html#illustration",
    "title": "countreg: Count Data Regression in R",
    "section": "Illustration",
    "text": "Illustration\nFor demonstrating some of the available count regression models we analyze the number of male satellites around nesting female horseshoe crabs during mating. Because larger females that are in better condition tend to attract more satellites we consider their carapace width and color as (numeric) regressor variables. Package and data can be loaded and tranformed via:\n\nlibrary(\"countreg\")\n## Loading required package: MASS\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites |&gt;\n  transform(color = as.numeric(color)) |&gt;\n  subset(select = c(\"satellites\", \"width\", \"color\"))\n\nThe following four count models are compared: a Poisson generalized linear model via glm(), a negative binomial regression with nbreg(), a negative binomial hurdle model with hurdle(), and a zero-inflated negative binomial model with zeroinfl().\n\ncs_p    &lt;-      glm(satellites ~ ., data = cs, family = poisson)\ncs_nb   &lt;-    nbreg(satellites ~ ., data = cs)\ncs_hnb  &lt;-   hurdle(satellites ~ ., data = cs, dist = \"negbin\")\ncs_zinb &lt;- zeroinfl(satellites ~ ., data = cs, dist = \"negbin\")\n\nThe glm() function is from base R, all other models are from countreg. All negative binomial models employ the type 2 parameterization. The hurdle and zero-inflated models employ binary models for the zero hurdle and zero-inflated part, respectively.\nFor a quick overview we compare the estimated coefficients from all parts of the models and compute their BIC.\n\ncbind(\n  poisson = coef(cs_p),\n  nb = coef(cs_nb, model = \"mu\"),\n  hnb_count = coef(cs_hnb, model = \"count\"),\n  hnb_zero = coef(cs_hnb, model = \"zero\"),\n  zinb_count = coef(cs_zinb, model = \"count\"),\n  zinb_zero = coef(cs_zinb, model = \"zero\")\n)\n##                poisson         nb   hnb_count    hnb_zero zinb_count  zinb_zero\n## (Intercept) -2.5199832 -3.2409843 0.428566899 -10.0708390 0.29870287 10.4341839\n## width        0.1495725  0.1776543 0.037845165   0.4583097 0.04171687 -0.4890186\n## color       -0.1694036 -0.1815656 0.006928678  -0.5090467 0.01866320  0.6068262\nBIC(cs_p, cs_nb, cs_hnb, cs_zinb)\n##         df      BIC\n## cs_p     3 930.9589\n## cs_nb    4 769.5455\n## cs_hnb   7 736.7985\n## cs_zinb  7 736.8958\n\nThis shows that\n\nThe number of male satellites increase with width and decrease with color of the female carapace.\nThe effect of the regressors mostly distinguishes between having any satellites or not while the expectation in the count part has almost no dependency on the regressors.\nThere is overdispersion in the number of satellites because the negative binomial models fit much better than the simple Poisson model.\nThere is only litte difference between the hurdle and zero-inflated model (except that the sign in the zero part is flipped because one models the probability to exceed zero while the other models the probability of an excess zero).\n\nThe summary of the hurdle negative binomial model provides further details such as the non-significance of the regressors in the count part and the estimate of the overdispersion parameter theta.\n\nsummary(cs_hnb)\n## \n## Call:\n## hurdle(formula = satellites ~ ., data = cs, dist = \"negbin\")\n## \n## Pearson residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.3835 -0.7244 -0.2636  0.5557  3.6080 \n## \n## Count model coefficients (truncated negbin with log link):\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) 0.428567   0.941077   0.455    0.649    \n## width       0.037845   0.032749   1.156    0.248    \n## color       0.006929   0.091078   0.076    0.939    \n## Log(theta)  1.527382   0.352950   4.327 1.51e-05 ***\n## Zero hurdle model coefficients (binomial with logit link):\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -10.0708     2.8065  -3.588 0.000333 ***\n## width         0.4583     0.1040   4.407 1.05e-05 ***\n## color        -0.5090     0.2237  -2.276 0.022862 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n## \n## Theta: count = 4.6061\n## Number of iterations in BFGS optimization: 17 \n## Log-likelihood: -350.4 on 7 Df"
  },
  {
    "objectID": "vignettes/basics.html",
    "href": "vignettes/basics.html",
    "title": "Basics",
    "section": "",
    "text": "\\[\n\\newcommand{\\E}{\\mathsf{E}}\n\\newcommand{\\VAR}{\\mathsf{VAR}}\n\\newcommand{\\COV}{\\mathsf{COV}}\n\\newcommand{\\Prob}{\\mathsf{P}}\n\\]"
  },
  {
    "objectID": "vignettes/basics.html#generalized-linear-models",
    "href": "vignettes/basics.html#generalized-linear-models",
    "title": "Basics",
    "section": "\n1 Generalized linear models",
    "text": "1 Generalized linear models\nThe basic count data regression models can be represented and understood using the GLM framework that emerged in the statistical literature in the early 1970s (Nelder and Wedderburn 1972). In the following, we briefly sketch some important aspects relating to the unifying conceptual properties and their implementation in R—for a detailed theoretical account of GLMs see McCullagh and Nelder (1989).\nGLMs describe the dependence of a scalar variable \\(y_i\\) (\\(i = 1, \\dots, n\\)) on a vector of regressors \\(x_i\\). The conditional distribution of \\(y_i | x_i\\) is a linear exponential family with probability density function \\[\nf(y; \\lambda, \\phi) \\quad = \\quad\n                              \\exp \\left( \\frac{y \\cdot \\lambda - b(\\lambda)}{\\phi} +\n                              c(y, \\phi) \\right),\n\\tag{1}\\] where \\(\\lambda\\) is the canonical parameter that depends on the regressors via a linear predictor and \\(\\phi\\) is a dispersion parameter that is often known. The functions \\(b(\\cdot)\\) and \\(c(\\cdot)\\) are known and determine which member of the family is used, e.g., the normal, binomial or Poisson distribution. Conditional mean and variance of \\(y_i\\) are given by \\(\\E[y_i \\, | \\, x_i] = \\mu_i = b'(\\lambda_i)\\) and \\(\\VAR[y_i \\, | \\, x_i] = \\phi \\cdot b''(\\lambda_i)\\). Thus, up to a scale or dispersion parameter \\(\\phi\\), the distribution of \\(y_i\\) is determined by its mean. Its variance is proportional to \\(V(\\mu) = b''(\\lambda(\\mu))\\), also called variance function.\nThe dependence of the conditional mean \\(\\E[y_i \\, | \\, x_i] = \\mu_i\\) on the regressors \\(x_i\\) is specified via \\[\ng(\\mu_i) \\quad = \\quad x_i^\\top \\beta,\n\\tag{2}\\] where \\(g(\\cdot)\\) is a known link function and \\(\\beta\\) is the vector of regression coefficients which are typically estimated by maximum likelihood (ML) using the iterative weighted least squares (IWLS) algorithm.\nInstead of viewing GLMs as models for the full likelihood (as determined by Equation 1), they can also be regarded as regression models for the mean only (as specified in Equation 2) where the estimating functions used for fitting the model are derived from a particular family. As illustrated in the remainder of this section, the estimating function point of view is particularly useful for relaxing the assumptions imposed by the Poisson likelihood.\nR provides a very flexible implementation of the general GLM framework in the function glm() (Chambers and Hastie 1992) contained in the stats package. Its most important arguments are\nwhere formula plus data is the now standard way of specifying regression relationships in R/S introduced in Chambers and Hastie (1992). The remaining arguments in the first line (subset, na.action, weights, and offset) are also standard for setting up formula-based regression models in R/S. The arguments in the second line control aspects specific to GLMs while the arguments in the last line specify which components are returned in the fitted model object (of class glm which inherits from lm). By default the model frame (model) and the vector \\((y_1, \\dots, y_n)^\\top\\) (y) but not the model matrix (x, containing \\(x_1, \\dots, x_n\\) combined row-wise) are included. The family argument specifies the link \\(g(\\mu)\\) and variance function \\(V(\\mu)\\) of the model, start can be used to set starting values for \\(\\beta\\), and control contains control parameters for the IWLS algorithm. For further arguments to glm() (including alternative specifications of starting values) see ?glm. The high-level glm() interface relies on the function glm.fit() which carries out the actual model fitting (without taking a formula-based input or returning classed output).\nFor glm objects, a set of standard methods (including print(), predict(), logLik() and many others) are provided. Inference can easily be performed using the summary() method for assessing the regression coefficients via partial Wald tests or the anova() method for comparing nested models via an analysis of deviance. These inference functions are complemented by further generic inference functions in contributed packages: e.g., lmtest (Zeileis and Hothorn 2002) provides a coeftest() function that also computes partial Wald tests but allows for specification of alternative (robust) standard errors. Similarly, waldtest() from lmtest and linearHypothesis() from car (Fox 2002) assess nested models via Wald tests (using different specifications for the nested models). Finally, lrtest() from lmtest compares nested models via likelihood ratio (LR) tests based on an interface similar to waldtest() and anova()."
  },
  {
    "objectID": "vignettes/basics.html#poisson-model",
    "href": "vignettes/basics.html#poisson-model",
    "title": "Basics",
    "section": "\n2 Poisson Model",
    "text": "2 Poisson Model\nThe simplest distribution used for modeling count data is the Poisson distribution with probability density function \\[\nf(y; \\mu) \\quad = \\quad \\frac{\\exp(-\\mu) \\cdot \\mu^{y}}{y!},\n\\tag{3}\\] which is of type (1) and thus Poisson regression is a special case of the GLM framework. The canonical link is \\(g(\\mu) = \\log(\\mu)\\) resulting in a log-linear relationship between mean and linear predictor. The variance in the Poisson model is identical to the mean, thus the dispersion is fixed at \\(\\phi = 1\\) and the variance function is \\(V(\\mu) = \\mu\\).\nIn R, this can easily be specified in the glm() call just by setting family = poisson (where the default log link could also be changed in the poisson() call).\nIn practice, the Poisson model is often useful for describing the mean \\(\\mu_i\\) but underestimates the variance in the data, rendering all model-based tests liberal. One way of dealing with this is to use the same estimating functions for the mean, but to base inference on the more robust sandwich covariance matrix estimator. In R, this estimator is provided by the sandwich() function in the sandwich package (Zeileis 2004, 2006)."
  },
  {
    "objectID": "vignettes/basics.html#quasi-poisson-model",
    "href": "vignettes/basics.html#quasi-poisson-model",
    "title": "Basics",
    "section": "\n3 Quasi-Poisson model",
    "text": "3 Quasi-Poisson model\nAnother way of dealing with over-dispersion is to use the mean regression function and the variance function from the Poisson GLM but to leave the dispersion parameter \\(\\phi\\) unrestricted. Thus, \\(\\phi\\) is not assumed to be fixed at \\(1\\) but is estimated from the data. This strategy leads to the same coefficient estimates as the standard Poisson model but inference is adjusted for over-dispersion. Consequently, both models (quasi-Poisson and sandwich-adjusted Poisson) adopt the estimating function view of the Poisson model and do not correspond to models with fully specified likelihoods.\nIn R, the quasi-Poisson model with estimated dispersion parameter can also be fitted with the glm() function, simply setting family = quasipoisson."
  },
  {
    "objectID": "vignettes/basics.html#negative-binomial-models",
    "href": "vignettes/basics.html#negative-binomial-models",
    "title": "Basics",
    "section": "\n4 Negative binomial models",
    "text": "4 Negative binomial models\nA third way of modeling over-dispersed count data is to assume a negative binomial (NB) distribution for \\(y_i | x_i\\) which can arise as a gamma mixture of Poisson distributions. One parameterization of its probability density function is \\[\nf(y; \\mu, \\theta) \\quad = \\quad \\frac{\\Gamma(y + \\theta)}{\\Gamma(\\theta) \\cdot y!} \\cdot\n                            \\frac{\\mu^{y} \\cdot \\theta^\\theta}{(\\mu + \\theta)^{y + \\theta}},\n\\tag{4}\\] with mean \\(\\mu\\) and shape parameter \\(\\theta\\); \\(\\Gamma(\\cdot)\\) is the gamma function. For every fixed \\(\\theta\\), this is of type (1) and thus is another special case of the GLM framework. It also has \\(\\phi = 1\\) but with variance function \\(V(\\mu) = \\mu + \\frac{\\mu^2}{\\theta}\\).\nPackage MASS (Venables and Ripley 2002) provides the family function negative.binomial() that can directly be plugged into glm() provided the argument theta is specified. One application would be the geometric model, the special case where \\(\\theta = 1\\), which can consequently be fitted in R by setting family = negative.binomial(theta = 1) in the glm() call.\nIf \\(\\theta\\) is not known but to be estimated from the data, the negative binomial model is not a special case of the general GLM—however, an ML fit can easily be computed re-using GLM methodology by iterating estimation of \\(\\beta\\) given \\(\\theta\\) and vice versa. This leads to ML estimates for both \\(\\beta\\) and \\(\\theta\\) which can be computed using the function glm.nb() from the package MASS. It returns a model of class negbin inheriting from glm for which appropriate methods to the generic functions described above are again available."
  },
  {
    "objectID": "vignettes/basics.html#sec-illustrations",
    "href": "vignettes/basics.html#sec-illustrations",
    "title": "Basics",
    "section": "\n5 Illustrations",
    "text": "5 Illustrations\nIn the following, we illustrate all models described above by applying them to a cross-sectional data set from health economics. Before the parametric models are fitted, a basic exploratory analysis of the data set is carried out that addresses some problems typically encountered when visualizing count data. At the end of the section, all fitted models are compared highlighting that the modelled mean function is similar but the fitted likelihood is different and thus, the models differ with respect to explaining over-dispersion.\nDeb and Trivedi (1997) analyze data on 4406 individuals, aged 66 and over, who are covered by Medicare, a public insurance program. Originally obtained from the US National Medical Expenditure Survey (NMES) for 1987/88, the data are available from the data archive of the Journal of Applied Econometrics at https://journaldata.zbw.eu/dataset/demand-for-medical-care-by-the-elderly-a-finite-mixture-approach. It was prepared for the R package AER accompanying Kleiber and Zeileis (2008) and is also available as DebTrivedi.rda in the Journal of Statistical Software together with Zeileis (2006). The objective is to model the demand for medical care—as captured by the number of physician/non-physician office and hospital outpatient visits—by the covariates available for the patients. Here, we adopt the number of physician office visits visits as the dependent variable and use the health status variables1 health (self-perceived health status), chronic (number of chronic conditions), as well as the socio-economic variables gender, school (number of years of education), and insurance (private insurance indicator) as regressors. For convenience, we select the variables used from the full data set:\n\ndata(\"NMES1988\", package = \"AER\")\ndt &lt;- NMES1988[, c(1, 7, 8, 13, 15, 18)]\n\nTo obtain a first overview of the dependent variable, we employ a histogram of the observed count frequencies. In R various tools could be used, e.g., hist(dt$visits, breaks = 0:90 - 0.5) for a histogram with rectangles or\n\nplot(table(dt$visits))\n\n(see Figure 1) for a histogram with lines which brings out the extremely large counts somewhat better. The histogram illustrates that the marginal distribution exhibits both substantial variation and a rather large number of zeros.\n\n\n\n\n\n\n\nFigure 1: Frequency distribution for number of physician office visits.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Bivariate explorative displays for number of physician office visits plotted against number of chronic conditions.\n\n\n\n\nA natural second step in the exploratory analysis is to look at pairwise bivariate displays of the dependent variable against each of the regressors bringing out the partial relationships. In R, such bivariate displays can easily be generated with the plot() method for formulas, e.g., via plot(y ~ x). This chooses different types of displays depending on the combination of quantitative and qualitative variables as dependent or regressor variable, respectively. However, count variables are treated as all numerical variables and therefore the command\n\nplot(visits ~ chronic, data = dt)\n\nproduces a simple scatterplot as shown in the left panel of Figure 2. This is clearly not useful as both variables are count variables producing numerous ties in the bivariate distribution and thus obscuring a large number of points in the display. To overcome the problem, it is useful to group the number of chronic conditions into a factor with levels 0',1’, 2', and3 or more’ and produce a boxplot instead of a scatterplot. Furthermore, the picture is much clearer if the dependent variable is log-transformed (just as all count regression models discussed above also use a log link by default). As there are zero counts as well, we use a convenience function clog() providing a continuity-corrected logarithm.\n\nclog &lt;- function(x) log(x + 0.5)\n\nFor transforming a count variable to a factor (for visualization purposes only), we define another convenience function cfac()\n\ncfac &lt;- function(x, breaks = NULL) {\n  if(is.null(breaks)) breaks &lt;- unique(quantile(x, 0:10/10))\n  x &lt;- cut(x, breaks, include.lowest = TRUE, right = FALSE)\n  levels(x) &lt;- paste(breaks[-length(breaks)], ifelse(diff(breaks) &gt; 1,\n    c(paste(\"-\", breaks[-c(1, length(breaks))] - 1, sep = \"\"), \"+\"), \"\"),\n    sep = \"\")\n  return(x)\n}\n\nwhich by default tries to take an educated guess how to choose the breaks between the categories. Clearly, the resulting exploratory display of the transformed variables produced by\n\nplot(clog(visits) ~ cfac(chronic), data = dt)\n\n(shown in the right panel of Figure 2) brings out much better how the number of doctor visits increases with the number of chronic conditions.\n\n\n\n\n\n\n\nFigure 3: Number of physician office visits plotted against regressors used.\n\n\n\n\nAnalogous displays for the number of physician office visits against all regressors can be produced via\n\nplot(clog(visits) ~ health, data = dt, varwidth = TRUE)\nplot(clog(visits) ~ cfac(chronic), data = dt)\nplot(clog(visits) ~ insurance, data = dt, varwidth = TRUE)\nplot(clog(visits) ~ gender, data = dt, varwidth = TRUE)\nplot(cfac(visits, c(0:2, 4, 6, 10, 100)) ~ school, data = dt, breaks = 9)\n\nand are shown (with slightly enhanced labeling) in Figure 3. The last plot uses a different type of display. Here, the dependent count variable is not log-transformed but grouped into a factor and then a spinogram is produced. This also groups the regressor (as in a histogram) and then produces a highlighted mosaic plot. All displays show that the number of doctor visits increases or decreases with the regressors as expected: visits decreases with the general health status but increases with the number of chronic conditions or hospital stays. The median number of visits is also slightly higher for patients with a private insurance and higher level of education. It is slightly lower for male compared to female patients. The overall impression from all displays is that the changes in the mean can only explain a modest amount of variation in the data.\n\n5.1 Poisson regression\nAs a first attempt to capture the relationship between the number of physician office visits and all regressors—described in R by the formula visits ~ .—in a parametric regression model, we fit the basic Poisson regression model\n\nfm_pois &lt;- glm(visits ~ ., data = dt, family = poisson)\n\nand obtain the coefficient estimates along with associated partial Wald tests\n\nsummary(fm_pois)\n\n\nCall:\nglm(formula = visits ~ ., family = poisson, data = dt)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.034542   0.023857  43.364   &lt;2e-16 ***\nhealthpoor       0.318205   0.017479  18.205   &lt;2e-16 ***\nhealthexcellent -0.379045   0.030291 -12.514   &lt;2e-16 ***\nchronic          0.168793   0.004471  37.755   &lt;2e-16 ***\ngendermale      -0.108014   0.012943  -8.346   &lt;2e-16 ***\nschool           0.025754   0.001843  13.972   &lt;2e-16 ***\ninsuranceyes     0.216007   0.016872  12.803   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 26943  on 4405  degrees of freedom\nResidual deviance: 23808  on 4399  degrees of freedom\nAIC: 36597\n\nNumber of Fisher Scoring iterations: 5\n\n\nAll coefficient estimates confirm the results from the exploratory analysis in Figure 3. All coefficients are highly significant with the health variables leading to somewhat larger Wald statistics compared to the socio-economic variables. However, the Wald test results might be too optimistic due to a misspecification of the likelihood. As the exploratory analysis suggested that over-dispersion is present in this data set, we re-compute the Wald tests using sandwich standard errors via\n\ncoeftest(fm_pois, vcov = sandwich)\n\n\nz test of coefficients:\n\n                  Estimate Std. Error z value  Pr(&gt;|z|)    \n(Intercept)      1.0345418  0.0648436 15.9544 &lt; 2.2e-16 ***\nhealthpoor       0.3182048  0.0555787  5.7253 1.032e-08 ***\nhealthexcellent -0.3790454  0.0777311 -4.8764 1.081e-06 ***\nchronic          0.1687932  0.0121860 13.8514 &lt; 2.2e-16 ***\ngendermale      -0.1080145  0.0357357 -3.0226  0.002506 ** \nschool           0.0257542  0.0051316  5.0187 5.202e-07 ***\ninsuranceyes     0.2160070  0.0430293  5.0200 5.167e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAll regressors are still significant but the standard errors seem to be more appropriate. This will also be confirmed by the following models that deal with over-dispersion (and excess zeros) in a more formal way.\n\n5.2 Quasi-Poisson regression\nThe quasi-Poisson model\n\nfm_qpois &lt;- glm(visits ~ ., data = dt, family = quasipoisson)\n\nleads to an estimated dispersion of \\(\\hat \\phi = 6.98\\) which is clearly larger than \\(1\\) confirming that over-dispersion is present in the data.2 The resulting partial Wald tests of the coefficients are rather similar to the results obtained from the Poisson regression with sandwich standard errors, leading to the same conclusions. As before, they can be obtained via\n\nsummary(fm_qpois)\n\nThe output is suppressed here and is presented in tabular form in Table 1.\n\n5.3 Negative binomial regression\nA more formal way to accommodate over-dispersion in a count data regression model is to use a negative binomial model, as in\n\nfm_nbin &lt;- glm.nb(visits ~ ., data = dt)\nsummary(fm_nbin)\n\nAs shown in Table 1, both regression coefficients and standard errors are rather similar to the quasi-Poisson and the sandwich-adjusted Poisson results above. Thus, in terms of predicted means all three models give very similar results; the associated partial Wald tests also lead to the same conclusions.\nOne advantage of the negative binomial model is that it is associated with a formal likelihood so that information criteria are readily available. Furthermore, the expected number of zeros can be computed from the fitted densities via \\(\\sum_i f(0, \\hat \\mu_i, \\hat \\theta)\\).\n\n\n\nTable 1: Summary of fitted count regression models for NMES data: coefficient estimates from count model (with standard errors in parantheses) , number of estimated parameters, maximized log-likelihood, AIC, BIC and expected number of zeros (sum of fitted densities evaluated at zero). The observed number of zeros is 683 in 4406 observations.\n\n\n \n\n  \n    \n\ntinytable_yzy11g5gfiq9t9xlagl3\n\n\n      \n\n \n                ML-Pois\n                Adj-Pois\n                Quasi-Pois\n                NB\n              \n\n\n(Intercept)                \n                  1.035     \n                  1.035     \n                  1.035  \n                  0.940     \n                \n\n                           \n                  (0.024)   \n                  (0.065)   \n                  (0.063)\n                  (0.055)   \n                \n\nhealthpoor                 \n                  0.318     \n                  0.318     \n                  0.318  \n                  0.368     \n                \n\n                           \n                  (0.017)   \n                  (0.056)   \n                  (0.046)\n                  (0.049)   \n                \n\nhealthexcellent            \n                  -0.379    \n                  -0.379    \n                  -0.379 \n                  -0.374    \n                \n\n                           \n                  (0.030)   \n                  (0.078)   \n                  (0.080)\n                  (0.062)   \n                \n\nchronic                    \n                  0.169     \n                  0.169     \n                  0.169  \n                  0.196     \n                \n\n                           \n                  (0.004)   \n                  (0.012)   \n                  (0.012)\n                  (0.012)   \n                \n\ngendermale                 \n                  -0.108    \n                  -0.108    \n                  -0.108 \n                  -0.115    \n                \n\n                           \n                  (0.013)   \n                  (0.036)   \n                  (0.034)\n                  (0.032)   \n                \n\nschool                     \n                  0.026     \n                  0.026     \n                  0.026  \n                  0.027     \n                \n\n                           \n                  (0.002)   \n                  (0.005)   \n                  (0.005)\n                  (0.004)   \n                \n\ninsuranceyes               \n                  0.216     \n                  0.216     \n                  0.216  \n                  0.250     \n                \n\n                           \n                  (0.017)   \n                  (0.043)   \n                  (0.045)\n                  (0.040)   \n                \n\nAIC                        \n                  36597.0   \n                  36597.0   \n                         \n                  24469.9   \n                \n\nBIC                        \n                  36641.7   \n                  36641.7   \n                         \n                  24521.0   \n                \n\nLog.Lik.                   \n                  -18291.494\n                  -18291.494\n                         \n                  -12226.953\n                \n\nno. of parameters          \n                  7         \n                  7         \n                  8      \n                  8         \n                \n\n$\\sum_{i} \\hat{f}_{i}(0)$\n                  46        \n                            \n                         \n                  618"
  },
  {
    "objectID": "vignettes/basics.html#footnotes",
    "href": "vignettes/basics.html#footnotes",
    "title": "Basics",
    "section": "Footnotes",
    "text": "Footnotes\n\nIn addition to the variables considered here, Zeileis, Kleiber, and Jackman (2008) also employ hospital, the number of hospital days. As this is more appropriately used as a dependent variable for medical care rather than a regressor, it is omitted from the analysis here.↩︎\nAlternatively, over-dispersion can be confirmed by comparison of the log-likelihoods of the Poisson and negative binomial model.↩︎"
  },
  {
    "objectID": "vignettes/countreg-dists.html",
    "href": "vignettes/countreg-dists.html",
    "title": "Countreg: Distributions",
    "section": "",
    "text": "This document gives an overview of the parametric count data distributions implemented within countreg."
  },
  {
    "objectID": "vignettes/countreg-dists.html#xpois",
    "href": "vignettes/countreg-dists.html#xpois",
    "title": "Countreg: Distributions",
    "section": "Poisson (\"xpois\")",
    "text": "Poisson (\"xpois\")\nThe Poisson distribution has the density \\[\\begin{equation}\n  f_{Pois}(y\\,|\\,\\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\quad \\text{with} \\quad y = 0, 1, 2, \\ldots,\n\\end{equation}\\] with expected value \\(\\mathsf{E}(\\mathrm{Y}) = \\lambda\\) and variance \\(\\mathsf{VAR}(\\mathrm{Y}) = \\lambda\\).\nThe score function is \\[\\begin{equation}\n  s(\\lambda\\,|\\,y) = \\frac{y}{\\lambda} - 1.\n\\end{equation}\\] The hessian is \\[\\begin{equation}\n  h(\\lambda\\,|\\,y) = - \\frac{y}{\\lambda^2}.\n\\end{equation}\\]"
  },
  {
    "objectID": "vignettes/countreg-dists.html#binomial-xbinom",
    "href": "vignettes/countreg-dists.html#binomial-xbinom",
    "title": "Countreg: Distributions",
    "section": "Binomial (\"xbinom\")",
    "text": "Binomial (\"xbinom\")\nThe binomial distribution with size \\(= n\\) and prob \\(= \\pi\\) has the density \\[\\begin{equation}\n  f_{Binom}(y\\,|\\,\\pi,n) = {n \\choose y} {\\pi}^{y} {(1-\\pi)}^{n-y}, \\quad \\text{for} \\quad y = 0, \\ldots, n,\n\\end{equation}\\] with expected value \\(\\mathsf{E}(\\mathrm{Y}) = n \\cdot \\pi\\) and variance \\(\\mathsf{VAR}(\\mathrm{Y}) = n \\cdot \\pi \\cdot (1 - \\pi)\\).\nThe score function is \\[\\begin{equation}\n  s(\\pi\\,|\\,y,n) = \\frac{y}{\\pi} - \\frac{n-y}{1-\\pi}\n\\end{equation}\\]\nThe hessian is \\[\\begin{equation}\n  h(\\pi\\,|\\,y,n) = - \\frac{y}{\\pi^2} - \\frac{n-y}{(1-\\pi)^2}\n\\end{equation}\\]"
  },
  {
    "objectID": "vignettes/countreg-dists.html#xnbinom",
    "href": "vignettes/countreg-dists.html#xnbinom",
    "title": "Countreg: Distributions",
    "section": "Negative Binomial (\"xnbinom\")",
    "text": "Negative Binomial (\"xnbinom\")\nThe negative binomial (type 2) has the density, \\[\\begin{equation}\nf_{NB}(y\\,|\\,\\mu,\\theta) = \\frac{\\Gamma(\\theta + y)}{\\Gamma({\\theta}) \\cdot y!} \\cdot\n\\frac{\\mu^y \\cdot \\theta^\\theta}{(\\mu + \\theta)^{\\theta + y}}, \\quad y = 0, 1, 2, \\dots\n\\end{equation}\\] with expected value \\(\\mathsf{E}(\\mathrm{Y}) = \\mu\\) and variance \\(\\mathsf{VAR}(\\mathrm{Y}) = \\mu + \\mu^2 / \\theta\\).\nThe score functions are: \\[\\begin{equation}\ns_{\\mu}(\\mu,\\theta\\,|\\,y) = \\frac{y}{\\mu} - \\frac{y + \\theta}{\\mu + \\theta}\n\\end{equation}\\] \\[\\begin{equation}\ns_{\\theta}(\\mu,\\theta\\,|\\,y) = \\psi_0(y + \\theta) - \\psi_0(\\theta)\n+ \\log(\\theta) + 1 - \\log(\\mu + \\theta) - \\frac{y + \\theta}{\\mu + \\theta}\n\\end{equation}\\] where \\(\\psi_0\\) is the digamma function.\nThe elements of the hessian are \\[\\begin{equation}\nh_{\\mu\\mu}(\\mu,\\theta\\,|\\,y) = - \\frac{y}{\\mu^2}\n+ \\frac{y + \\theta}{(\\mu + \\theta)^2}\n\\end{equation}\\] \\[\\begin{equation}\nh_{\\theta\\theta}(\\mu,\\theta\\,|\\,y) = \\psi_1(y + \\theta) - \\psi_1(\\theta)\n+ \\frac{1}{\\theta} - \\frac{2}{\\mu + \\theta} + \\frac{y + \\theta}{(\\mu + \\theta)^2}\n\\end{equation}\\] where \\(\\psi_1\\) is the trigamma function. \\[\\begin{equation}\nh_{\\mu\\theta}(\\mu,\\theta\\,|\\,y) = \\frac{y - \\mu}{(\\mu + \\theta)^2}.\n\\end{equation}\\]"
  },
  {
    "objectID": "vignettes/countreg-dists.html#xztpois",
    "href": "vignettes/countreg-dists.html#xztpois",
    "title": "Countreg: Distributions",
    "section": "Zero-Truncated Poisson (\"xztpois\")",
    "text": "Zero-Truncated Poisson (\"xztpois\")\nThe zero truncated Poisson has the density \\[\\begin{equation}\n  f_{ZTP}(x\\,|\\,\\lambda) = \\frac{f_{Pois}(x\\,|\\,\\lambda)}{1 - f_{Pois}(0\\,|\\,\\lambda)} \\quad \\text{for} \\quad x = 1, 2, \\ldots\n\\end{equation}\\] where \\(f_{Pois}\\) is the density of the Poisson distribution. The zero-truncated distribution has expectation \\(\\mathsf{E}(x) = \\mu = \\lambda / (1 - \\exp(-\\lambda))\\) and variance \\(\\mathsf{VAR}(x) = \\mu \\cdot (\\lambda + 1 - \\mu)\\), where \\(\\lambda\\) is the expectation of the untruncated Poisson distribution. Within countreg both parameterizations, either with \\(\\lambda\\) (\"lambda\") or \\(\\mu\\) (\"mean\"), are implemented. Thus, the score functions can be calculated either wrt \\(\\lambda\\) (\"lambda\") or \\(\\mu\\) (\"mean\"): \\[\\begin{equation}\n  s_{\\lambda}(\\lambda\\,|\\,x) = \\frac{x}{\\lambda} - 1 - \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}}\n\\end{equation}\\] \\[\\begin{equation}\n  s_{\\mu}(\\lambda\\,|\\,x) = s_{\\lambda} \\cdot \\frac{\\lambda}{\\mu \\cdot (\\lambda + 1 - \\mu)}\n\\end{equation}\\] The hessian is \\[\\begin{equation}\n  h_{\\lambda\\lambda}(\\lambda\\,|\\,x) = - \\frac{x}{\\lambda^2} + \\frac{e^{-\\lambda}}{(1 - e^{-\\lambda})^2}.\n\\end{equation}\\]"
  },
  {
    "objectID": "vignettes/countreg-dists.html#xztnbinom",
    "href": "vignettes/countreg-dists.html#xztnbinom",
    "title": "Countreg: Distributions",
    "section": "Zero-Truncated Negative Binomial (\"xztnbinom\")",
    "text": "Zero-Truncated Negative Binomial (\"xztnbinom\")\nThe zero-truncated negative binomial has density \\[\\begin{equation}\nf_{ZTNB}(y\\,|\\,\\mu,\\theta) = \\frac{f_{NB}(y\\,|\\,\\mu, \\theta)}{1 - f_{NB}(0\\,|\\,\\mu, \\theta)}, \\quad y = 1, 2, \\dots\n\\end{equation}\\] with expectation \\[\\begin{equation}\n\\mathsf{E}(\\mathrm{Y}) = \\frac{\\mu}{1 - f_{NB}(0\\,|\\,\\mu,\\theta)}\n= \\frac{\\mu}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta},\n\\end{equation}\\] where \\(f_{NB}\\) is the density of the negative binomial distribution, and variance \\[\\begin{align}\n\\mathsf{VAR}(\\mathrm{Y}) & = \\frac{\\mu}{1 - f_{NB}(0\\,|\\,\\mu,\\theta)} \\cdot\n\\left( 1 + \\frac{\\mu}{\\theta} + \\mu - \\frac{\\mu}{1 - f_{NB}(0\\,|\\,\\mu,\\theta)} \\right) \\\\\n& = \\frac{\\mu}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\cdot\n\\left( 1 + \\frac{\\mu}{\\theta} + \\mu - \\frac{\\mu}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\right)\n\\end{align}\\]\nThe score functions are: \\[\\begin{equation}\n  s_{\\mu}(\\mu,\\theta\\,|\\,y) =\n  \\frac{y}{\\mu} - \\frac{y + \\theta}{\\mu + \\theta} -\n  \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^{\\theta+1}}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}\n\\end{equation}\\]\n\\[\\begin{equation}\n  s_{\\theta}(\\mu,\\theta\\,|\\,y) =\n  \\psi_0(y + \\theta) - \\psi_0(\\theta) + \\log(\\theta) + 1 - \\log(\\mu + \\theta) -\n  \\frac{y + \\theta}{\\mu + \\theta} +\n  \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\cdot\n  \\left( \\log\\frac{\\theta}{\\mu + \\theta} + 1 - \\frac{\\theta}{\\mu + \\theta} \\right)\n\\end{equation}\\]\nThe elements of the hessian are: \\[\\begin{align}\nh_{\\mu\\mu}(\\mu,\\theta\\,|\\,y) & =\n\\frac{\\partial^2 \\ell}{\\partial \\mu^2}\n+ \\frac{f(0)}{1 - f(0)} \\cdot \\frac{\\theta \\cdot (1 + \\theta)}{(\\mu + \\theta)^2}\n+ \\left( \\frac{f(0)}{1 - f(0)} \\cdot \\frac{\\theta}{\\mu + \\theta} \\right)^2 \\\\\n& = - \\frac{y}{\\mu^2} + \\frac{y + \\theta}{(\\mu + \\theta)^2}\n+ \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^{\\theta+1}}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}\n     \\cdot \\frac{1 + \\theta}{\\mu + \\theta}\n+ \\left( \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^{\\theta+1}}{ 1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\right)^2,\n\\end{align}\\] \\[\\begin{align}\nh_{\\theta\\theta}(\\mu,\\theta\\,|\\,y)  =\n   \\psi_1(y + \\theta) - \\psi_1(\\theta)\n   + \\frac{1}{\\theta} - \\frac{2}{\\mu + \\theta} + \\frac{y + \\theta}{(\\mu + \\theta)^2}\n+  \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}  \\cdot\n\\left\\{\n\\frac{1}{\\theta} - \\frac{2}{\\mu + \\theta} + \\frac{\\theta}{(\\mu + \\theta)^2} +\n\\left( \\log\\frac{\\theta}{\\mu + \\theta} + 1 - \\frac{\\theta}{\\mu + \\theta} \\right)^2\n\\right\\}\n+ \\left( \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\cdot \\left( \\log\\frac{\\theta}{\\mu + \\theta} +\n1 - \\frac{\\theta}{\\mu + \\theta} \\right)  \\right)^2,\n\\end{align}\\] and \\[\\begin{align}\nh_{\\mu\\theta}(\\mu,\\theta\\,|\\,y) = \\frac{y - \\mu}{(\\mu + \\theta)^2}\n- \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\cdot\n\\left(\n  \\frac{\\theta}{\\mu+\\theta}\\log\\frac{\\theta}{\\mu+\\theta}\n  + \\frac{\\mu\\cdot(\\theta+1)}{(\\mu+\\theta)^2}\n\\right)\n&- \\left( \\frac{\\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta}{1 - \\left( \\frac{\\theta}{\\mu + \\theta} \\right)^\\theta} \\right)^2 \\cdot\n\\left(\n  \\frac{\\theta}{\\mu+\\theta}\\log\\frac{\\theta}{\\mu+\\theta}\n  + \\frac{\\mu\\cdot\\theta}{(\\mu+\\theta)^2}\n\\right)\n\\end{align}\\]"
  },
  {
    "objectID": "vignettes/countreg-dists.html#hurdle-poisson-xhpois",
    "href": "vignettes/countreg-dists.html#hurdle-poisson-xhpois",
    "title": "Countreg: Distributions",
    "section": "Hurdle Poisson (\"xhpois\")",
    "text": "Hurdle Poisson (\"xhpois\")\nThe hurdle poisson has density \\[\\begin{equation}\nf(y\\,|\\,\\pi,\\lambda) =\n  \\begin{cases}\n    1-\\pi & y = 0 \\\\\n    \\pi \\cdot \\frac{f_{Pois}(y\\,|\\,\\lambda)}{1 - f_{Pois}(0\\,|\\,\\lambda)} & y &gt; 0\n  \\end{cases}\n\\end{equation}\\] with expectation \\[\\begin{equation}\n\\mathsf{E}(\\pi,\\lambda) = \\frac{\\pi}{1-e^{-\\lambda}} \\cdot \\lambda\n\\end{equation}\\] and variance \\[\\begin{equation}\n\\mathsf{VAR}(\\pi,\\lambda) = \\frac{\\pi\\cdot\\lambda}{1-e^{-\\lambda}} \\cdot\n\\left(\\lambda + 1 - \\frac{\\pi\\cdot\\lambda}{1-e^{-\\lambda}}\\right).\n\\end{equation}\\] The score functions are \\[\\begin{equation}\ns_\\pi(\\pi, \\lambda\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot \\frac{1}{\\pi} -\n  \\mathbf{I}_{\\{0\\}}(y) \\cdot \\frac{1}{1 - \\pi}\n\\end{equation}\\] and \\[\\begin{equation}\ns_\\lambda(\\pi, \\lambda\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot \\left(\n  \\frac{x}{\\lambda} - 1 - \\frac{e^{-\\lambda}}{1 - e^{-\\lambda}} \\right)\n\\end{equation}\\] where \\(\\mathbf{I}_{\\{0\\}}(y)\\) is an indicator function which takes the value one if \\(y\\) equals zero, and zero otherwise.\nThe elements of the Hessian are, \\[\\begin{equation}\nh_{\\pi\\pi}(\\pi, \\lambda \\,|\\, y) = (\\mathbf{I}_{\\{0\\}}(y) - 1) \\cdot \\frac{1}{\\pi^2} -\n  \\mathbf{I}_{\\{0\\}}(y) \\cdot \\frac{1}{(1 - \\pi)^2},\n\\end{equation}\\] \\[\\begin{equation}\nh_{\\lambda\\lambda}(\\pi, \\lambda \\,|\\, y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot \\left(\n  - \\frac{x}{\\lambda^2} + \\frac{e^{-\\lambda}}{(1 - e^{-\\lambda})^2} \\right)\n\\end{equation}\\] and \\[\\begin{equation}\nh_{\\pi\\lambda}(\\pi, \\lambda \\,|\\, y) = 0.\n\\end{equation}\\]"
  },
  {
    "objectID": "vignettes/countreg-dists.html#hurdle-negative-binomial-xhnbinom",
    "href": "vignettes/countreg-dists.html#hurdle-negative-binomial-xhnbinom",
    "title": "Countreg: Distributions",
    "section": "Hurdle Negative Binomial (\"xhnbinom\")",
    "text": "Hurdle Negative Binomial (\"xhnbinom\")\nThe hurdle negative binomial has density \\[\\begin{equation}\nf(y\\,|\\,\\pi, \\mu, \\theta) =\n  \\begin{cases}\n    1-\\pi & y = 0 \\\\\n    \\pi \\cdot \\frac{f_{NB}(y\\,|\\,\\mu,\\theta)}{1-f_{NB}(0\\,|\\,\\mu,\\theta)} & y &gt; 0\n  \\end{cases}\n\\end{equation}\\] with expectation \\[\\begin{equation}\n\\mathsf{E}(\\pi,\\mu,\\lambda) = \\frac{\\pi}{1-f_{NB}(0\\,|\\,\\mu,\\theta)} \\cdot \\mu\n\\end{equation}\\] and variance \\[\\begin{equation}\n\\mathsf{VAR}(\\pi,\\mu,\\lambda) = \\frac{\\pi\\cdot\\mu}{1-f_{NB}(0\\,|\\,\\mu,\\theta)} \\cdot\n\\left(1+\\frac{\\mu}{\\theta}+\\mu-\\frac{\\pi\\cdot\\mu}{1-f_{NB}(0\\,|\\,\\mu,\\theta)}\\right)\n\\end{equation}\\] The score functions are, \\[\\begin{equation}\ns_\\pi(\\pi, \\mu, \\theta\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot \\frac{1}{\\pi} -\n  \\mathbf{I}_{\\{0\\}}(y) \\cdot \\frac{1}{1 - \\pi},\n\\end{equation}\\] \\[\\begin{equation}\ns_\\mu(\\pi, \\mu, \\theta\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot s_{\\mu,\\,NB}(\\mu,\\theta\\,|\\,y)\n\\end{equation}\\] and \\[\\begin{equation}\ns_\\theta(\\pi, \\mu, \\theta\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot s_{\\theta,\\,NB}(\\mu,\\theta\\,|\\,y),\n\\end{equation}\\] where \\(s_{\\star,\\,NB}(\\cdot)\\) are the score functions of the zero-truncated negative binomial.\nThe elements of the hessian \\[\\begin{equation}\nh_{\\pi\\pi}(\\pi, \\mu, \\theta\\,|\\,y) = (\\mathbf{I}_{\\{0\\}}(y) - 1) \\cdot \\frac{1}{\\pi^2} -\n  \\mathbf{I}_{\\{0\\}}(y) \\cdot \\frac{1}{(1 - \\pi)^2},\n\\end{equation}\\]\n\\[\\begin{equation}\nh_{\\mu\\mu}(\\pi, \\mu, \\theta\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot h_{\\mu\\mu,\\,NB}(\\mu,\\theta\\,|\\,y)\n\\end{equation}\\]\n\\[\\begin{equation}\nh_{\\theta\\theta}(\\pi, \\mu, \\theta\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot h_{\\theta\\theta,\\,NB}(\\mu,\\theta\\,|\\,y)\n\\end{equation}\\]\n\\[\\begin{equation}\nh_{\\mu\\theta}(\\pi, \\mu, \\theta\\,|\\,y) = (1-\\mathbf{I}_{\\{0\\}}(y)) \\cdot h_{\\mu\\theta,\\,NB}(\\mu,\\theta\\,|\\,y)\n\\end{equation}\\]\n\\[\\begin{equation}\nh_{\\pi\\mu}(\\pi, \\mu, \\theta\\,|\\,y) = h_{\\pi\\theta}(\\pi, \\mu, \\theta\\,|\\,y) = 0.\n\\end{equation}\\]"
  },
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite countreg in publications use:\n\nZeileis A, Kleiber C (2023). countreg: Count Data Regression. R package version 0.3-0, https://R-Forge.R-project.org/projects/countreg/.\n\nIf count data regression models zeroinfl()/hurdle() are used, please cite:\n\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nIf rootogram() is used, please cite:\n\nKleiber C, Zeileis A (2016). “Visualizing Count Data Regressions Using Rootograms.” The American Statistician, 70(3), 296–303. doi:10.1080/00031305.2016.1173590."
  },
  {
    "objectID": "man/nbreg.control.html",
    "href": "man/nbreg.control.html",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of negative binomial regression models using nbreg.\n\nnbreg.control(method = \"BFGS\", maxit = 10000, start = NULL, hessian = TRUE,\n    dot = \"separate\", ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\nstart\n\n\nan optional list with elements “mu” and “theta” containing the coefficients for the corresponding component.\n\n\n\n\nhessian\n\n\nlogical. Should the numerically approximated Hessian be computed to derive an estimate of the variance-covariance matrix? If FALSE and parameter hessA = FALSE in nbreg(), the variance-covariance matrix contains only NAs.\n\n\n\n\ndot\n\n\ncharacter. Controls how two-part Formula’s are processed. See model.frame.Formula.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in nbreg are estimated by maximum likelihood using optim with control options set in nbreg.control. Most arguments are passed on directly to optim and start controls the choice of starting values for calling optim.\nStarting values can be supplied or are estimated by a Poisson regression in glm.fit (the default, starting values of coefficients in \\(\\theta\\) are set to zero to ensure compatibility with NB1). Standard errors are derived using the analytical Hessian matrix or by numerical approximation of the Hessian.\n\nA list with the arguments specified.\n\nnbreg\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nfm1 &lt;- nbreg(satellites ~ width + as.numeric(color), data = CrabSatellites)\n\n## user-supplied start values\nfm2 &lt;- nbreg(satellites ~ width + as.numeric(color), data = CrabSatellites,\n                start = list(mu = c(0, 0, 0), theta = c(0.5)))",
    "crumbs": [
      "Negative binomial regression",
      "nbreg.control"
    ]
  },
  {
    "objectID": "man/nbreg.control.html#control-parameters-for-negative-binomial-count-data-regression",
    "href": "man/nbreg.control.html#control-parameters-for-negative-binomial-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of negative binomial regression models using nbreg.\n\nnbreg.control(method = \"BFGS\", maxit = 10000, start = NULL, hessian = TRUE,\n    dot = \"separate\", ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\nstart\n\n\nan optional list with elements “mu” and “theta” containing the coefficients for the corresponding component.\n\n\n\n\nhessian\n\n\nlogical. Should the numerically approximated Hessian be computed to derive an estimate of the variance-covariance matrix? If FALSE and parameter hessA = FALSE in nbreg(), the variance-covariance matrix contains only NAs.\n\n\n\n\ndot\n\n\ncharacter. Controls how two-part Formula’s are processed. See model.frame.Formula.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in nbreg are estimated by maximum likelihood using optim with control options set in nbreg.control. Most arguments are passed on directly to optim and start controls the choice of starting values for calling optim.\nStarting values can be supplied or are estimated by a Poisson regression in glm.fit (the default, starting values of coefficients in \\(\\theta\\) are set to zero to ensure compatibility with NB1). Standard errors are derived using the analytical Hessian matrix or by numerical approximation of the Hessian.\n\nA list with the arguments specified.\n\nnbreg\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nfm1 &lt;- nbreg(satellites ~ width + as.numeric(color), data = CrabSatellites)\n\n## user-supplied start values\nfm2 &lt;- nbreg(satellites ~ width + as.numeric(color), data = CrabSatellites,\n                start = list(mu = c(0, 0, 0), theta = c(0.5)))",
    "crumbs": [
      "Negative binomial regression",
      "nbreg.control"
    ]
  },
  {
    "objectID": "man/pois.html",
    "href": "man/pois.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the Poisson distribution with parameter lambda.\n\nspois(x, lambda, parameter = \"lambda\", drop = TRUE)\nhpois(x, lambda, parameter = \"lambda\", drop = TRUE)\nmean_pois(lambda, drop = TRUE)\nvar_pois(lambda, drop = TRUE)\n\n\n\n\n\nx\n\n\nvector of quantiles.\n\n\n\n\nlambda\n\n\nvector of (non-negative) means.\n\n\n\n\nparameter\n\n\ncharacter. Derivatives are computed wrt this paramter. Note: Only “lambda” is implemented.\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\nThe Poisson distribution has density\n\n\n\\(p(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\) .\nThe score function is\n\n\n\\(s(\\lambda) = \\frac{x}{\\lambda} - 1\\)\nThe hessian is\n\n\n\\(h(\\lambda) = - \\frac{x}{\\lambda^2}\\)\n\nspois gives the score function, i.e., the 1st derivative of the log-density wrt lambda and hpois gives the hessian, i.e., the 2nd derivative of the log-density wrt lambda. mean and var give the mean and variance, respectively.\n\nPoisson encompassing dpois, ppois, qpois and rpois.\n\n\nlibrary(\"countreg\")\n\n## Simulate some data\nset.seed(123)\ny &lt;- rpois(50, lambda = 3)\n\n## Plot log-likelihood function\npar(mfrow = c(1,3))\nll &lt;- function(x) {sum(dpois(y, x, log = TRUE))}\ncurve(sapply(x, ll), 1, 5, xlab = expression(lambda), ylab = \"\",\n      main = \"Log-likelihood\")\nabline(v = 3, lty = 3)\n\n## Plot score function\ncurve(sapply(x, function(x) sum(spois(y, x))), 1, 5,\n      xlab = expression(lambda), ylab = \"\", main = \"Score\")\nabline(h = 0, lty = 3)\nabline(v = 3, lty = 3)\n\n## Plot hessian\ncurve( sapply(x, function(x) sum(hpois(y, x))), 1, 5,\n      xlab = expression(lambda), ylab = \"\", main = \"Hessian\")\nabline(v = 3, lty = 3)",
    "crumbs": [
      "Distribution extensions",
      "Poisson"
    ]
  },
  {
    "objectID": "man/pois.html#extension-of-the-poisson-distribution",
    "href": "man/pois.html#extension-of-the-poisson-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the Poisson distribution with parameter lambda.\n\nspois(x, lambda, parameter = \"lambda\", drop = TRUE)\nhpois(x, lambda, parameter = \"lambda\", drop = TRUE)\nmean_pois(lambda, drop = TRUE)\nvar_pois(lambda, drop = TRUE)\n\n\n\n\n\nx\n\n\nvector of quantiles.\n\n\n\n\nlambda\n\n\nvector of (non-negative) means.\n\n\n\n\nparameter\n\n\ncharacter. Derivatives are computed wrt this paramter. Note: Only “lambda” is implemented.\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\nThe Poisson distribution has density\n\n\n\\(p(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\) .\nThe score function is\n\n\n\\(s(\\lambda) = \\frac{x}{\\lambda} - 1\\)\nThe hessian is\n\n\n\\(h(\\lambda) = - \\frac{x}{\\lambda^2}\\)\n\nspois gives the score function, i.e., the 1st derivative of the log-density wrt lambda and hpois gives the hessian, i.e., the 2nd derivative of the log-density wrt lambda. mean and var give the mean and variance, respectively.\n\nPoisson encompassing dpois, ppois, qpois and rpois.\n\n\nlibrary(\"countreg\")\n\n## Simulate some data\nset.seed(123)\ny &lt;- rpois(50, lambda = 3)\n\n## Plot log-likelihood function\npar(mfrow = c(1,3))\nll &lt;- function(x) {sum(dpois(y, x, log = TRUE))}\ncurve(sapply(x, ll), 1, 5, xlab = expression(lambda), ylab = \"\",\n      main = \"Log-likelihood\")\nabline(v = 3, lty = 3)\n\n## Plot score function\ncurve(sapply(x, function(x) sum(spois(y, x))), 1, 5,\n      xlab = expression(lambda), ylab = \"\", main = \"Score\")\nabline(h = 0, lty = 3)\nabline(v = 3, lty = 3)\n\n## Plot hessian\ncurve( sapply(x, function(x) sum(hpois(y, x))), 1, 5,\n      xlab = expression(lambda), ylab = \"\", main = \"Hessian\")\nabline(v = 3, lty = 3)",
    "crumbs": [
      "Distribution extensions",
      "Poisson"
    ]
  },
  {
    "objectID": "man/hurdle.html",
    "href": "man/hurdle.html",
    "title": "countreg",
    "section": "",
    "text": "Fit hurdle regression models for count data via maximum likelihood.\n\nhurdle(formula, data, subset, na.action, weights, offset,\n  dist = c(\"poisson\", \"negbin\", \"geometric\", \"binomial\"),\n  zero.dist = c(\"binomial\", \"poisson\", \"negbin\", \"geometric\"),\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\"),\n  size = NULL, control = hurdle.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, see details.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor of the count model. See below for more information on offsets.\n\n\n\n\ndist\n\n\ncharacter specification of count model family.\n\n\n\n\nzero.dist\n\n\ncharacter specification of the zero hurdle model family.\n\n\n\n\nlink\n\n\ncharacter specification of link function in the binomial zero hurdle (only used if zero.dist = “binomial”.\n\n\n\n\nsize\n\n\nsize parameter in case the a binomial count model is used (dist = “binomial”). By default the maximum count is used.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via hurdle.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\n…\n\n\narguments passed to hurdle.control in the default setup.\n\n\n\nHurdle count models are two-component models with a truncated count component for positive counts and a hurdle component that models the zero counts. Thus, unlike zero-inflation models, there are not two sources of zeros: the count model is only employed if the hurdle for modeling the occurence of zeros is exceeded. The count model is typically a truncated Poisson or negative binomial regression (with log link). The geometric distribution is a special case of the negative binomial with size parameter equal to 1. For modeling the hurdle (occurence of positive counts) either a binomial model can be employed or a censored count distribution. Binomial logit and censored geometric models as the hurdle part both lead to the same likelihood function and thus to the same coefficient estimates. A censored negative binomial model for the zero hurdle is only identified if there is at least one non-constant regressor with (true) coefficient different from zero (and if all coefficients are close to zero the model can be poorly conditioned).\nThe formula can be used to specify both components of the model: If a formula of type y ~ x1 + x2 is supplied, then the same regressors are employed in both components. This is equivalent to y ~ x1 + x2 | x1 + x2. Of course, a different set of regressors could be specified for the zero hurdle component, e.g., y ~ x1 + x2 | z1 + z2 + z3 giving the count data model y ~ x1 + x2 conditional on (|) the zero hurdle model y ~ z1 + z2 + z3.\nOffsets can be specified in both parts of the model pertaining to count and zero hurdle model: y ~ x1 + offset(x2) | z1 + z2 + offset(z3), where x2 is used as an offset (i.e., with coefficient fixed to 1) in the count part and z3 analogously in the zero hurdle part. By the rule stated above y ~ x1 + offset(x2) is expanded to y ~ x1 + offset(x2) | x1 + offset(x2). Instead of using the offset() wrapper within the formula, the offset argument can also be employed which sets an offset only for the count model. Thus, formula = y ~ x1 and offset = x2 is equivalent to formula = y ~ x1 + offset(x2) | x1.\nAll parameters are estimated by maximum likelihood using optim, with control options set in hurdle.control. Starting values can be supplied, otherwise they are estimated by glm.fit (the default). By default, the two components of the model are estimated separately using two optim calls. Standard errors are derived numerically using the Hessian matrix returned by optim. See hurdle.control for details.\nThe returned fitted model object is of class “hurdle” and is similar to fitted “glm” objects. For elements such as “coefficients” or “terms” a list is returned with elements for the zero and count components, respectively. For details see below.\nA set of standard extractor functions for fitted model objects is available for objects of class “hurdle”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.matrix. See predict.hurdle for more details on all methods.\n\nAn object of class “hurdle”, i.e., a list with components including\n\n\n\ncoefficients\n\n\na list with elements “count” and “zero” containing the coefficients from the respective models,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list (of lists) with the output(s) from the optim call(s) for minimizing the negative log-likelihood(s),\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call(s),\n\n\n\n\nweights\n\n\nthe case weights used,\n\n\n\n\noffset\n\n\na list with elements “count” and “zero” containing the offset vectors (if any) from the respective models,\n\n\n\n\nn\n\n\nnumber of observations (with weights &gt; 0),\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model (= n - 2),\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\na list with elements “count”, “zero” and “full” containing the terms objects for the respective models,\n\n\n\n\ntheta\n\n\nestimate of the additional \\(\\theta\\) parameter of the negative binomial model(s) (if negative binomial component is used),\n\n\n\n\nSE.logtheta\n\n\nstandard error(s) for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all coefficients in the model (derived from the Hessian of the optim output(s)),\n\n\n\n\ndist\n\n\na list with elements “count” and “zero” with character strings describing the respective distributions used,\n\n\n\n\nlink\n\n\ncharacter string describing the link if a binomial zero hurdle model is used,\n\n\n\n\nlinkinv\n\n\nthe inverse link function corresponding to link,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “count” and “zero” containing the contrasts corresponding to levels from the respective models,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\na list with elements “count” and “zero” containing the model matrices from the respective models (if x = TRUE).\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\nMullahy J (1986). “Specification and Testing of Some Modified Count Data Models”. Journal of Econometrics. 33, 341–365.\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nhurdle.control, glm, glm.fit, glm.nb, zeroinfl\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\n\n## logit-poisson\n## \"satellites ~ .\" is the same as \"satellites ~ . | .\", i.e.\n## \"satellites ~ width + color | width + color\"\nfm_hp1 &lt;- hurdle(satellites ~ ., data = cs)\nsummary(fm_hp1)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.7371 -0.8383 -0.2976  0.6431  4.2699 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007165   0.066627   0.108    0.914\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.0708     2.8065  -3.588 0.000333 ***\nwidth         0.4583     0.1040   4.407 1.05e-05 ***\ncolor        -0.5090     0.2237  -2.276 0.022862 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 13 \nLog-likelihood: -362.1 on 6 Df\n\n## geometric-poisson\nfm_hp2 &lt;- hurdle(satellites ~ ., data = cs, zero = \"geometric\")\nsummary(fm_hp2)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs, zero.dist = \"geometric\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.7371 -0.8383 -0.2976  0.6431  4.2699 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007165   0.066627   0.108    0.914\nZero hurdle model coefficients (censored geometric with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.0708     2.8065  -3.588 0.000333 ***\nwidth         0.4583     0.1040   4.407 1.05e-05 ***\ncolor        -0.5090     0.2237  -2.276 0.022862 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 13 \nLog-likelihood: -362.1 on 6 Df\n\n## logit and geometric model are equivalent\ncoef(fm_hp1, model = \"zero\") - coef(fm_hp2, model = \"zero\")\n\n(Intercept)       width       color \n          0           0           0 \n\n## logit-negbin\nfm_hnb1 &lt;- hurdle(satellites ~ ., data = cs, dist = \"negbin\")\nsummary(fm_hnb1)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.3835 -0.7244 -0.2636  0.5557  3.6080 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.428567   0.941077   0.455    0.649    \nwidth       0.037845   0.032749   1.156    0.248    \ncolor       0.006929   0.091078   0.076    0.939    \nLog(theta)  1.527382   0.352950   4.327 1.51e-05 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.0708     2.8065  -3.588 0.000333 ***\nwidth         0.4583     0.1040   4.407 1.05e-05 ***\ncolor        -0.5090     0.2237  -2.276 0.022862 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 4.6061\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -350.4 on 7 Df\n\n## negbin-negbin\n## (poorly conditioned zero hurdle, note increased standard errors)\nfm_hnb2 &lt;- hurdle(satellites ~ ., data = cs, dist = \"negbin\", zero = \"negbin\")\nsummary(fm_hnb2)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs, dist = \"negbin\", zero.dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.3984 -0.7163 -0.2724  0.5615  3.6221 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.428567   0.941077   0.455    0.649    \nwidth       0.037845   0.032749   1.156    0.248    \ncolor       0.006929   0.091078   0.076    0.939    \nLog(theta)  1.527382   0.352950   4.327 1.51e-05 ***\nZero hurdle model coefficients (censored negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -8.7334     5.6289  -1.552    0.121\nwidth         0.3872     0.2840   1.363    0.173\ncolor        -0.4097     0.4230  -0.969    0.333\nLog(theta)    0.5124     2.5691   0.199    0.842\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 4.6061, zero = 1.6693\nNumber of iterations in BFGS optimization: 35 \nLog-likelihood: -350.3 on 8 Df",
    "crumbs": [
      "Hurdle models",
      "hurdle"
    ]
  },
  {
    "objectID": "man/hurdle.html#hurdle-models-for-count-data-regression",
    "href": "man/hurdle.html#hurdle-models-for-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Fit hurdle regression models for count data via maximum likelihood.\n\nhurdle(formula, data, subset, na.action, weights, offset,\n  dist = c(\"poisson\", \"negbin\", \"geometric\", \"binomial\"),\n  zero.dist = c(\"binomial\", \"poisson\", \"negbin\", \"geometric\"),\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\"),\n  size = NULL, control = hurdle.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, see details.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor of the count model. See below for more information on offsets.\n\n\n\n\ndist\n\n\ncharacter specification of count model family.\n\n\n\n\nzero.dist\n\n\ncharacter specification of the zero hurdle model family.\n\n\n\n\nlink\n\n\ncharacter specification of link function in the binomial zero hurdle (only used if zero.dist = “binomial”.\n\n\n\n\nsize\n\n\nsize parameter in case the a binomial count model is used (dist = “binomial”). By default the maximum count is used.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via hurdle.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\n…\n\n\narguments passed to hurdle.control in the default setup.\n\n\n\nHurdle count models are two-component models with a truncated count component for positive counts and a hurdle component that models the zero counts. Thus, unlike zero-inflation models, there are not two sources of zeros: the count model is only employed if the hurdle for modeling the occurence of zeros is exceeded. The count model is typically a truncated Poisson or negative binomial regression (with log link). The geometric distribution is a special case of the negative binomial with size parameter equal to 1. For modeling the hurdle (occurence of positive counts) either a binomial model can be employed or a censored count distribution. Binomial logit and censored geometric models as the hurdle part both lead to the same likelihood function and thus to the same coefficient estimates. A censored negative binomial model for the zero hurdle is only identified if there is at least one non-constant regressor with (true) coefficient different from zero (and if all coefficients are close to zero the model can be poorly conditioned).\nThe formula can be used to specify both components of the model: If a formula of type y ~ x1 + x2 is supplied, then the same regressors are employed in both components. This is equivalent to y ~ x1 + x2 | x1 + x2. Of course, a different set of regressors could be specified for the zero hurdle component, e.g., y ~ x1 + x2 | z1 + z2 + z3 giving the count data model y ~ x1 + x2 conditional on (|) the zero hurdle model y ~ z1 + z2 + z3.\nOffsets can be specified in both parts of the model pertaining to count and zero hurdle model: y ~ x1 + offset(x2) | z1 + z2 + offset(z3), where x2 is used as an offset (i.e., with coefficient fixed to 1) in the count part and z3 analogously in the zero hurdle part. By the rule stated above y ~ x1 + offset(x2) is expanded to y ~ x1 + offset(x2) | x1 + offset(x2). Instead of using the offset() wrapper within the formula, the offset argument can also be employed which sets an offset only for the count model. Thus, formula = y ~ x1 and offset = x2 is equivalent to formula = y ~ x1 + offset(x2) | x1.\nAll parameters are estimated by maximum likelihood using optim, with control options set in hurdle.control. Starting values can be supplied, otherwise they are estimated by glm.fit (the default). By default, the two components of the model are estimated separately using two optim calls. Standard errors are derived numerically using the Hessian matrix returned by optim. See hurdle.control for details.\nThe returned fitted model object is of class “hurdle” and is similar to fitted “glm” objects. For elements such as “coefficients” or “terms” a list is returned with elements for the zero and count components, respectively. For details see below.\nA set of standard extractor functions for fitted model objects is available for objects of class “hurdle”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.matrix. See predict.hurdle for more details on all methods.\n\nAn object of class “hurdle”, i.e., a list with components including\n\n\n\ncoefficients\n\n\na list with elements “count” and “zero” containing the coefficients from the respective models,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list (of lists) with the output(s) from the optim call(s) for minimizing the negative log-likelihood(s),\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call(s),\n\n\n\n\nweights\n\n\nthe case weights used,\n\n\n\n\noffset\n\n\na list with elements “count” and “zero” containing the offset vectors (if any) from the respective models,\n\n\n\n\nn\n\n\nnumber of observations (with weights &gt; 0),\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model (= n - 2),\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\na list with elements “count”, “zero” and “full” containing the terms objects for the respective models,\n\n\n\n\ntheta\n\n\nestimate of the additional \\(\\theta\\) parameter of the negative binomial model(s) (if negative binomial component is used),\n\n\n\n\nSE.logtheta\n\n\nstandard error(s) for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all coefficients in the model (derived from the Hessian of the optim output(s)),\n\n\n\n\ndist\n\n\na list with elements “count” and “zero” with character strings describing the respective distributions used,\n\n\n\n\nlink\n\n\ncharacter string describing the link if a binomial zero hurdle model is used,\n\n\n\n\nlinkinv\n\n\nthe inverse link function corresponding to link,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “count” and “zero” containing the contrasts corresponding to levels from the respective models,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\na list with elements “count” and “zero” containing the model matrices from the respective models (if x = TRUE).\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\nMullahy J (1986). “Specification and Testing of Some Modified Count Data Models”. Journal of Econometrics. 33, 341–365.\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nhurdle.control, glm, glm.fit, glm.nb, zeroinfl\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\n\n## logit-poisson\n## \"satellites ~ .\" is the same as \"satellites ~ . | .\", i.e.\n## \"satellites ~ width + color | width + color\"\nfm_hp1 &lt;- hurdle(satellites ~ ., data = cs)\nsummary(fm_hp1)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.7371 -0.8383 -0.2976  0.6431  4.2699 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007165   0.066627   0.108    0.914\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.0708     2.8065  -3.588 0.000333 ***\nwidth         0.4583     0.1040   4.407 1.05e-05 ***\ncolor        -0.5090     0.2237  -2.276 0.022862 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 13 \nLog-likelihood: -362.1 on 6 Df\n\n## geometric-poisson\nfm_hp2 &lt;- hurdle(satellites ~ ., data = cs, zero = \"geometric\")\nsummary(fm_hp2)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs, zero.dist = \"geometric\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.7371 -0.8383 -0.2976  0.6431  4.2699 \n\nCount model coefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007165   0.066627   0.108    0.914\nZero hurdle model coefficients (censored geometric with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.0708     2.8065  -3.588 0.000333 ***\nwidth         0.4583     0.1040   4.407 1.05e-05 ***\ncolor        -0.5090     0.2237  -2.276 0.022862 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 13 \nLog-likelihood: -362.1 on 6 Df\n\n## logit and geometric model are equivalent\ncoef(fm_hp1, model = \"zero\") - coef(fm_hp2, model = \"zero\")\n\n(Intercept)       width       color \n          0           0           0 \n\n## logit-negbin\nfm_hnb1 &lt;- hurdle(satellites ~ ., data = cs, dist = \"negbin\")\nsummary(fm_hnb1)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.3835 -0.7244 -0.2636  0.5557  3.6080 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.428567   0.941077   0.455    0.649    \nwidth       0.037845   0.032749   1.156    0.248    \ncolor       0.006929   0.091078   0.076    0.939    \nLog(theta)  1.527382   0.352950   4.327 1.51e-05 ***\nZero hurdle model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -10.0708     2.8065  -3.588 0.000333 ***\nwidth         0.4583     0.1040   4.407 1.05e-05 ***\ncolor        -0.5090     0.2237  -2.276 0.022862 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 4.6061\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -350.4 on 7 Df\n\n## negbin-negbin\n## (poorly conditioned zero hurdle, note increased standard errors)\nfm_hnb2 &lt;- hurdle(satellites ~ ., data = cs, dist = \"negbin\", zero = \"negbin\")\nsummary(fm_hnb2)\n\n\nCall:\nhurdle(formula = satellites ~ ., data = cs, dist = \"negbin\", zero.dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.3984 -0.7163 -0.2724  0.5615  3.6221 \n\nCount model coefficients (truncated negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.428567   0.941077   0.455    0.649    \nwidth       0.037845   0.032749   1.156    0.248    \ncolor       0.006929   0.091078   0.076    0.939    \nLog(theta)  1.527382   0.352950   4.327 1.51e-05 ***\nZero hurdle model coefficients (censored negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -8.7334     5.6289  -1.552    0.121\nwidth         0.3872     0.2840   1.363    0.173\ncolor        -0.4097     0.4230  -0.969    0.333\nLog(theta)    0.5124     2.5691   0.199    0.842\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 4.6061, zero = 1.6693\nNumber of iterations in BFGS optimization: 35 \nLog-likelihood: -350.3 on 8 Df",
    "crumbs": [
      "Hurdle models",
      "hurdle"
    ]
  },
  {
    "objectID": "man/CodParasites.html",
    "href": "man/CodParasites.html",
    "title": "countreg",
    "section": "",
    "text": "Data on parasite infection in cod along the coast of Finmark.\n\ndata(\"CodParasites\")\n\nA data frame containing 1254 observations on 10 variables.\n\n\nintensity\n\n\nNumber of parasites.\n\n\nprevalence\n\n\nFactor indicating presence of parasites (i.e., intensity &gt; 0).\n\n\narea\n\n\nFactor indicating sampling area.\n\n\nyear\n\n\nFactor indicating sampling year.\n\n\ndepth\n\n\nDepth at which the fish were caught.\n\n\nweight\n\n\nWeight of the fish.\n\n\nlength\n\n\nLength of the fish.\n\n\nsex\n\n\nFactor indicating sex of the fish.\n\n\nstage\n\n\nFactor indicating stage of the fish.\n\n\nage\n\n\nAge of the fish.\n\n\nThe red king crab Paralithodes camtschaticus was deliberately introduced to the Barents Sea in the 1960s and 1970s from its native area in the North Pacific. The carapace of these crabs is used by the leech Johanssonia arctica to deposit its eggs. The leech in turn is a vector for the blood parasite Trypanosoma murmanensis that can infect marine fish, including cod.\nHemmingsen et al. (2005) examined cod for trypanosome infections during annual cruises along the coast of Finnmark in North Norway over three successive years and in four different areas (A1 Sørøya; A2 Magerøya; A3 Tanafjord; A4 Varangerfjord). They show that trypanosome infections are strongest in the area Varangerfjord where the density of of red king crabs is highest. Thus, there is evidence that the introduction of the foreign red king crabs had an indirect detrimental effect on the health of the native cod population. This situation stands out because it is not an introduced parasite that is dangerous for a native host, but rather an introduced host that promotes transmission of two endemic parasites.\nZuur et al. (2009) reanalyze the data using binary and count data regression models in Chapters 10.2.2, 11.3.2, 11.4.2, 11.5.2.\n\nThe data are taken from the online supplements of Zuur et al. (2009). http://highstat.com/index.php/mixed-effects-models-and-extensions-in-ecology-with-r\n\nHemmingsen W, Jansen PA, MacKenzie K (2005). “Crabs, Leeches and Trypanosomes: An Unholy Trinity?”, Marine Pollution Bulletin 50(3), 336–339.\nZuur AF, Ieno EN, Walker NJ, Saveliev AA, Smith GM (2009). Mixed Effects Models and Extensions in Ecology with R, Springer-Verlag, New York.\n\n\nlibrary(\"countreg\")\n\n## load data\ndata(\"CodParasites\", package = \"countreg\")\n\n## Table 1 from Hemmingsen et al. (2005)\n## number of observations\nxtabs(~ area + year, data = CodParasites)\n\n               year\narea            1999 2000 2001\n  soroya         147   55   70\n  mageroya        98   50  107\n  tanafjord      183   75  157\n  varangerfjord  139   50  123\n\n## prevalence of parasites (NAs counted as \"yes\")\ntab &lt;- xtabs(~ area + year + factor(is.na(prevalence) | prevalence == \"yes\"),\n  data = CodParasites)\nround(100 * prop.table(tab, 1:2)[,,2], digits = 1)\n\n               year\narea            1999 2000 2001\n  soroya        61.2 70.9 10.0\n  mageroya      32.7 36.0 31.8\n  tanafjord     33.9 57.3 28.7\n  varangerfjord 75.5 88.0 65.9\n\n## omit NAs in response\nCodParasites &lt;- subset(CodParasites, !is.na(intensity))\n\n## exploratory displays for hurdle and counts\npar(mfrow = c(2, 2))\nplot(factor(intensity == 0) ~ interaction(year, area), data = CodParasites)\nplot(factor(intensity == 0) ~ length, data = CodParasites, breaks = c(15, 3:8 * 10, 105))\nplot(jitter(intensity) ~ interaction(year, area), data = CodParasites,\n  subset = intensity &gt; 0, log = \"y\")\nplot(jitter(intensity) ~ length, data = CodParasites, subset = intensity &gt; 0, log = \"y\")\n\n\n\n\n\n\n## count data models\ncp_p   &lt;-    glm(intensity ~ length + area * year, data = CodParasites, family = poisson)\ncp_nb  &lt;- glm.nb(intensity ~ length + area * year, data = CodParasites)\ncp_hp  &lt;- hurdle(intensity ~ length + area * year, data = CodParasites, dist = \"poisson\")\ncp_hnb &lt;- hurdle(intensity ~ length + area * year, data = CodParasites, dist = \"negbin\")\nAIC(cp_p, cp_nb, cp_hp, cp_hnb)\n\n       df       AIC\ncp_p   13 20377.862\ncp_nb  14  5030.673\ncp_hp  26 13687.576\ncp_hnb 27  4937.085\n\nBIC(cp_p, cp_nb, cp_hp, cp_hnb)\n\n       df       BIC\ncp_p   13 20443.935\ncp_nb  14  5101.829\ncp_hp  26 13819.722\ncp_hnb 27  5074.314\n\n## rootograms\nif(require(\"topmodels\")) {\npar(mfrow = c(2, 2))\nrootogram(cp_p, max = 50, main = \"Poisson\")\nrootogram(cp_nb, max = 50, main = \"Negative Binomial\")\nrootogram(cp_hp, max = 50, main = \"Hurdle Poisson\")\nrootogram(cp_hnb, max = 50, main = \"Hurdle Negative Binomial\")\n}"
  },
  {
    "objectID": "man/CodParasites.html#parasite-infections-in-cod",
    "href": "man/CodParasites.html#parasite-infections-in-cod",
    "title": "countreg",
    "section": "",
    "text": "Data on parasite infection in cod along the coast of Finmark.\n\ndata(\"CodParasites\")\n\nA data frame containing 1254 observations on 10 variables.\n\n\nintensity\n\n\nNumber of parasites.\n\n\nprevalence\n\n\nFactor indicating presence of parasites (i.e., intensity &gt; 0).\n\n\narea\n\n\nFactor indicating sampling area.\n\n\nyear\n\n\nFactor indicating sampling year.\n\n\ndepth\n\n\nDepth at which the fish were caught.\n\n\nweight\n\n\nWeight of the fish.\n\n\nlength\n\n\nLength of the fish.\n\n\nsex\n\n\nFactor indicating sex of the fish.\n\n\nstage\n\n\nFactor indicating stage of the fish.\n\n\nage\n\n\nAge of the fish.\n\n\nThe red king crab Paralithodes camtschaticus was deliberately introduced to the Barents Sea in the 1960s and 1970s from its native area in the North Pacific. The carapace of these crabs is used by the leech Johanssonia arctica to deposit its eggs. The leech in turn is a vector for the blood parasite Trypanosoma murmanensis that can infect marine fish, including cod.\nHemmingsen et al. (2005) examined cod for trypanosome infections during annual cruises along the coast of Finnmark in North Norway over three successive years and in four different areas (A1 Sørøya; A2 Magerøya; A3 Tanafjord; A4 Varangerfjord). They show that trypanosome infections are strongest in the area Varangerfjord where the density of of red king crabs is highest. Thus, there is evidence that the introduction of the foreign red king crabs had an indirect detrimental effect on the health of the native cod population. This situation stands out because it is not an introduced parasite that is dangerous for a native host, but rather an introduced host that promotes transmission of two endemic parasites.\nZuur et al. (2009) reanalyze the data using binary and count data regression models in Chapters 10.2.2, 11.3.2, 11.4.2, 11.5.2.\n\nThe data are taken from the online supplements of Zuur et al. (2009). http://highstat.com/index.php/mixed-effects-models-and-extensions-in-ecology-with-r\n\nHemmingsen W, Jansen PA, MacKenzie K (2005). “Crabs, Leeches and Trypanosomes: An Unholy Trinity?”, Marine Pollution Bulletin 50(3), 336–339.\nZuur AF, Ieno EN, Walker NJ, Saveliev AA, Smith GM (2009). Mixed Effects Models and Extensions in Ecology with R, Springer-Verlag, New York.\n\n\nlibrary(\"countreg\")\n\n## load data\ndata(\"CodParasites\", package = \"countreg\")\n\n## Table 1 from Hemmingsen et al. (2005)\n## number of observations\nxtabs(~ area + year, data = CodParasites)\n\n               year\narea            1999 2000 2001\n  soroya         147   55   70\n  mageroya        98   50  107\n  tanafjord      183   75  157\n  varangerfjord  139   50  123\n\n## prevalence of parasites (NAs counted as \"yes\")\ntab &lt;- xtabs(~ area + year + factor(is.na(prevalence) | prevalence == \"yes\"),\n  data = CodParasites)\nround(100 * prop.table(tab, 1:2)[,,2], digits = 1)\n\n               year\narea            1999 2000 2001\n  soroya        61.2 70.9 10.0\n  mageroya      32.7 36.0 31.8\n  tanafjord     33.9 57.3 28.7\n  varangerfjord 75.5 88.0 65.9\n\n## omit NAs in response\nCodParasites &lt;- subset(CodParasites, !is.na(intensity))\n\n## exploratory displays for hurdle and counts\npar(mfrow = c(2, 2))\nplot(factor(intensity == 0) ~ interaction(year, area), data = CodParasites)\nplot(factor(intensity == 0) ~ length, data = CodParasites, breaks = c(15, 3:8 * 10, 105))\nplot(jitter(intensity) ~ interaction(year, area), data = CodParasites,\n  subset = intensity &gt; 0, log = \"y\")\nplot(jitter(intensity) ~ length, data = CodParasites, subset = intensity &gt; 0, log = \"y\")\n\n\n\n\n\n\n## count data models\ncp_p   &lt;-    glm(intensity ~ length + area * year, data = CodParasites, family = poisson)\ncp_nb  &lt;- glm.nb(intensity ~ length + area * year, data = CodParasites)\ncp_hp  &lt;- hurdle(intensity ~ length + area * year, data = CodParasites, dist = \"poisson\")\ncp_hnb &lt;- hurdle(intensity ~ length + area * year, data = CodParasites, dist = \"negbin\")\nAIC(cp_p, cp_nb, cp_hp, cp_hnb)\n\n       df       AIC\ncp_p   13 20377.862\ncp_nb  14  5030.673\ncp_hp  26 13687.576\ncp_hnb 27  4937.085\n\nBIC(cp_p, cp_nb, cp_hp, cp_hnb)\n\n       df       BIC\ncp_p   13 20443.935\ncp_nb  14  5101.829\ncp_hp  26 13819.722\ncp_hnb 27  5074.314\n\n## rootograms\nif(require(\"topmodels\")) {\npar(mfrow = c(2, 2))\nrootogram(cp_p, max = 50, main = \"Poisson\")\nrootogram(cp_nb, max = 50, main = \"Negative Binomial\")\nrootogram(cp_hp, max = 50, main = \"Hurdle Poisson\")\nrootogram(cp_hnb, max = 50, main = \"Hurdle Negative Binomial\")\n}"
  },
  {
    "objectID": "man/zerotrunc.control.html",
    "href": "man/zerotrunc.control.html",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of zero-truncated count regression models using zerotrunc.\n\nzerotrunc.control(method = \"BFGS\", maxit = 10000, start = NULL, ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\nstart\n\n\nan optional vector of starting values, see details.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in zerotrunc are estimated by maximum likelihood using optim with control options set in zerotrunc.control. Most arguments are passed on directly to optim, only start is used to control how optim is called.\nStarting values can be supplied via start or estimated by glm.fit (default). Standard errors are derived numerically using the Hessian matrix returned by optim. To supply starting values, start should be a vector with (at least) starting values for the regression coefficients. In case a negative binomial distribution with unknown theta is used, a starting value for theta may be supplied by adding an additional vector element (e.g., start = c(coef, theta)); by default theta = 1 is used as the starting value otherwise.\n\nA list with the arguments specified.\n\nzerotrunc\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nzt_nb &lt;- zerotrunc(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  subset = satellites &gt; 0, dist = \"negbin\")\n\n## user-supplied start values and other options\nzt_nb2 &lt;- zerotrunc(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  subset = satellites &gt; 0, dist = \"negbin\", start = c(0.5, 0, 0))",
    "crumbs": [
      "Zero-truncated models",
      "zerotrunc.control"
    ]
  },
  {
    "objectID": "man/zerotrunc.control.html#control-parameters-for-zero-truncated-count-data-regression",
    "href": "man/zerotrunc.control.html#control-parameters-for-zero-truncated-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Various parameters that control fitting of zero-truncated count regression models using zerotrunc.\n\nzerotrunc.control(method = \"BFGS\", maxit = 10000, start = NULL, ...)\n\n\n\n\n\nmethod\n\n\ncharacters string specifying the method argument passed to optim.\n\n\n\n\nmaxit\n\n\ninteger specifying the maxit argument (maximal number of iterations) passed to optim.\n\n\n\n\nstart\n\n\nan optional vector of starting values, see details.\n\n\n\n\n…\n\n\narguments passed to optim.\n\n\n\nAll parameters in zerotrunc are estimated by maximum likelihood using optim with control options set in zerotrunc.control. Most arguments are passed on directly to optim, only start is used to control how optim is called.\nStarting values can be supplied via start or estimated by glm.fit (default). Standard errors are derived numerically using the Hessian matrix returned by optim. To supply starting values, start should be a vector with (at least) starting values for the regression coefficients. In case a negative binomial distribution with unknown theta is used, a starting value for theta may be supplied by adding an additional vector element (e.g., start = c(coef, theta)); by default theta = 1 is used as the starting value otherwise.\n\nA list with the arguments specified.\n\nzerotrunc\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\n\n## default start values\nzt_nb &lt;- zerotrunc(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  subset = satellites &gt; 0, dist = \"negbin\")\n\n## user-supplied start values and other options\nzt_nb2 &lt;- zerotrunc(satellites ~ width + as.numeric(color), data = CrabSatellites,\n  subset = satellites &gt; 0, dist = \"negbin\", start = c(0.5, 0, 0))",
    "crumbs": [
      "Zero-truncated models",
      "zerotrunc.control"
    ]
  },
  {
    "objectID": "man/zeroinfl.html",
    "href": "man/zeroinfl.html",
    "title": "countreg",
    "section": "",
    "text": "Fit zero-inflated regression models for count data via maximum likelihood.\n\nzeroinfl(formula, data, subset, na.action, weights, offset,\n  dist = c(\"poisson\", \"negbin\", \"geometric\", \"binomial\"),\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\"),\n  size = NULL, control = zeroinfl.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, see details.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor of the count model. See below for more information on offsets.\n\n\n\n\ndist\n\n\ncharacter specification of count model family (a log link is always used).\n\n\n\n\nlink\n\n\ncharacter specification of link function in the binary zero-inflation model (a binomial family is always used).\n\n\n\n\nsize\n\n\nsize parameter in case the a binomial count model is used (dist = “binomial”). By default the maximum count is used.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via zeroinfl.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\n…\n\n\narguments passed to zeroinfl.control in the default setup.\n\n\n\nZero-inflated count models are two-component mixture models combining a point mass at zero with a proper count distribution. Thus, there are two sources of zeros: zeros may come from both the point mass and from the count component. Usually the count model is a Poisson or negative binomial regression (with log link). The geometric distribution is a special case of the negative binomial with size parameter equal to 1. For modeling the unobserved state (zero vs. count), a binary model is used that captures the probability of zero inflation. in the simplest case only with an intercept but potentially containing regressors. For this zero-inflation model, a binomial model with different links can be used, typically logit or probit.\nThe formula can be used to specify both components of the model: If a formula of type y ~ x1 + x2 is supplied, then the same regressors are employed in both components. This is equivalent to y ~ x1 + x2 | x1 + x2. Of course, a different set of regressors could be specified for the count and zero-inflation component, e.g., y ~ x1 + x2 | z1 + z2 + z3 giving the count data model y ~ x1 + x2 conditional on (|) the zero-inflation model y ~ z1 + z2 + z3. A simple inflation model where all zero counts have the same probability of belonging to the zero component can by specified by the formula y ~ x1 + x2 | 1.\nOffsets can be specified in both components of the model pertaining to count and zero-inflation model: y ~ x1 + offset(x2) | z1 + z2 + offset(z3), where x2 is used as an offset (i.e., with coefficient fixed to 1) in the count component and z3 analogously in the zero-inflation component. By the rule stated above y ~ x1 + offset(x2) is expanded to y ~ x1 + offset(x2) | x1 + offset(x2). Instead of using the offset() wrapper within the formula, the offset argument can also be employed which sets an offset only for the count model. Thus, formula = y ~ x1 and offset = x2 is equivalent to formula = y ~ x1 + offset(x2) | x1.\nAll parameters are estimated by maximum likelihood using optim, with control options set in zeroinfl.control. Starting values can be supplied, estimated by the EM (expectation maximization) algorithm, or by glm.fit (the default). Standard errors are derived numerically using the Hessian matrix returned by optim. See zeroinfl.control for details.\nThe returned fitted model object is of class “zeroinfl” and is similar to fitted “glm” objects. For elements such as “coefficients” or “terms” a list is returned with elements for the zero and count component, respectively. For details see below.\nA set of standard extractor functions for fitted model objects is available for objects of class “zeroinfl”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.matrix. See predict.zeroinfl for more details on all methods.\n\nAn object of class “zeroinfl”, i.e., a list with components including\n\n\n\ncoefficients\n\n\na list with elements “count” and “zero” containing the coefficients from the respective models,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list with the output from the optim call for minimizing the negative log-likelihood,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call,\n\n\n\n\nweights\n\n\nthe case weights used,\n\n\n\n\noffset\n\n\na list with elements “count” and “zero” containing the offset vectors (if any) from the respective models,\n\n\n\n\nn\n\n\nnumber of observations (with weights &gt; 0),\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model (= n - 2),\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\na list with elements “count”, “zero” and “full” containing the terms objects for the respective models,\n\n\n\n\ntheta\n\n\nestimate of the additional \\(\\theta\\) parameter of the negative binomial model (if a negative binomial regression is used),\n\n\n\n\nSE.logtheta\n\n\nstandard error for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all coefficients in the model (derived from the Hessian of the optim output),\n\n\n\n\ndist\n\n\ncharacter string describing the count distribution used,\n\n\n\n\nlink\n\n\ncharacter string describing the link of the zero-inflation model,\n\n\n\n\nlinkinv\n\n\nthe inverse link function corresponding to link,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “count” and “zero” containing the contrasts corresponding to levels from the respective models,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\na list with elements “count” and “zero” containing the model matrices from the respective models (if x = TRUE),\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\nLambert D (1992). “Zero-Inflated Poisson Regression, with an Application to Defects in Manufacturing”. Technometrics. 34(1), 1–14.\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nzeroinfl.control, glm, glm.fit, glm.nb, hurdle\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\n\n## without inflation\n## (\"satellites ~ .\" is \"satellites ~ width + color\")\nfm_pois &lt;- glm(satellites ~ ., data = cs, family = poisson)\nfm_qpois &lt;- glm(satellites ~ ., data = cs, family = quasipoisson)\nfm_nb &lt;- glm.nb(satellites ~ ., data = cs)\n\n## with simple inflation (no regressors for zero component)\nfm_zip &lt;- zeroinfl(satellites ~ . | 1, data = cs)\nfm_zinb &lt;- zeroinfl(satellites ~ . | 1, data = cs, dist = \"negbin\")\n\n## inflation with regressors\n## (\"satellites ~ . | .\" is \"satellites ~ width + color | width + color\")\nfm_zip2 &lt;- zeroinfl(satellites ~ . | ., data = cs)\nfm_zinb2 &lt;- zeroinfl(satellites ~ . | ., data = cs, dist = \"negbin\")",
    "crumbs": [
      "Zero-inflated models",
      "zeroinfl"
    ]
  },
  {
    "objectID": "man/zeroinfl.html#zero-inflated-count-data-regression",
    "href": "man/zeroinfl.html#zero-inflated-count-data-regression",
    "title": "countreg",
    "section": "",
    "text": "Fit zero-inflated regression models for count data via maximum likelihood.\n\nzeroinfl(formula, data, subset, na.action, weights, offset,\n  dist = c(\"poisson\", \"negbin\", \"geometric\", \"binomial\"),\n  link = c(\"logit\", \"probit\", \"cloglog\", \"cauchit\", \"log\"),\n  size = NULL, control = zeroinfl.control(...),\n  model = TRUE, y = TRUE, x = FALSE, ...)\n\n\n\n\n\nformula\n\n\nsymbolic description of the model, see details.\n\n\n\n\ndata, subset, na.action\n\n\narguments controlling formula processing via model.frame.\n\n\n\n\nweights\n\n\noptional numeric vector of weights.\n\n\n\n\noffset\n\n\noptional numeric vector with an a priori known component to be included in the linear predictor of the count model. See below for more information on offsets.\n\n\n\n\ndist\n\n\ncharacter specification of count model family (a log link is always used).\n\n\n\n\nlink\n\n\ncharacter specification of link function in the binary zero-inflation model (a binomial family is always used).\n\n\n\n\nsize\n\n\nsize parameter in case the a binomial count model is used (dist = “binomial”). By default the maximum count is used.\n\n\n\n\ncontrol\n\n\na list of control arguments specified via zeroinfl.control.\n\n\n\n\nmodel, y, x\n\n\nlogicals. If TRUE the corresponding components of the fit (model frame, response, model matrix) are returned.\n\n\n\n\n…\n\n\narguments passed to zeroinfl.control in the default setup.\n\n\n\nZero-inflated count models are two-component mixture models combining a point mass at zero with a proper count distribution. Thus, there are two sources of zeros: zeros may come from both the point mass and from the count component. Usually the count model is a Poisson or negative binomial regression (with log link). The geometric distribution is a special case of the negative binomial with size parameter equal to 1. For modeling the unobserved state (zero vs. count), a binary model is used that captures the probability of zero inflation. in the simplest case only with an intercept but potentially containing regressors. For this zero-inflation model, a binomial model with different links can be used, typically logit or probit.\nThe formula can be used to specify both components of the model: If a formula of type y ~ x1 + x2 is supplied, then the same regressors are employed in both components. This is equivalent to y ~ x1 + x2 | x1 + x2. Of course, a different set of regressors could be specified for the count and zero-inflation component, e.g., y ~ x1 + x2 | z1 + z2 + z3 giving the count data model y ~ x1 + x2 conditional on (|) the zero-inflation model y ~ z1 + z2 + z3. A simple inflation model where all zero counts have the same probability of belonging to the zero component can by specified by the formula y ~ x1 + x2 | 1.\nOffsets can be specified in both components of the model pertaining to count and zero-inflation model: y ~ x1 + offset(x2) | z1 + z2 + offset(z3), where x2 is used as an offset (i.e., with coefficient fixed to 1) in the count component and z3 analogously in the zero-inflation component. By the rule stated above y ~ x1 + offset(x2) is expanded to y ~ x1 + offset(x2) | x1 + offset(x2). Instead of using the offset() wrapper within the formula, the offset argument can also be employed which sets an offset only for the count model. Thus, formula = y ~ x1 and offset = x2 is equivalent to formula = y ~ x1 + offset(x2) | x1.\nAll parameters are estimated by maximum likelihood using optim, with control options set in zeroinfl.control. Starting values can be supplied, estimated by the EM (expectation maximization) algorithm, or by glm.fit (the default). Standard errors are derived numerically using the Hessian matrix returned by optim. See zeroinfl.control for details.\nThe returned fitted model object is of class “zeroinfl” and is similar to fitted “glm” objects. For elements such as “coefficients” or “terms” a list is returned with elements for the zero and count component, respectively. For details see below.\nA set of standard extractor functions for fitted model objects is available for objects of class “zeroinfl”, including methods to the generic functions print, summary, coef, vcov, logLik, residuals, predict, fitted, terms, model.matrix. See predict.zeroinfl for more details on all methods.\n\nAn object of class “zeroinfl”, i.e., a list with components including\n\n\n\ncoefficients\n\n\na list with elements “count” and “zero” containing the coefficients from the respective models,\n\n\n\n\nresiduals\n\n\na vector of raw residuals (observed - fitted),\n\n\n\n\nfitted.values\n\n\na vector of fitted means,\n\n\n\n\noptim\n\n\na list with the output from the optim call for minimizing the negative log-likelihood,\n\n\n\n\ncontrol\n\n\nthe control arguments passed to the optim call,\n\n\n\n\nstart\n\n\nthe starting values for the parameters passed to the optim call,\n\n\n\n\nweights\n\n\nthe case weights used,\n\n\n\n\noffset\n\n\na list with elements “count” and “zero” containing the offset vectors (if any) from the respective models,\n\n\n\n\nn\n\n\nnumber of observations (with weights &gt; 0),\n\n\n\n\ndf.null\n\n\nresidual degrees of freedom for the null model (= n - 2),\n\n\n\n\ndf.residual\n\n\nresidual degrees of freedom for fitted model,\n\n\n\n\nterms\n\n\na list with elements “count”, “zero” and “full” containing the terms objects for the respective models,\n\n\n\n\ntheta\n\n\nestimate of the additional \\(\\theta\\) parameter of the negative binomial model (if a negative binomial regression is used),\n\n\n\n\nSE.logtheta\n\n\nstandard error for \\(\\log(\\theta)\\),\n\n\n\n\nloglik\n\n\nlog-likelihood of the fitted model,\n\n\n\n\nvcov\n\n\ncovariance matrix of all coefficients in the model (derived from the Hessian of the optim output),\n\n\n\n\ndist\n\n\ncharacter string describing the count distribution used,\n\n\n\n\nlink\n\n\ncharacter string describing the link of the zero-inflation model,\n\n\n\n\nlinkinv\n\n\nthe inverse link function corresponding to link,\n\n\n\n\nconverged\n\n\nlogical indicating successful convergence of optim,\n\n\n\n\ncall\n\n\nthe original function call,\n\n\n\n\nformula\n\n\nthe original formula,\n\n\n\n\nlevels\n\n\nlevels of the categorical regressors,\n\n\n\n\ncontrasts\n\n\na list with elements “count” and “zero” containing the contrasts corresponding to levels from the respective models,\n\n\n\n\nmodel\n\n\nthe full model frame (if model = TRUE),\n\n\n\n\ny\n\n\nthe response count vector (if y = TRUE),\n\n\n\n\nx\n\n\na list with elements “count” and “zero” containing the model matrices from the respective models (if x = TRUE),\n\n\n\nCameron AC, Trivedi PK (2013). Regression Analysis of Count Data, 2nd ed. New York: Cambridge University Press.\nCameron AC, Trivedi PK (2005). Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\nLambert D (1992). “Zero-Inflated Poisson Regression, with an Application to Defects in Manufacturing”. Technometrics. 34(1), 1–14.\nZeileis A, Kleiber C, Jackman S (2008). “Regression Models for Count Data in R.” Journal of Statistical Software, 27(8), 1–25. doi:10.18637/jss.v027.i08.\n\nzeroinfl.control, glm, glm.fit, glm.nb, hurdle\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- CrabSatellites[, c(\"satellites\", \"width\", \"color\")]\ncs$color &lt;- as.numeric(cs$color)\n\n## without inflation\n## (\"satellites ~ .\" is \"satellites ~ width + color\")\nfm_pois &lt;- glm(satellites ~ ., data = cs, family = poisson)\nfm_qpois &lt;- glm(satellites ~ ., data = cs, family = quasipoisson)\nfm_nb &lt;- glm.nb(satellites ~ ., data = cs)\n\n## with simple inflation (no regressors for zero component)\nfm_zip &lt;- zeroinfl(satellites ~ . | 1, data = cs)\nfm_zinb &lt;- zeroinfl(satellites ~ . | 1, data = cs, dist = \"negbin\")\n\n## inflation with regressors\n## (\"satellites ~ . | .\" is \"satellites ~ width + color | width + color\")\nfm_zip2 &lt;- zeroinfl(satellites ~ . | ., data = cs)\nfm_zinb2 &lt;- zeroinfl(satellites ~ . | ., data = cs, dist = \"negbin\")",
    "crumbs": [
      "Zero-inflated models",
      "zeroinfl"
    ]
  },
  {
    "objectID": "man/ztpoisson.html",
    "href": "man/ztpoisson.html",
    "title": "countreg",
    "section": "",
    "text": "Family object for specification of zero-truncated Poisson models as a glm.\n\nztpoisson()\n\n\nThe ztpoisson family allows to estimate zero-truncated Poisson regression models as generalized linear models. As in the zerotrunc function, the link function is a log-link between the mean \\(\\lambda\\) of the untruncated Poisson distribution and the linear predictor. This corresponds to a non-canonical link between for the mean of the zero-truncated Poisson distribution which does not have a closed-form representation.\nNote that for new family objects ‘glm()’ estimates a dispersion parameter by default. Thus, unlike for the poisson family the dispersion parameter is not fixed, unless dispersion = 1 is set explicitly .\n\nAn object of class “family”.\n\ndztpois, poisson, zerotrunc\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- subset(CrabSatellites, subset = satellites &gt; 0)\ncs$color &lt;- as.numeric(cs$color)\n\n## model\nztp1 &lt;- glm(satellites ~ width + color, data = cs, family = ztpoisson)\nztp2 &lt;- zerotrunc(satellites ~ width + color, data = cs)\nsummary(ztp1, dispersion = 1) ## to get fixed dispersion as for poisson\n\n\nCall:\nglm(formula = satellites ~ width + color, family = ztpoisson, \n    data = cs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645491   0.872    0.383\nwidth       0.034238   0.022231   1.540    0.124\ncolor       0.007166   0.066627   0.108    0.914\n\n(Dispersion parameter for ztpoisson family taken to be 1)\n\n    Null deviance: 220.54  on 110  degrees of freedom\nResidual deviance: 218.16  on 108  degrees of freedom\nAIC: 541.09\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(ztp2)\n\n\nCall:\nzerotrunc(formula = satellites ~ width + color, data = cs)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.5409 -0.9350 -0.2051  0.6278  3.7722 \n\nCoefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007166   0.066627   0.108    0.914\n\nNumber of iterations in BFGS optimization: 6 \nLog-likelihood: -267.5 on 3 Df",
    "crumbs": [
      "Finite mixtures, boosting, GLMs",
      "ztpoisson"
    ]
  },
  {
    "objectID": "man/ztpoisson.html#family-object-for-the-zero-truncated-poisson-distribution",
    "href": "man/ztpoisson.html#family-object-for-the-zero-truncated-poisson-distribution",
    "title": "countreg",
    "section": "",
    "text": "Family object for specification of zero-truncated Poisson models as a glm.\n\nztpoisson()\n\n\nThe ztpoisson family allows to estimate zero-truncated Poisson regression models as generalized linear models. As in the zerotrunc function, the link function is a log-link between the mean \\(\\lambda\\) of the untruncated Poisson distribution and the linear predictor. This corresponds to a non-canonical link between for the mean of the zero-truncated Poisson distribution which does not have a closed-form representation.\nNote that for new family objects ‘glm()’ estimates a dispersion parameter by default. Thus, unlike for the poisson family the dispersion parameter is not fixed, unless dispersion = 1 is set explicitly .\n\nAn object of class “family”.\n\ndztpois, poisson, zerotrunc\n\n\nlibrary(\"countreg\")\n\n## data\ndata(\"CrabSatellites\", package = \"countreg\")\ncs &lt;- subset(CrabSatellites, subset = satellites &gt; 0)\ncs$color &lt;- as.numeric(cs$color)\n\n## model\nztp1 &lt;- glm(satellites ~ width + color, data = cs, family = ztpoisson)\nztp2 &lt;- zerotrunc(satellites ~ width + color, data = cs)\nsummary(ztp1, dispersion = 1) ## to get fixed dispersion as for poisson\n\n\nCall:\nglm(formula = satellites ~ width + color, family = ztpoisson, \n    data = cs)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645491   0.872    0.383\nwidth       0.034238   0.022231   1.540    0.124\ncolor       0.007166   0.066627   0.108    0.914\n\n(Dispersion parameter for ztpoisson family taken to be 1)\n\n    Null deviance: 220.54  on 110  degrees of freedom\nResidual deviance: 218.16  on 108  degrees of freedom\nAIC: 541.09\n\nNumber of Fisher Scoring iterations: 5\n\nsummary(ztp2)\n\n\nCall:\nzerotrunc(formula = satellites ~ width + color, data = cs)\n\nDeviance residuals:\n    Min      1Q  Median      3Q     Max \n-2.5409 -0.9350 -0.2051  0.6278  3.7722 \n\nCoefficients (truncated poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) 0.562699   0.645439   0.872    0.383\nwidth       0.034238   0.022227   1.540    0.123\ncolor       0.007166   0.066627   0.108    0.914\n\nNumber of iterations in BFGS optimization: 6 \nLog-likelihood: -267.5 on 3 Df",
    "crumbs": [
      "Finite mixtures, boosting, GLMs",
      "ztpoisson"
    ]
  },
  {
    "objectID": "man/predict.zeroinfl.html",
    "href": "man/predict.zeroinfl.html",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted zero-inflated regression model objects of class “zeroinfl”.\n\n## S3 method for class 'zeroinfl'\npredict(object, newdata,\n  type = c(\"mean\", \"variance\", \"quantile\", \"probability\", \"density\", \"loglikelihood\", \"parameters\", \"distribution\"),\n  model = c(\"full\", \"count\", \"zero\", \"truncated\"),\n  na.action = na.pass, at = NULL, drop = TRUE, ...)\n## S3 method for class 'zeroinfl'\nresiduals(object, type = c(\"pearson\", \"response\"), ...)\n\n## S3 method for class 'zeroinfl'\ncoef(object, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'zeroinfl'\nvcov(object, model = c(\"full\", \"count\", \"zero\"), ...)\n\n## S3 method for class 'zeroinfl'\nterms(x, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'zeroinfl'\nmodel.matrix(object, model = c(\"count\", \"zero\"), ...)\n\n\n\n\n\nobject, x\n\n\nan object of class “zeroinfl” as returned by zeroinfl.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model the terms or model matrix should be extracted.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nat\n\n\noptionally, if type = “prob”, a numeric vector at which the probabilities are evaluated. By default 0:max(y) is used where y is the original observed response.\n\n\n\n\ndrop\n\n\nlogical. Should predictions be returned in a data frame or (if possible) dropped to a vector (default).\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “zeroinfl”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix (estimated from the Hessian in the numerical optimization of the log-likelihood). As usual, the summary method returns an object of class “summary.zeroinfl” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nThe methods for coef and vcov by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting the model argument, the estimates for the corresponding model components can be extracted.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts), the predicted mean from the count component (without zero inflation) and the predicted probability for the zero component. The residuals method can compute raw residuals (observed - fitted) and Pearson residuals (raw residuals scaled by square root of variance function).\nThe terms and model.matrix extractors can be used to extract the relevant information for either component of the model.\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nzeroinfl\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm_zip &lt;- zeroinfl(satellites ~ 1 | width + color, data = CrabSatellites)\n\nplot(residuals(fm_zip) ~ fitted(fm_zip))\n\n\n\n\n\n\ncoef(fm_zip)\n\ncount_(Intercept)  zero_(Intercept)        zero_width      zero_color.L \n       1.50478077       11.93600259       -0.47633984        0.98356312 \n     zero_color.Q      zero_color.C \n       0.58845929        0.09246361 \n\ncoef(fm_zip, model = \"count\")\n\n(Intercept) \n   1.504781 \n\nsummary(fm_zip)\n\n\nCall:\nzeroinfl(formula = satellites ~ 1 | width + color, data = CrabSatellites)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.6684 -0.8202 -0.3252  0.5901  4.4682 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.50478    0.04556   33.02   &lt;2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.93600    2.81853   4.235 2.29e-05 ***\nwidth       -0.47634    0.10868  -4.383 1.17e-05 ***\ncolor.L      0.98356    0.59803   1.645    0.100    \ncolor.Q      0.58846    0.48886   1.204    0.229    \ncolor.C      0.09246    0.34567   0.267    0.789    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -362.6 on 6 Df\n\nlogLik(fm_zip)\n\n'log Lik.' -362.5605 (df=6)",
    "crumbs": [
      "Zero-inflated models",
      "predict.zeroinfl"
    ]
  },
  {
    "objectID": "man/predict.zeroinfl.html#methods-for-zeroinfl-objects",
    "href": "man/predict.zeroinfl.html#methods-for-zeroinfl-objects",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted zero-inflated regression model objects of class “zeroinfl”.\n\n## S3 method for class 'zeroinfl'\npredict(object, newdata,\n  type = c(\"mean\", \"variance\", \"quantile\", \"probability\", \"density\", \"loglikelihood\", \"parameters\", \"distribution\"),\n  model = c(\"full\", \"count\", \"zero\", \"truncated\"),\n  na.action = na.pass, at = NULL, drop = TRUE, ...)\n## S3 method for class 'zeroinfl'\nresiduals(object, type = c(\"pearson\", \"response\"), ...)\n\n## S3 method for class 'zeroinfl'\ncoef(object, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'zeroinfl'\nvcov(object, model = c(\"full\", \"count\", \"zero\"), ...)\n\n## S3 method for class 'zeroinfl'\nterms(x, model = c(\"full\", \"count\", \"zero\"), ...)\n## S3 method for class 'zeroinfl'\nmodel.matrix(object, model = c(\"count\", \"zero\"), ...)\n\n\n\n\n\nobject, x\n\n\nan object of class “zeroinfl” as returned by zeroinfl.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model the terms or model matrix should be extracted.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nat\n\n\noptionally, if type = “prob”, a numeric vector at which the probabilities are evaluated. By default 0:max(y) is used where y is the original observed response.\n\n\n\n\ndrop\n\n\nlogical. Should predictions be returned in a data frame or (if possible) dropped to a vector (default).\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “zeroinfl”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix (estimated from the Hessian in the numerical optimization of the log-likelihood). As usual, the summary method returns an object of class “summary.zeroinfl” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nThe methods for coef and vcov by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting the model argument, the estimates for the corresponding model components can be extracted.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts), the predicted mean from the count component (without zero inflation) and the predicted probability for the zero component. The residuals method can compute raw residuals (observed - fitted) and Pearson residuals (raw residuals scaled by square root of variance function).\nThe terms and model.matrix extractors can be used to extract the relevant information for either component of the model.\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nzeroinfl\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm_zip &lt;- zeroinfl(satellites ~ 1 | width + color, data = CrabSatellites)\n\nplot(residuals(fm_zip) ~ fitted(fm_zip))\n\n\n\n\n\n\ncoef(fm_zip)\n\ncount_(Intercept)  zero_(Intercept)        zero_width      zero_color.L \n       1.50478077       11.93600259       -0.47633984        0.98356312 \n     zero_color.Q      zero_color.C \n       0.58845929        0.09246361 \n\ncoef(fm_zip, model = \"count\")\n\n(Intercept) \n   1.504781 \n\nsummary(fm_zip)\n\n\nCall:\nzeroinfl(formula = satellites ~ 1 | width + color, data = CrabSatellites)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.6684 -0.8202 -0.3252  0.5901  4.4682 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.50478    0.04556   33.02   &lt;2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.93600    2.81853   4.235 2.29e-05 ***\nwidth       -0.47634    0.10868  -4.383 1.17e-05 ***\ncolor.L      0.98356    0.59803   1.645    0.100    \ncolor.Q      0.58846    0.48886   1.204    0.229    \ncolor.C      0.09246    0.34567   0.267    0.789    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -362.6 on 6 Df\n\nlogLik(fm_zip)\n\n'log Lik.' -362.5605 (df=6)",
    "crumbs": [
      "Zero-inflated models",
      "predict.zeroinfl"
    ]
  },
  {
    "objectID": "man/MBnegbin.html",
    "href": "man/MBnegbin.html",
    "title": "countreg",
    "section": "",
    "text": "Family generators for model-based boosting of count data regressions using mboost.\n\nMBbinomial(link = \"logit\")\n\nMBnegbin(theta = NULL, link = \"log\",\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\nMBztpoisson(link = \"log\",\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\nMBztnegbin(theta = NULL, link = \"log\",\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\n\n\n\n\nlink\n\n\ncharacter or object of class “link-glm” for the link function linking the expectation and the predictor.\n\n\n\n\ntheta\n\n\nnumeric or NULL. Value of the theta parameter of the negative binomial model. If NULL, theta is estimated along with the regression coefficients.\n\n\n\n\ncontrol\n\n\nlist with control parameters passed to optim.\n\n\n\nThe family generators MBbinomial, MBnegbin, MBztpoisson, MBztnegbin enable boosting of binary regressions, negative binomial count regressions, zero-truncated Poisson count regressions, and zero-truncated negative binomial count regressions, respectively. Family MBbinomial is comparable to Binomial but supports any link function (not just logit and probit). Family MBnegbin is comparable to NBinomial but is typically much faster because the nuisance parameter theta is estimated using analytical gradients (via optim) and setting better starting values. MBztpoisson and MBztnegbin enable zero-truncated Poisson and negative binomial regressions so that also the count parts of hurdle models can be easily estimated.\n\nAn object of class boost_family_glm.\n\nmboost, glmboost, gamboost, Binomial, Poisson, NBinomial\n\n\nlibrary(\"countreg\")\n\n### Negative binomial regression for CrabSatellites ----------------------------\n\nif(require(\"mboost\")) {\n## crab satellite data using ordered factors as numeric\ndata(\"CrabSatellites\", package = \"countreg\")\nCrabSatellites &lt;- transform(CrabSatellites,\n  color = as.numeric(color),\n  spine = as.numeric(spine)\n)  \n\n## comparison of ML and boosting with NBinomial() vs. MBnegbin()\nsystem.time(m0 &lt;- glm.nb(satellites ~ width + color, data = CrabSatellites))\nsystem.time(m1 &lt;- glmboost(satellites ~ width + color, data = CrabSatellites,\n  family = NBinomial(), control = boost_control(mstop = 500)))\nsystem.time(m2 &lt;- glmboost(satellites ~ width + color, data = CrabSatellites,\n  family = MBnegbin(), control = boost_control(mstop = 500)))\n## note that mstop is _not_ tuned here to (ab)use mboost to get the ML estimator\n\n## compare coefficients\ncbind(c(coef(m0), \"theta\" = m0$theta),\n  c(coef(m1, off2int = TRUE, which = \"\"), nuisance(m1)),\n  c(coef(m1, off2int = TRUE, which = \"\"), nuisance(m1))\n)\n}\n\n### Hurdle regression for CrabSatellites using spline terms --------------------\n\n\nif(require(\"mboost\")) {\n## ML estimation\ng &lt;- hurdle(satellites ~ width + color, data = CrabSatellites, dist = \"negbin\")\nsummary(g)\n\n## boosting of zero hurdle\ng0 &lt;- gamboost(factor(satellites &gt; 0) ~ bbs(width) + bbs(color, knots = 3),\n  data = CrabSatellites, family = MBbinomial())\nset.seed(0)\ng0cv &lt;- cvrisk(g0)\ng0[mstop(g0cv)]\n\n## boosting of count regression\ng1 &lt;- gamboost(satellites ~ bbs(width) + bbs(color, knots = 3),\n  data = subset(CrabSatellites, satellites &gt; 0), family = MBztnegbin())\nset.seed(1)\ng1cv &lt;- cvrisk(g1)\ng1[mstop(g1cv)]\n\npar(mfrow = c(1, 2))\n\n## optimal mstop values\nplot(g0cv)\nplot(g1cv)\n## -&gt; no effects in covariates for count part\n\n## partial effects in zero hurdle\nplot(g0)\n## -&gt; large effect of width, moderate effect of color with\n## width effect almost linear\n}\n\n\n### Hurdle regression for RecreationDemand using linear terms ------------------\n\nlibrary(\"mboost\")\ndata(\"RecreationDemand\", package = \"AER\")\n\n### Zero hurdle ##\n\n## ML vs. boosting\nz0 &lt;- glm(factor(trips &gt; 0) ~ ., data = RecreationDemand, family = binomial)\nz1 &lt;- glmboost(factor(trips &gt; 0) ~ ., data = RecreationDemand, family = MBbinomial(),\n  control = boost_control(mstop = 5000))\nplot(z1)\n\n## tune mstop\nset.seed(0)\nz1cv &lt;- cvrisk(z1)\nz1cv\nplot(z1cv)\n## very flat (presumably due to separation?)\n## -&gt; stop earlier manually\nz1[3000]\n\n## compare coefficients\ncbind(coef(z0), coef(z1, off2int = TRUE, which = \"\"))\n## -&gt; some shrunken entirely to zero,\n## coefficient of variable with separation (userfee) shrunken considerably\n\n\n### Count (zero-truncated)\n\n## ML and boosting count part\nc0 &lt;- zerotrunc(trips ~ ., data = subset(RecreationDemand, trips &gt; 0), dist = \"negbin\")\nc1 &lt;- glmboost(trips ~ ., data = subset(RecreationDemand, trips &gt; 0),\n  family = MBztnegbin(), control = boost_control(mstop = 5000))\nplot(c1)\n\n## tune mstop\nset.seed(0)\nc1cv &lt;- cvrisk(c1)\nc1cv\nplot(c1cv)\n\n## use mstop from cvrisk\nc1[mstop(c1cv)]\n\n## compare coefficients\ncbind(c(coef(c0), \"theta\" = c0$theta),\n  c(coef(c1, off2int = TRUE, which = \"\"), nuisance(c1)))\n## -&gt; similar",
    "crumbs": [
      "Finite mixtures, boosting, GLMs",
      "MBnegbin"
    ]
  },
  {
    "objectID": "man/MBnegbin.html#mboost-families-for-binary-zero-truncated-negative-binomial-and-zero-truncated-poisson-regression",
    "href": "man/MBnegbin.html#mboost-families-for-binary-zero-truncated-negative-binomial-and-zero-truncated-poisson-regression",
    "title": "countreg",
    "section": "",
    "text": "Family generators for model-based boosting of count data regressions using mboost.\n\nMBbinomial(link = \"logit\")\n\nMBnegbin(theta = NULL, link = \"log\",\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\nMBztpoisson(link = \"log\",\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\nMBztnegbin(theta = NULL, link = \"log\",\n  control = list(reltol = .Machine\\$double.eps^(1/1.5), maxit = 500))\n\n\n\n\n\nlink\n\n\ncharacter or object of class “link-glm” for the link function linking the expectation and the predictor.\n\n\n\n\ntheta\n\n\nnumeric or NULL. Value of the theta parameter of the negative binomial model. If NULL, theta is estimated along with the regression coefficients.\n\n\n\n\ncontrol\n\n\nlist with control parameters passed to optim.\n\n\n\nThe family generators MBbinomial, MBnegbin, MBztpoisson, MBztnegbin enable boosting of binary regressions, negative binomial count regressions, zero-truncated Poisson count regressions, and zero-truncated negative binomial count regressions, respectively. Family MBbinomial is comparable to Binomial but supports any link function (not just logit and probit). Family MBnegbin is comparable to NBinomial but is typically much faster because the nuisance parameter theta is estimated using analytical gradients (via optim) and setting better starting values. MBztpoisson and MBztnegbin enable zero-truncated Poisson and negative binomial regressions so that also the count parts of hurdle models can be easily estimated.\n\nAn object of class boost_family_glm.\n\nmboost, glmboost, gamboost, Binomial, Poisson, NBinomial\n\n\nlibrary(\"countreg\")\n\n### Negative binomial regression for CrabSatellites ----------------------------\n\nif(require(\"mboost\")) {\n## crab satellite data using ordered factors as numeric\ndata(\"CrabSatellites\", package = \"countreg\")\nCrabSatellites &lt;- transform(CrabSatellites,\n  color = as.numeric(color),\n  spine = as.numeric(spine)\n)  \n\n## comparison of ML and boosting with NBinomial() vs. MBnegbin()\nsystem.time(m0 &lt;- glm.nb(satellites ~ width + color, data = CrabSatellites))\nsystem.time(m1 &lt;- glmboost(satellites ~ width + color, data = CrabSatellites,\n  family = NBinomial(), control = boost_control(mstop = 500)))\nsystem.time(m2 &lt;- glmboost(satellites ~ width + color, data = CrabSatellites,\n  family = MBnegbin(), control = boost_control(mstop = 500)))\n## note that mstop is _not_ tuned here to (ab)use mboost to get the ML estimator\n\n## compare coefficients\ncbind(c(coef(m0), \"theta\" = m0$theta),\n  c(coef(m1, off2int = TRUE, which = \"\"), nuisance(m1)),\n  c(coef(m1, off2int = TRUE, which = \"\"), nuisance(m1))\n)\n}\n\n### Hurdle regression for CrabSatellites using spline terms --------------------\n\n\nif(require(\"mboost\")) {\n## ML estimation\ng &lt;- hurdle(satellites ~ width + color, data = CrabSatellites, dist = \"negbin\")\nsummary(g)\n\n## boosting of zero hurdle\ng0 &lt;- gamboost(factor(satellites &gt; 0) ~ bbs(width) + bbs(color, knots = 3),\n  data = CrabSatellites, family = MBbinomial())\nset.seed(0)\ng0cv &lt;- cvrisk(g0)\ng0[mstop(g0cv)]\n\n## boosting of count regression\ng1 &lt;- gamboost(satellites ~ bbs(width) + bbs(color, knots = 3),\n  data = subset(CrabSatellites, satellites &gt; 0), family = MBztnegbin())\nset.seed(1)\ng1cv &lt;- cvrisk(g1)\ng1[mstop(g1cv)]\n\npar(mfrow = c(1, 2))\n\n## optimal mstop values\nplot(g0cv)\nplot(g1cv)\n## -&gt; no effects in covariates for count part\n\n## partial effects in zero hurdle\nplot(g0)\n## -&gt; large effect of width, moderate effect of color with\n## width effect almost linear\n}\n\n\n### Hurdle regression for RecreationDemand using linear terms ------------------\n\nlibrary(\"mboost\")\ndata(\"RecreationDemand\", package = \"AER\")\n\n### Zero hurdle ##\n\n## ML vs. boosting\nz0 &lt;- glm(factor(trips &gt; 0) ~ ., data = RecreationDemand, family = binomial)\nz1 &lt;- glmboost(factor(trips &gt; 0) ~ ., data = RecreationDemand, family = MBbinomial(),\n  control = boost_control(mstop = 5000))\nplot(z1)\n\n## tune mstop\nset.seed(0)\nz1cv &lt;- cvrisk(z1)\nz1cv\nplot(z1cv)\n## very flat (presumably due to separation?)\n## -&gt; stop earlier manually\nz1[3000]\n\n## compare coefficients\ncbind(coef(z0), coef(z1, off2int = TRUE, which = \"\"))\n## -&gt; some shrunken entirely to zero,\n## coefficient of variable with separation (userfee) shrunken considerably\n\n\n### Count (zero-truncated)\n\n## ML and boosting count part\nc0 &lt;- zerotrunc(trips ~ ., data = subset(RecreationDemand, trips &gt; 0), dist = \"negbin\")\nc1 &lt;- glmboost(trips ~ ., data = subset(RecreationDemand, trips &gt; 0),\n  family = MBztnegbin(), control = boost_control(mstop = 5000))\nplot(c1)\n\n## tune mstop\nset.seed(0)\nc1cv &lt;- cvrisk(c1)\nc1cv\nplot(c1cv)\n\n## use mstop from cvrisk\nc1[mstop(c1cv)]\n\n## compare coefficients\ncbind(c(coef(c0), \"theta\" = c0$theta),\n  c(coef(c1, off2int = TRUE, which = \"\"), nuisance(c1)))\n## -&gt; similar",
    "crumbs": [
      "Finite mixtures, boosting, GLMs",
      "MBnegbin"
    ]
  },
  {
    "objectID": "man/predict.nbreg.html",
    "href": "man/predict.nbreg.html",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted negative binomial count regression model objects of class “nbreg”.\n\n## S3 method for class 'nbreg'\npredict(object, newdata,\n  type = c(\"response\", \"prob\", \"theta\", \"parameters\"), na.action = na.pass, ...)\n## S3 method for class 'nbreg'\nresiduals(object, type = c(\"pearson\", \"deviance\", \"response\"), ...)\n\n## S3 method for class 'nbreg'\ncoef(object, model = c(\"full\", \"mu\", \"theta\"), ...)\n## S3 method for class 'nbreg'\nvcov(object, model = c(\"full\", \"mu\", \"theta\"), ...)\n\n## S3 method for class 'nbreg'\nterms(x, model = c(\"full\", \"mu\", \"theta\"), ...)\n## S3 method for class 'nbreg'\nmodel.matrix(object, model = c(\"mu\", \"theta\"), ...)\n\n\n\n\n\nobject, x\n\n\nan object of class “nbreg” as returned by nbreg.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model the terms or model matrix should be extracted.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “nbreg”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix. As usual, the summary method returns an object of class “summary.nbreg” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nThe methods for coef and vcov by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting the model argument, the estimates for the corresponding model component can be extracted.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts) and the predicted dispersion parameter theta. The residuals method can compute raw residuals\n(observed - fitted), Pearson residuals (raw residuals scaled by square root of variance function), and deviance residuals. The latter are only supported for negative binomial type 2 models (dist = NB2) (includes NBH).\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nnbreg\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm &lt;- nbreg(satellites ~ width + color, data = CrabSatellites)\n\nplot(residuals(fm, type = \"pearson\") ~ fitted(fm))\n\n\n\n\n\n\ncoef(fm)\n\n   mu_(Intercept)          mu_width        mu_color.L        mu_color.Q \n      -3.68546352        0.17839042       -0.41422641        0.13011660 \n       mu_color.C theta_(Intercept) \n       0.04408676       -0.07031756 \n\nsummary(fm)\n\n\nCall:\nnbreg(formula = satellites ~ width + color, data = CrabSatellites)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.8886 -0.7714 -0.2416  0.5114  4.2830 \n\nCoefficients (NB2 with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.68546    1.26893  -2.904 0.003680 ** \nwidth        0.17839    0.04807   3.711 0.000206 ***\ncolor.L     -0.41423    0.29429  -1.408 0.159266    \ncolor.Q      0.13012    0.24244   0.537 0.591484    \ncolor.C      0.04409    0.17887   0.246 0.805320    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTheta coefficients (log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.07032    0.18028   -0.39    0.697\n\nNumber of iterations in BFGS optimization: 11 \nLog-likelihood: -374.3 on 6 Df\n\nlogLik(fm)\n\n'log Lik.' -374.2979 (df=6)\n\nAIC(fm)\n\n[1] 760.5958",
    "crumbs": [
      "Negative binomial regression",
      "predict.nbreg"
    ]
  },
  {
    "objectID": "man/predict.nbreg.html#methods-for-nbreg-objects",
    "href": "man/predict.nbreg.html#methods-for-nbreg-objects",
    "title": "countreg",
    "section": "",
    "text": "Methods for extracting information from fitted negative binomial count regression model objects of class “nbreg”.\n\n## S3 method for class 'nbreg'\npredict(object, newdata,\n  type = c(\"response\", \"prob\", \"theta\", \"parameters\"), na.action = na.pass, ...)\n## S3 method for class 'nbreg'\nresiduals(object, type = c(\"pearson\", \"deviance\", \"response\"), ...)\n\n## S3 method for class 'nbreg'\ncoef(object, model = c(\"full\", \"mu\", \"theta\"), ...)\n## S3 method for class 'nbreg'\nvcov(object, model = c(\"full\", \"mu\", \"theta\"), ...)\n\n## S3 method for class 'nbreg'\nterms(x, model = c(\"full\", \"mu\", \"theta\"), ...)\n## S3 method for class 'nbreg'\nmodel.matrix(object, model = c(\"mu\", \"theta\"), ...)\n\n\n\n\n\nobject, x\n\n\nan object of class “nbreg” as returned by nbreg.\n\n\n\n\nnewdata\n\n\noptionally, a data frame in which to look for variables with which to predict. If omitted, the original observations are used.\n\n\n\n\ntype\n\n\ncharacter specifying the type of predictions or residuals, respectively. For details see below.\n\n\n\n\nna.action\n\n\nfunction determining what should be done with missing values in newdata. The default is to predict NA.\n\n\n\n\nmodel\n\n\ncharacter specifying for which component of the model the terms or model matrix should be extracted.\n\n\n\n\n…\n\n\ncurrently not used.\n\n\n\nA set of standard extractor functions for fitted model objects is available for objects of class “nbreg”, including methods to the generic functions print and summary which print the estimated coefficients along with some further information. The summary in particular supplies partial Wald tests based on the coefficients and the covariance matrix. As usual, the summary method returns an object of class “summary.nbreg” containing the relevant summary statistics which can subsequently be printed using the associated print method.\nThe methods for coef and vcov by default return a single vector of coefficients and their associated covariance matrix, respectively, i.e., all coefficients are concatenated. By setting the model argument, the estimates for the corresponding model component can be extracted.\nBoth the fitted and predict methods can compute fitted responses. The latter additionally provides the predicted density (i.e., probabilities for the observed counts) and the predicted dispersion parameter theta. The residuals method can compute raw residuals\n(observed - fitted), Pearson residuals (raw residuals scaled by square root of variance function), and deviance residuals. The latter are only supported for negative binomial type 2 models (dist = NB2) (includes NBH).\nA logLik method is provided, hence AIC can be called to compute information criteria.\n\nnbreg\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nfm &lt;- nbreg(satellites ~ width + color, data = CrabSatellites)\n\nplot(residuals(fm, type = \"pearson\") ~ fitted(fm))\n\n\n\n\n\n\ncoef(fm)\n\n   mu_(Intercept)          mu_width        mu_color.L        mu_color.Q \n      -3.68546352        0.17839042       -0.41422641        0.13011660 \n       mu_color.C theta_(Intercept) \n       0.04408676       -0.07031756 \n\nsummary(fm)\n\n\nCall:\nnbreg(formula = satellites ~ width + color, data = CrabSatellites)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-0.8886 -0.7714 -0.2416  0.5114  4.2830 \n\nCoefficients (NB2 with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.68546    1.26893  -2.904 0.003680 ** \nwidth        0.17839    0.04807   3.711 0.000206 ***\ncolor.L     -0.41423    0.29429  -1.408 0.159266    \ncolor.Q      0.13012    0.24244   0.537 0.591484    \ncolor.C      0.04409    0.17887   0.246 0.805320    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTheta coefficients (log link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.07032    0.18028   -0.39    0.697\n\nNumber of iterations in BFGS optimization: 11 \nLog-likelihood: -374.3 on 6 Df\n\nlogLik(fm)\n\n'log Lik.' -374.2979 (df=6)\n\nAIC(fm)\n\n[1] 760.5958",
    "crumbs": [
      "Negative binomial regression",
      "predict.nbreg"
    ]
  },
  {
    "objectID": "man/CrabSatellites.html",
    "href": "man/CrabSatellites.html",
    "title": "countreg",
    "section": "",
    "text": "Determinants for male satellites to nesting horseshoe crabs.\n\ndata(\"CrabSatellites\")\n\nA data frame containing 173 observations on 5 variables.\n\n\ncolor\n\n\nOrdered factor indicating color (light medium, medium, dark medium, dark).\n\n\nspine\n\n\nOrdered factor indicating spine condition (both good, one worn or broken, both worn or broken).\n\n\nwidth\n\n\nCarapace width (cm).\n\n\nweight\n\n\nWeight (kg).\n\n\nsatellites\n\n\nNumber of satellites.\n\n\nBrockmann (1996) investigates horshoe crab mating. The crabs arrive on the beach in pairs to spawn. Furthermore, unattached males also come to the beach, crowd around the nesting couples and compete with attached males for fertilizations. These so-called satellite males form large groups around some couples while ignoring others. Brockmann (1996) shows that the groupings are not driven by environmental factors but by properties of the nesting female crabs. Larger females that are in better condition attract more satellites.\nAgresti (2002, 2013) reanalyzes the number of satellites using count models. Explanatory variables are the female crab’s color, spine condition, weight, and carapace width. Color and spine condition are ordered factors but are treated as numeric in some analyses.\n\nTable 4.3 in Agresti (2002).\n\nAgresti A (2002). Categorical Data Analysis, 2nd ed., John Wiley & Sons, Hoboken.\nAgresti A (2013). Categorical Data Analysis, 3rd ed., John Wiley & Sons, Hoboken.\nBrockmann HJ (1996). “Satellite Male Groups in Horseshoe Crabs, Limulus polyphemus”, Ethology, 102(1), 1–21.\n\n\nlibrary(\"countreg\")\n\n## load data, use ordered factors as numeric, and\n## grouped factor version of width\ndata(\"CrabSatellites\", package = \"countreg\")\nCrabSatellites &lt;- transform(CrabSatellites,\n  color = as.numeric(color),\n  spine = as.numeric(spine),\n  cwidth = cut(width, c(-Inf, seq(23.25, 29.25), Inf))\n)\n\n## Agresti, Table 4.4\naggregate(CrabSatellites$satellites, list(CrabSatellites$cwidth), function(x)\n  round(c(Number = length(x), Sum = sum(x), Mean = mean(x), Var = var(x)), digits = 2))\n\n      Group.1 x.Number  x.Sum x.Mean  x.Var\n1 (-Inf,23.2]    14.00  14.00   1.00   2.77\n2 (23.2,24.2]    14.00  20.00   1.43   8.88\n3 (24.2,25.2]    28.00  67.00   2.39   6.54\n4 (25.2,26.2]    39.00 105.00   2.69  11.38\n5 (26.2,27.2]    22.00  63.00   2.86   6.89\n6 (27.2,28.2]    24.00  93.00   3.88   8.81\n7 (28.2,29.2]    18.00  71.00   3.94  16.88\n8 (29.2, Inf]    14.00  72.00   5.14   8.29\n\n## Agresti, Figure 4.4\nplot(tapply(satellites, cwidth, mean) ~ tapply(width, cwidth, mean),\n  data = CrabSatellites, ylim = c(0, 6), pch = 19)\n\n\n\n\n\n\n## alternatively: exploratory displays for hurdle (= 0 vs. &gt; 0) and counts (&gt; 0)\npar(mfrow = c(2, 2))\nplot(factor(satellites == 0) ~ width, data = CrabSatellites, breaks = seq(20, 33.5, by = 1.5))\nplot(factor(satellites == 0) ~ color, data = CrabSatellites, breaks = 1:5 - 0.5)\nplot(jitter(satellites) ~ width, data = CrabSatellites, subset = satellites &gt; 0, log = \"y\")\nplot(jitter(satellites) ~ factor(color), data = CrabSatellites, subset = satellites &gt; 0, log = \"y\")\n\n\n\n\n\n\n## count data models\ncs_p    &lt;-    glm(satellites ~ width + color, data = CrabSatellites, family = poisson)\ncs_nb   &lt;- glm.nb(satellites ~ width + color, data = CrabSatellites)\ncs_hp   &lt;- hurdle(satellites ~ width + color, data = CrabSatellites, dist = \"poisson\")\ncs_hnb  &lt;- hurdle(satellites ~ width + color, data = CrabSatellites, dist = \"negbin\")\ncs_hnb2 &lt;- hurdle(satellites ~ 1 | width + color, data = CrabSatellites, dist = \"negbin\")\nAIC(cs_p, cs_nb, cs_hp, cs_hnb, cs_hnb2)\n\n        df      AIC\ncs_p     3 921.4990\ncs_nb    4 756.9323\ncs_hp    6 736.2160\ncs_hnb   7 714.7255\ncs_hnb2  5 712.0658\n\nBIC(cs_p, cs_nb, cs_hp, cs_hnb, cs_hnb2)\n\n        df      BIC\ncs_p     3 930.9589\ncs_nb    4 769.5455\ncs_hp    6 755.1358\ncs_hnb   7 736.7985\ncs_hnb2  5 727.8323\n\n## rootograms\nif(require(\"topmodels\")) {\npar(mfrow = c(2, 2))\nr_p   &lt;- rootogram(cs_p,   xlim = c(0, 15), main = \"Poisson\")\nr_nb  &lt;- rootogram(cs_nb,  xlim = c(0, 15), main = \"Negative Binomial\")\nr_hp  &lt;- rootogram(cs_hp,  xlim = c(0, 15), main = \"Hurdle Poisson\")\nr_hnb &lt;- rootogram(cs_hnb, xlim = c(0, 15), main = \"Hurdle Negative Binomial\")\n}\n\n\n\n\n\n\n## fitted curves\npar(mfrow = c(1, 1))\nplot(jitter(satellites) ~ width, data = CrabSatellites)\nnd &lt;- data.frame(width = 20:34, color = 2)\npred &lt;- function(m) predict(m, newdata = nd, type = \"response\")\ncs_ag &lt;- glm(satellites ~ width, data = CrabSatellites, family = poisson(link = \"identity\"),\n  start = coef(lm(satellites ~ width, data = CrabSatellites)))\nlines(pred(cs_ag)   ~ width, data = nd, col = 2, lwd = 1.5)\nlines(pred(cs_p)    ~ width, data = nd, col = 3, lwd = 1.5)\nlines(pred(cs_hnb)  ~ width, data = nd, col = 4, lwd = 1.5)\nlines(pred(cs_hnb2) ~ width, data = nd, col = 4, lwd = 1.5, lty = 2)\nlegend(\"topleft\", c(\"Hurdle NB\", \"Hurdle NB 2\", \"Poisson (id)\", \"Poisson (log)\"),\n  col = c(4, 4, 2, 3), lty = c(1, 2, 1, 1), lwd = 1.5, bty = \"n\")\n\n\n\n\n\n\n## alternative displays: Q-Q residuals plot, barplot, residuals vs. fitted\nif(require(\"topmodels\")) {\npar(mfrow= c(3, 2))\nqqrplot(cs_p, range = c(0.05, 0.95), main = \"Q-Q residuals plot: Poisson\")\nqqrplot(cs_hnb, range = c(0.05, 0.95), main = \"Q-Q residuals plot: Hurdle NB\")\n} else {\npar(mfrow= c(2, 2))\n}\n\nbarplot(t(matrix(c(r_p$observed, r_p$expected), ncol = 2,\n  dimnames = list(r_p$x, c(\"Observed\", \"Expected\")))),\n  beside = TRUE, main = \"Barplot: Poisson\",\n  xlab = \"satellites\", ylab = \"Frequency\",\n  legend.text = TRUE, args.legend = list(x = \"topright\", bty = \"n\"))\nbarplot(t(matrix(c(r_hnb$observed, r_hnb$expected), ncol = 2,\n  dimnames = list(r_hnb$x, c(\"Observed\", \"Expected\")))),\n  beside = TRUE, main = \"Barplot: Hurdle NB\",\n  xlab = \"satellites\", ylab = \"Frequency\",\n  legend.text = TRUE, args.legend = list(x = \"topright\", bty = \"n\"))\n\nplot(predict(cs_p, type = \"response\"),\n  residuals(cs_p, type = \"pearson\"),\n  xlab = \"Fitted values\", ylab = \"Pearson residuals\",\n  main = \"Residuals vs. fitted: Poisson\")\nplot(predict(cs_hnb, type = \"response\"),\n  residuals(cs_hnb, type = \"pearson\"),\n  xlab = \"Fitted values\", ylab = \"Pearson residuals\",\n  main = \"Residuals vs. fitted: Hurdle NB\")",
    "crumbs": [
      "Data sets",
      "CrabSatellites"
    ]
  },
  {
    "objectID": "man/CrabSatellites.html#horseshoe-crab-mating",
    "href": "man/CrabSatellites.html#horseshoe-crab-mating",
    "title": "countreg",
    "section": "",
    "text": "Determinants for male satellites to nesting horseshoe crabs.\n\ndata(\"CrabSatellites\")\n\nA data frame containing 173 observations on 5 variables.\n\n\ncolor\n\n\nOrdered factor indicating color (light medium, medium, dark medium, dark).\n\n\nspine\n\n\nOrdered factor indicating spine condition (both good, one worn or broken, both worn or broken).\n\n\nwidth\n\n\nCarapace width (cm).\n\n\nweight\n\n\nWeight (kg).\n\n\nsatellites\n\n\nNumber of satellites.\n\n\nBrockmann (1996) investigates horshoe crab mating. The crabs arrive on the beach in pairs to spawn. Furthermore, unattached males also come to the beach, crowd around the nesting couples and compete with attached males for fertilizations. These so-called satellite males form large groups around some couples while ignoring others. Brockmann (1996) shows that the groupings are not driven by environmental factors but by properties of the nesting female crabs. Larger females that are in better condition attract more satellites.\nAgresti (2002, 2013) reanalyzes the number of satellites using count models. Explanatory variables are the female crab’s color, spine condition, weight, and carapace width. Color and spine condition are ordered factors but are treated as numeric in some analyses.\n\nTable 4.3 in Agresti (2002).\n\nAgresti A (2002). Categorical Data Analysis, 2nd ed., John Wiley & Sons, Hoboken.\nAgresti A (2013). Categorical Data Analysis, 3rd ed., John Wiley & Sons, Hoboken.\nBrockmann HJ (1996). “Satellite Male Groups in Horseshoe Crabs, Limulus polyphemus”, Ethology, 102(1), 1–21.\n\n\nlibrary(\"countreg\")\n\n## load data, use ordered factors as numeric, and\n## grouped factor version of width\ndata(\"CrabSatellites\", package = \"countreg\")\nCrabSatellites &lt;- transform(CrabSatellites,\n  color = as.numeric(color),\n  spine = as.numeric(spine),\n  cwidth = cut(width, c(-Inf, seq(23.25, 29.25), Inf))\n)\n\n## Agresti, Table 4.4\naggregate(CrabSatellites$satellites, list(CrabSatellites$cwidth), function(x)\n  round(c(Number = length(x), Sum = sum(x), Mean = mean(x), Var = var(x)), digits = 2))\n\n      Group.1 x.Number  x.Sum x.Mean  x.Var\n1 (-Inf,23.2]    14.00  14.00   1.00   2.77\n2 (23.2,24.2]    14.00  20.00   1.43   8.88\n3 (24.2,25.2]    28.00  67.00   2.39   6.54\n4 (25.2,26.2]    39.00 105.00   2.69  11.38\n5 (26.2,27.2]    22.00  63.00   2.86   6.89\n6 (27.2,28.2]    24.00  93.00   3.88   8.81\n7 (28.2,29.2]    18.00  71.00   3.94  16.88\n8 (29.2, Inf]    14.00  72.00   5.14   8.29\n\n## Agresti, Figure 4.4\nplot(tapply(satellites, cwidth, mean) ~ tapply(width, cwidth, mean),\n  data = CrabSatellites, ylim = c(0, 6), pch = 19)\n\n\n\n\n\n\n## alternatively: exploratory displays for hurdle (= 0 vs. &gt; 0) and counts (&gt; 0)\npar(mfrow = c(2, 2))\nplot(factor(satellites == 0) ~ width, data = CrabSatellites, breaks = seq(20, 33.5, by = 1.5))\nplot(factor(satellites == 0) ~ color, data = CrabSatellites, breaks = 1:5 - 0.5)\nplot(jitter(satellites) ~ width, data = CrabSatellites, subset = satellites &gt; 0, log = \"y\")\nplot(jitter(satellites) ~ factor(color), data = CrabSatellites, subset = satellites &gt; 0, log = \"y\")\n\n\n\n\n\n\n## count data models\ncs_p    &lt;-    glm(satellites ~ width + color, data = CrabSatellites, family = poisson)\ncs_nb   &lt;- glm.nb(satellites ~ width + color, data = CrabSatellites)\ncs_hp   &lt;- hurdle(satellites ~ width + color, data = CrabSatellites, dist = \"poisson\")\ncs_hnb  &lt;- hurdle(satellites ~ width + color, data = CrabSatellites, dist = \"negbin\")\ncs_hnb2 &lt;- hurdle(satellites ~ 1 | width + color, data = CrabSatellites, dist = \"negbin\")\nAIC(cs_p, cs_nb, cs_hp, cs_hnb, cs_hnb2)\n\n        df      AIC\ncs_p     3 921.4990\ncs_nb    4 756.9323\ncs_hp    6 736.2160\ncs_hnb   7 714.7255\ncs_hnb2  5 712.0658\n\nBIC(cs_p, cs_nb, cs_hp, cs_hnb, cs_hnb2)\n\n        df      BIC\ncs_p     3 930.9589\ncs_nb    4 769.5455\ncs_hp    6 755.1358\ncs_hnb   7 736.7985\ncs_hnb2  5 727.8323\n\n## rootograms\nif(require(\"topmodels\")) {\npar(mfrow = c(2, 2))\nr_p   &lt;- rootogram(cs_p,   xlim = c(0, 15), main = \"Poisson\")\nr_nb  &lt;- rootogram(cs_nb,  xlim = c(0, 15), main = \"Negative Binomial\")\nr_hp  &lt;- rootogram(cs_hp,  xlim = c(0, 15), main = \"Hurdle Poisson\")\nr_hnb &lt;- rootogram(cs_hnb, xlim = c(0, 15), main = \"Hurdle Negative Binomial\")\n}\n\n\n\n\n\n\n## fitted curves\npar(mfrow = c(1, 1))\nplot(jitter(satellites) ~ width, data = CrabSatellites)\nnd &lt;- data.frame(width = 20:34, color = 2)\npred &lt;- function(m) predict(m, newdata = nd, type = \"response\")\ncs_ag &lt;- glm(satellites ~ width, data = CrabSatellites, family = poisson(link = \"identity\"),\n  start = coef(lm(satellites ~ width, data = CrabSatellites)))\nlines(pred(cs_ag)   ~ width, data = nd, col = 2, lwd = 1.5)\nlines(pred(cs_p)    ~ width, data = nd, col = 3, lwd = 1.5)\nlines(pred(cs_hnb)  ~ width, data = nd, col = 4, lwd = 1.5)\nlines(pred(cs_hnb2) ~ width, data = nd, col = 4, lwd = 1.5, lty = 2)\nlegend(\"topleft\", c(\"Hurdle NB\", \"Hurdle NB 2\", \"Poisson (id)\", \"Poisson (log)\"),\n  col = c(4, 4, 2, 3), lty = c(1, 2, 1, 1), lwd = 1.5, bty = \"n\")\n\n\n\n\n\n\n## alternative displays: Q-Q residuals plot, barplot, residuals vs. fitted\nif(require(\"topmodels\")) {\npar(mfrow= c(3, 2))\nqqrplot(cs_p, range = c(0.05, 0.95), main = \"Q-Q residuals plot: Poisson\")\nqqrplot(cs_hnb, range = c(0.05, 0.95), main = \"Q-Q residuals plot: Hurdle NB\")\n} else {\npar(mfrow= c(2, 2))\n}\n\nbarplot(t(matrix(c(r_p$observed, r_p$expected), ncol = 2,\n  dimnames = list(r_p$x, c(\"Observed\", \"Expected\")))),\n  beside = TRUE, main = \"Barplot: Poisson\",\n  xlab = \"satellites\", ylab = \"Frequency\",\n  legend.text = TRUE, args.legend = list(x = \"topright\", bty = \"n\"))\nbarplot(t(matrix(c(r_hnb$observed, r_hnb$expected), ncol = 2,\n  dimnames = list(r_hnb$x, c(\"Observed\", \"Expected\")))),\n  beside = TRUE, main = \"Barplot: Hurdle NB\",\n  xlab = \"satellites\", ylab = \"Frequency\",\n  legend.text = TRUE, args.legend = list(x = \"topright\", bty = \"n\"))\n\nplot(predict(cs_p, type = \"response\"),\n  residuals(cs_p, type = \"pearson\"),\n  xlab = \"Fitted values\", ylab = \"Pearson residuals\",\n  main = \"Residuals vs. fitted: Poisson\")\nplot(predict(cs_hnb, type = \"response\"),\n  residuals(cs_hnb, type = \"pearson\"),\n  xlab = \"Fitted values\", ylab = \"Pearson residuals\",\n  main = \"Residuals vs. fitted: Hurdle NB\")",
    "crumbs": [
      "Data sets",
      "CrabSatellites"
    ]
  },
  {
    "objectID": "man/OralHealthNL.html",
    "href": "man/OralHealthNL.html",
    "title": "countreg",
    "section": "",
    "text": "Data from a study on oral health status and the preventive dental behaviors of 9-year-old children in The Netherlands.\n\ndata(\"OralHealthNL\")\n\nA data frame containing 440 observations on 8 variables.\n\n\ndmfs\n\n\nNumeric index of decayed, missing, and filled surfaces (DMFS) in deciduous teeth.\n\n\neducation\n\n\nFactor indicating whether the highest completed education level of the mother is “high” (senior general secondary education, HAVO, or higher) or “low”.\n\n\ngender\n\n\nFactor indicating gender of the child (“female” or “male”).\n\n\nethnicity\n\n\nFactor indicating whether the mother is “immigrant” (born abroad) or “native” (born in The Netherlands).\n\n\nbrushing\n\n\nFactor indicating whether the frequency of brushing teeth is “&lt; 2” or “&gt;= 2” times per day.\n\n\nbreakfast\n\n\nFactor indicating whether the frequency of having breakfast is “7” or “&lt; 7” days per week.\n\n\nfooddrink\n\n\nFactor indicating whether the frequency of food and drinks in addition to the three main meals is “&lt;= 7” or “&gt; 7” times per day.\n\n\ncorah\n\n\nFactor indicating whether Corah’s Dental Anxiety score is “&lt; 13” or “&gt;= 13” (see also below).\n\n\nThe data are from the study “Oral Health in Children and Adolescents in The Netherlands” (Schuller et al. 2011). The aim of this study was to describe the oral health status and the preventive dental behaviors of children from different age groups (Dusseldorp et al. 2015). Here, the subset of children at the age of 9 years is provided as analyzed by Hofstetter et al. (2016).\nThe data collection consisted of a clinical oral examination and a questionnaire survey, using a repeated cross-sectional design. Data contained information about demographic variables (ethnicity and educational level), nutrition, children’s dental attendance, oral self-care, and dental anxiety. The score on Corah’s Dental Anxiety Questionnaire was used as a measure of dental anxiety. This questionnaire consists of four questions with answer categories from 1 (low anxiety) to 5 (high anxiety). A total Corah score was computed by taking the sum of the four items and then dichotomized into ‘lower than 13’ and ‘higher than or equal to 13’.\n\nSupplementary materials for Hofstetter et al. (2016). doi:10.1159/000448197\n\nDusseldorp E, Kamphuis M, Schuller AA (2015). “Impact of Lifestyle Factors on Caries Experience in Three Different Age Groups: 9, 15, and 21-Year-Olds”, Community Dentistry and Oral Epidemiology, 43(1), 9–16. doi:10.1111/cdoe.12123\nHofstetter H, Dusseldorp E, Zeileis A, Schuller AA (2016). “Modeling Caries Experience: Advantages of the Use of the Hurdle Model”, Caries Research, 50(6), 517–526. doi:10.1159/000448197\nSchuller AA, Poorterman JHG, van Kempen CPF, Dusseldorp E, van Dommelen P, Verrips GHW (2011). Kies voor tanden: Een onderzoek naar mondgezondheid en preventief tandheelkundig gedrag van jeugdigen. Tussenmeting 2009, een vervolg op de reeks TJZ-onderzoeken. TNO, Leiden.\n\n\nlibrary(\"countreg\")\n\n## Load data and omit NAs and one dmfs outlier\ndata(\"OralHealthNL\", package = \"countreg\")\nhead(OralHealthNL)\n\n  dmfs education gender ethnicity brushing breakfast fooddrink corah\n1    1       low   male    native     &gt;= 2         7      &lt;= 7  &lt;NA&gt;\n2    0      high   male    native     &gt;= 2         7      &lt;= 7  &lt;NA&gt;\n3    6      high female    native     &gt;= 2         7      &lt;= 7  &lt; 13\n4    9       low female    native     &gt;= 2         7       &gt; 7  &lt; 13\n5    0      high female    native      &lt; 2         7      &lt;= 7  &lt;NA&gt;\n6    0       low   male    native     &gt;= 2         7      &lt;= 7  &lt; 13\n\nOralHealthNL &lt;- na.omit(subset(OralHealthNL, dmfs &lt; 40))\n\n## Visualization: Is dmfs &gt; 0?\npar(mfrow = c(2, 4))\nplot(factor(dmfs &gt; 0, levels = c(TRUE, FALSE), labels = c(\"&gt; 0\", \"= 0\")) ~ .,\n  data = OralHealthNL, ylab = \"dmfs\")\n\n## Count: How large is log(dmfs) given dmfs &gt; 0?\npar(mfrow = c(2, 4))\n\n\n\n\n\n\nplot(log(dmfs) ~ ., data = OralHealthNL, subset = dmfs &gt; 0, ylab = \"dmfs\")\n\n## Relevel the factor variables so that non-risk group is the reference\nOralHealthNL &lt;- transform(OralHealthNL,\n  ethnicity = relevel(ethnicity, ref = \"native\"),\n  brushing = relevel(brushing, ref = \"&gt;= 2\"),\n  breakfast = relevel(breakfast, ref = \"7\")\n)\n\n## Count regression models\nzinb &lt;- zeroinfl(dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\nzip  &lt;- zeroinfl(dmfs ~ ., data = OralHealthNL, dist = \"poisson\")\nhnb  &lt;-   hurdle(dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\nhp   &lt;-   hurdle(dmfs ~ ., data = OralHealthNL, dist = \"poisson\")\n\n## Model comparisons (Table 3)\n## Information criteria\ncbind(AIC(hnb, zinb, hp, zip), BIC = BIC(hnb, zinb, hp, zip)[, 2])\n\n     df      AIC      BIC\nhnb  17 1710.477 1778.161\nzinb 17 1712.954 1780.638\nhp   16 1969.529 2033.232\nzip  16 1969.571 2033.274\n\n## Negative binomial vs. Poisson\nif(require(\"lmtest\")) lrtest(hnb, hp)\n\nLikelihood ratio test\n\nModel 1: dmfs ~ .\nModel 2: dmfs ~ .\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1  17 -838.24                         \n2  16 -968.76 -1 261.05  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nif(require(\"lmtest\")) lrtest(zinb, zip)\n\nLikelihood ratio test\n\nModel 1: dmfs ~ .\nModel 2: dmfs ~ .\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1  17 -839.48                         \n2  16 -968.79 -1 258.62  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Zero-inflation vs. hurdle\nif(require(\"nonnest2\")) vuongtest(zinb, hnb)\n\n\nModel 1 \n Class: zeroinfl \n Call: zeroinfl(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nModel 2 \n Class: hurdle \n Call: hurdle(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nVariance test \n  H0: Model 1 and Model 2 are indistinguishable \n  H1: Model 1 and Model 2 are distinguishable \n    w2 = 0.001,   p = 0.382\n\nNon-nested likelihood ratio test \n  H0: Model fits are equal for the focal population \n  H1A: Model 1 fits better than Model 2 \n    z = -2.511,   p = 0.994\n  H1B: Model 2 fits better than Model 1 \n    z = -2.511,   p = 0.006015\n\n## Coefficients, odds ratios, and rate ratios\n## Negative binomial hurdle model (Table 3)\nsummary(hnb)\n\n\nCall:\nhurdle(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.0564 -0.6754 -0.4458  0.3831  4.2094 \n\nCount model coefficients (truncated negbin with log link):\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.29286    0.13110   9.861  &lt; 2e-16 ***\neducationlow        0.30434    0.12956   2.349  0.01883 *  \ngendermale          0.05017    0.12481   0.402  0.68772    \nethnicityimmigrant  0.33572    0.15078   2.227  0.02598 *  \nbrushing&lt; 2         0.37770    0.14319   2.638  0.00835 ** \nbreakfast&lt; 7        0.13758    0.18510   0.743  0.45732    \nfooddrink&gt; 7       -0.08250    0.19674  -0.419  0.67497    \ncorah&gt;= 13          0.38452    0.22072   1.742  0.08148 .  \nLog(theta)          0.51214    0.18669   2.743  0.00608 ** \nZero hurdle model coefficients (binomial with logit link):\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -0.32742    0.19784  -1.655  0.09794 . \neducationlow         0.40429    0.21393   1.890  0.05877 . \ngendermale          -0.04384    0.21348  -0.205  0.83729   \nethnicityimmigrant   0.48796    0.29112   1.676  0.09371 . \nbrushing&lt; 2          0.26244    0.26812   0.979  0.32766   \nbreakfast&lt; 7         1.25730    0.47601   2.641  0.00826 **\nfooddrink&gt; 7         0.98360    0.45653   2.155  0.03120 * \ncorah&gt;= 13          16.14670  865.49661   0.019  0.98512   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 1.6689\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -838.2 on 17 Df\n\nexp(confint(hnb))\n\n                             2.5 %   97.5 %\ncount_(Intercept)        2.8176305 4.710609\ncount_educationlow       1.0516938 1.747675\ncount_gendermale         0.8232799 1.342851\ncount_ethnicityimmigrant 1.0410117 1.879962\ncount_brushing&lt; 2        1.1019136 1.931618\ncount_breakfast&lt; 7       0.7983455 1.649344\ncount_fooddrink&gt; 7       0.6261915 1.354050\ncount_corah&gt;= 13         0.9530647 2.263966\nzero_(Intercept)         0.4890990 1.062205\nzero_educationlow        0.9851189 2.278641\nzero_gendermale          0.6298696 1.454358\nzero_ethnicityimmigrant  0.9206978 2.882191\nzero_brushing&lt; 2         0.7686977 2.198867\nzero_breakfast&lt; 7        1.3831085 8.937661\nzero_fooddrink&gt; 7        1.0928891 6.542907\nzero_corah&gt;= 13          0.0000000      Inf\n\n## Negative binomial zero-inflated model (Table 4)\nsummary(hnb)\n\n\nCall:\nhurdle(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.0564 -0.6754 -0.4458  0.3831  4.2094 \n\nCount model coefficients (truncated negbin with log link):\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.29286    0.13110   9.861  &lt; 2e-16 ***\neducationlow        0.30434    0.12956   2.349  0.01883 *  \ngendermale          0.05017    0.12481   0.402  0.68772    \nethnicityimmigrant  0.33572    0.15078   2.227  0.02598 *  \nbrushing&lt; 2         0.37770    0.14319   2.638  0.00835 ** \nbreakfast&lt; 7        0.13758    0.18510   0.743  0.45732    \nfooddrink&gt; 7       -0.08250    0.19674  -0.419  0.67497    \ncorah&gt;= 13          0.38452    0.22072   1.742  0.08148 .  \nLog(theta)          0.51214    0.18669   2.743  0.00608 ** \nZero hurdle model coefficients (binomial with logit link):\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -0.32742    0.19784  -1.655  0.09794 . \neducationlow         0.40429    0.21393   1.890  0.05877 . \ngendermale          -0.04384    0.21348  -0.205  0.83729   \nethnicityimmigrant   0.48796    0.29112   1.676  0.09371 . \nbrushing&lt; 2          0.26244    0.26812   0.979  0.32766   \nbreakfast&lt; 7         1.25730    0.47601   2.641  0.00826 **\nfooddrink&gt; 7         0.98360    0.45653   2.155  0.03120 * \ncorah&gt;= 13          16.14670  865.49661   0.019  0.98512   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 1.6689\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -838.2 on 17 Df\n\nexp(confint(zinb))\n\n                              2.5 %    97.5 %\ncount_(Intercept)        2.87728799 4.7399839\ncount_educationlow       1.05358981 1.7408558\ncount_gendermale         0.83032458 1.3392020\ncount_ethnicityimmigrant 1.03700891 1.8491811\ncount_brushing&lt; 2        1.08222465 1.8760869\ncount_breakfast&lt; 7       0.81383157 1.6536956\ncount_fooddrink&gt; 7       0.63596745 1.3590801\ncount_corah&gt;= 13         1.01247274 2.3333949\nzero_(Intercept)         0.67229117 1.6908065\nzero_educationlow        0.43459181 1.1554917\nzero_gendermale          0.66228485 1.7408712\nzero_ethnicityimmigrant  0.32595659 1.2220538\nzero_brushing&lt; 2         0.42567249 1.4411974\nzero_breakfast&lt; 7        0.06835275 0.8785863\nzero_fooddrink&gt; 7        0.08307571 1.0765754\nzero_corah&gt;= 13          0.00000000       Inf\n\n## Rootograms (top left: Figure 1)\nif(require(\"topmodels\")) {\npar(mfrow = c(2, 2))\nrootogram(lm(OralHealthNL$dmfs ~ 1),\n  style = \"standing\", scale = \"raw\",\n  breaks = 0:23 - 0.5, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Normal distribution\")\nrootogram(hnb,\n  style = \"standing\", scale = \"raw\",\n  width = 1, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Negative binomial hurdle model\")\nrootogram(lm(OralHealthNL$dmfs ~ 1),\n  breaks = 0:23 - 0.5, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Normal distribution\")\nabline(h = c(-1, 1), lty = 2)\nrootogram(hnb,\n  width = 1, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Negative binomial hurdle model\")\nabline(h = c(-1, 1), lty = 2)\npar(mfrow = c(1, 1))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n## Number of zeros\nc(dmfs = sum(OralHealthNL$dmfs == 0),\n  ZINB = sum(predict(zinb, type = \"density\", at = 0)),\n  Hurdle = sum(predict(hnb, type = \"density\", at = 0)))\n\n    dmfs     ZINB   Hurdle \n176.0000 177.1231 176.0000 \n\n## Correlation of observations and fitted means\ncor(cbind(dmfs = OralHealthNL$dmfs,\n  ZINB = fitted(zinb), HNB = fitted(hnb)))\n\n          dmfs     ZINB       HNB\ndmfs 1.0000000 0.348679 0.3490063\nZINB 0.3486790 1.000000 0.9997870\nHNB  0.3490063 0.999787 1.0000000\n\n## Bias-reduced logistic regression (due to separation)\nif(require(\"brglm2\")) {\nbr &lt;- glm(\n  factor(dmfs == 0, levels = c(TRUE, FALSE), labels = c(\"= 0\", \"&gt; 0\")) ~ .,\n  data = OralHealthNL, family = binomial, method = \"brglmFit\")\nprint(coeftest(br), digits = 1)\n}\n\n\nz test of coefficients:\n\n                   Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)           -0.32       0.20    -1.6     0.10  \neducationlow           0.40       0.21     1.9     0.06 .\ngendermale            -0.04       0.21    -0.2     0.84  \nethnicityimmigrant     0.47       0.29     1.6     0.10  \nbrushing&lt; 2            0.26       0.27     1.0     0.34  \nbreakfast&lt; 7           1.19       0.46     2.6     0.01 *\nfooddrink&gt; 7           0.93       0.45     2.1     0.04 *\ncorah&gt;= 13             3.33       1.48     2.3     0.02 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Data sets",
      "OralHealthNL"
    ]
  },
  {
    "objectID": "man/OralHealthNL.html#oral-health-in-children-in-the-netherlands",
    "href": "man/OralHealthNL.html#oral-health-in-children-in-the-netherlands",
    "title": "countreg",
    "section": "",
    "text": "Data from a study on oral health status and the preventive dental behaviors of 9-year-old children in The Netherlands.\n\ndata(\"OralHealthNL\")\n\nA data frame containing 440 observations on 8 variables.\n\n\ndmfs\n\n\nNumeric index of decayed, missing, and filled surfaces (DMFS) in deciduous teeth.\n\n\neducation\n\n\nFactor indicating whether the highest completed education level of the mother is “high” (senior general secondary education, HAVO, or higher) or “low”.\n\n\ngender\n\n\nFactor indicating gender of the child (“female” or “male”).\n\n\nethnicity\n\n\nFactor indicating whether the mother is “immigrant” (born abroad) or “native” (born in The Netherlands).\n\n\nbrushing\n\n\nFactor indicating whether the frequency of brushing teeth is “&lt; 2” or “&gt;= 2” times per day.\n\n\nbreakfast\n\n\nFactor indicating whether the frequency of having breakfast is “7” or “&lt; 7” days per week.\n\n\nfooddrink\n\n\nFactor indicating whether the frequency of food and drinks in addition to the three main meals is “&lt;= 7” or “&gt; 7” times per day.\n\n\ncorah\n\n\nFactor indicating whether Corah’s Dental Anxiety score is “&lt; 13” or “&gt;= 13” (see also below).\n\n\nThe data are from the study “Oral Health in Children and Adolescents in The Netherlands” (Schuller et al. 2011). The aim of this study was to describe the oral health status and the preventive dental behaviors of children from different age groups (Dusseldorp et al. 2015). Here, the subset of children at the age of 9 years is provided as analyzed by Hofstetter et al. (2016).\nThe data collection consisted of a clinical oral examination and a questionnaire survey, using a repeated cross-sectional design. Data contained information about demographic variables (ethnicity and educational level), nutrition, children’s dental attendance, oral self-care, and dental anxiety. The score on Corah’s Dental Anxiety Questionnaire was used as a measure of dental anxiety. This questionnaire consists of four questions with answer categories from 1 (low anxiety) to 5 (high anxiety). A total Corah score was computed by taking the sum of the four items and then dichotomized into ‘lower than 13’ and ‘higher than or equal to 13’.\n\nSupplementary materials for Hofstetter et al. (2016). doi:10.1159/000448197\n\nDusseldorp E, Kamphuis M, Schuller AA (2015). “Impact of Lifestyle Factors on Caries Experience in Three Different Age Groups: 9, 15, and 21-Year-Olds”, Community Dentistry and Oral Epidemiology, 43(1), 9–16. doi:10.1111/cdoe.12123\nHofstetter H, Dusseldorp E, Zeileis A, Schuller AA (2016). “Modeling Caries Experience: Advantages of the Use of the Hurdle Model”, Caries Research, 50(6), 517–526. doi:10.1159/000448197\nSchuller AA, Poorterman JHG, van Kempen CPF, Dusseldorp E, van Dommelen P, Verrips GHW (2011). Kies voor tanden: Een onderzoek naar mondgezondheid en preventief tandheelkundig gedrag van jeugdigen. Tussenmeting 2009, een vervolg op de reeks TJZ-onderzoeken. TNO, Leiden.\n\n\nlibrary(\"countreg\")\n\n## Load data and omit NAs and one dmfs outlier\ndata(\"OralHealthNL\", package = \"countreg\")\nhead(OralHealthNL)\n\n  dmfs education gender ethnicity brushing breakfast fooddrink corah\n1    1       low   male    native     &gt;= 2         7      &lt;= 7  &lt;NA&gt;\n2    0      high   male    native     &gt;= 2         7      &lt;= 7  &lt;NA&gt;\n3    6      high female    native     &gt;= 2         7      &lt;= 7  &lt; 13\n4    9       low female    native     &gt;= 2         7       &gt; 7  &lt; 13\n5    0      high female    native      &lt; 2         7      &lt;= 7  &lt;NA&gt;\n6    0       low   male    native     &gt;= 2         7      &lt;= 7  &lt; 13\n\nOralHealthNL &lt;- na.omit(subset(OralHealthNL, dmfs &lt; 40))\n\n## Visualization: Is dmfs &gt; 0?\npar(mfrow = c(2, 4))\nplot(factor(dmfs &gt; 0, levels = c(TRUE, FALSE), labels = c(\"&gt; 0\", \"= 0\")) ~ .,\n  data = OralHealthNL, ylab = \"dmfs\")\n\n## Count: How large is log(dmfs) given dmfs &gt; 0?\npar(mfrow = c(2, 4))\n\n\n\n\n\n\nplot(log(dmfs) ~ ., data = OralHealthNL, subset = dmfs &gt; 0, ylab = \"dmfs\")\n\n## Relevel the factor variables so that non-risk group is the reference\nOralHealthNL &lt;- transform(OralHealthNL,\n  ethnicity = relevel(ethnicity, ref = \"native\"),\n  brushing = relevel(brushing, ref = \"&gt;= 2\"),\n  breakfast = relevel(breakfast, ref = \"7\")\n)\n\n## Count regression models\nzinb &lt;- zeroinfl(dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\nzip  &lt;- zeroinfl(dmfs ~ ., data = OralHealthNL, dist = \"poisson\")\nhnb  &lt;-   hurdle(dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\nhp   &lt;-   hurdle(dmfs ~ ., data = OralHealthNL, dist = \"poisson\")\n\n## Model comparisons (Table 3)\n## Information criteria\ncbind(AIC(hnb, zinb, hp, zip), BIC = BIC(hnb, zinb, hp, zip)[, 2])\n\n     df      AIC      BIC\nhnb  17 1710.477 1778.161\nzinb 17 1712.954 1780.638\nhp   16 1969.529 2033.232\nzip  16 1969.571 2033.274\n\n## Negative binomial vs. Poisson\nif(require(\"lmtest\")) lrtest(hnb, hp)\n\nLikelihood ratio test\n\nModel 1: dmfs ~ .\nModel 2: dmfs ~ .\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1  17 -838.24                         \n2  16 -968.76 -1 261.05  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nif(require(\"lmtest\")) lrtest(zinb, zip)\n\nLikelihood ratio test\n\nModel 1: dmfs ~ .\nModel 2: dmfs ~ .\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1  17 -839.48                         \n2  16 -968.79 -1 258.62  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Zero-inflation vs. hurdle\nif(require(\"nonnest2\")) vuongtest(zinb, hnb)\n\n\nModel 1 \n Class: zeroinfl \n Call: zeroinfl(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nModel 2 \n Class: hurdle \n Call: hurdle(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nVariance test \n  H0: Model 1 and Model 2 are indistinguishable \n  H1: Model 1 and Model 2 are distinguishable \n    w2 = 0.001,   p = 0.382\n\nNon-nested likelihood ratio test \n  H0: Model fits are equal for the focal population \n  H1A: Model 1 fits better than Model 2 \n    z = -2.511,   p = 0.994\n  H1B: Model 2 fits better than Model 1 \n    z = -2.511,   p = 0.006015\n\n## Coefficients, odds ratios, and rate ratios\n## Negative binomial hurdle model (Table 3)\nsummary(hnb)\n\n\nCall:\nhurdle(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.0564 -0.6754 -0.4458  0.3831  4.2094 \n\nCount model coefficients (truncated negbin with log link):\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.29286    0.13110   9.861  &lt; 2e-16 ***\neducationlow        0.30434    0.12956   2.349  0.01883 *  \ngendermale          0.05017    0.12481   0.402  0.68772    \nethnicityimmigrant  0.33572    0.15078   2.227  0.02598 *  \nbrushing&lt; 2         0.37770    0.14319   2.638  0.00835 ** \nbreakfast&lt; 7        0.13758    0.18510   0.743  0.45732    \nfooddrink&gt; 7       -0.08250    0.19674  -0.419  0.67497    \ncorah&gt;= 13          0.38452    0.22072   1.742  0.08148 .  \nLog(theta)          0.51214    0.18669   2.743  0.00608 ** \nZero hurdle model coefficients (binomial with logit link):\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -0.32742    0.19784  -1.655  0.09794 . \neducationlow         0.40429    0.21393   1.890  0.05877 . \ngendermale          -0.04384    0.21348  -0.205  0.83729   \nethnicityimmigrant   0.48796    0.29112   1.676  0.09371 . \nbrushing&lt; 2          0.26244    0.26812   0.979  0.32766   \nbreakfast&lt; 7         1.25730    0.47601   2.641  0.00826 **\nfooddrink&gt; 7         0.98360    0.45653   2.155  0.03120 * \ncorah&gt;= 13          16.14670  865.49661   0.019  0.98512   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 1.6689\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -838.2 on 17 Df\n\nexp(confint(hnb))\n\n                             2.5 %   97.5 %\ncount_(Intercept)        2.8176305 4.710609\ncount_educationlow       1.0516938 1.747675\ncount_gendermale         0.8232799 1.342851\ncount_ethnicityimmigrant 1.0410117 1.879962\ncount_brushing&lt; 2        1.1019136 1.931618\ncount_breakfast&lt; 7       0.7983455 1.649344\ncount_fooddrink&gt; 7       0.6261915 1.354050\ncount_corah&gt;= 13         0.9530647 2.263966\nzero_(Intercept)         0.4890990 1.062205\nzero_educationlow        0.9851189 2.278641\nzero_gendermale          0.6298696 1.454358\nzero_ethnicityimmigrant  0.9206978 2.882191\nzero_brushing&lt; 2         0.7686977 2.198867\nzero_breakfast&lt; 7        1.3831085 8.937661\nzero_fooddrink&gt; 7        1.0928891 6.542907\nzero_corah&gt;= 13          0.0000000      Inf\n\n## Negative binomial zero-inflated model (Table 4)\nsummary(hnb)\n\n\nCall:\nhurdle(formula = dmfs ~ ., data = OralHealthNL, dist = \"negbin\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.0564 -0.6754 -0.4458  0.3831  4.2094 \n\nCount model coefficients (truncated negbin with log link):\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.29286    0.13110   9.861  &lt; 2e-16 ***\neducationlow        0.30434    0.12956   2.349  0.01883 *  \ngendermale          0.05017    0.12481   0.402  0.68772    \nethnicityimmigrant  0.33572    0.15078   2.227  0.02598 *  \nbrushing&lt; 2         0.37770    0.14319   2.638  0.00835 ** \nbreakfast&lt; 7        0.13758    0.18510   0.743  0.45732    \nfooddrink&gt; 7       -0.08250    0.19674  -0.419  0.67497    \ncorah&gt;= 13          0.38452    0.22072   1.742  0.08148 .  \nLog(theta)          0.51214    0.18669   2.743  0.00608 ** \nZero hurdle model coefficients (binomial with logit link):\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -0.32742    0.19784  -1.655  0.09794 . \neducationlow         0.40429    0.21393   1.890  0.05877 . \ngendermale          -0.04384    0.21348  -0.205  0.83729   \nethnicityimmigrant   0.48796    0.29112   1.676  0.09371 . \nbrushing&lt; 2          0.26244    0.26812   0.979  0.32766   \nbreakfast&lt; 7         1.25730    0.47601   2.641  0.00826 **\nfooddrink&gt; 7         0.98360    0.45653   2.155  0.03120 * \ncorah&gt;= 13          16.14670  865.49661   0.019  0.98512   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta: count = 1.6689\nNumber of iterations in BFGS optimization: 17 \nLog-likelihood: -838.2 on 17 Df\n\nexp(confint(zinb))\n\n                              2.5 %    97.5 %\ncount_(Intercept)        2.87728799 4.7399839\ncount_educationlow       1.05358981 1.7408558\ncount_gendermale         0.83032458 1.3392020\ncount_ethnicityimmigrant 1.03700891 1.8491811\ncount_brushing&lt; 2        1.08222465 1.8760869\ncount_breakfast&lt; 7       0.81383157 1.6536956\ncount_fooddrink&gt; 7       0.63596745 1.3590801\ncount_corah&gt;= 13         1.01247274 2.3333949\nzero_(Intercept)         0.67229117 1.6908065\nzero_educationlow        0.43459181 1.1554917\nzero_gendermale          0.66228485 1.7408712\nzero_ethnicityimmigrant  0.32595659 1.2220538\nzero_brushing&lt; 2         0.42567249 1.4411974\nzero_breakfast&lt; 7        0.06835275 0.8785863\nzero_fooddrink&gt; 7        0.08307571 1.0765754\nzero_corah&gt;= 13          0.00000000       Inf\n\n## Rootograms (top left: Figure 1)\nif(require(\"topmodels\")) {\npar(mfrow = c(2, 2))\nrootogram(lm(OralHealthNL$dmfs ~ 1),\n  style = \"standing\", scale = \"raw\",\n  breaks = 0:23 - 0.5, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Normal distribution\")\nrootogram(hnb,\n  style = \"standing\", scale = \"raw\",\n  width = 1, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Negative binomial hurdle model\")\nrootogram(lm(OralHealthNL$dmfs ~ 1),\n  breaks = 0:23 - 0.5, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Normal distribution\")\nabline(h = c(-1, 1), lty = 2)\nrootogram(hnb,\n  width = 1, xlim = c(-0.5, 22.5),\n  xlab = \"dmfs\", main = \"Negative binomial hurdle model\")\nabline(h = c(-1, 1), lty = 2)\npar(mfrow = c(1, 1))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n## Number of zeros\nc(dmfs = sum(OralHealthNL$dmfs == 0),\n  ZINB = sum(predict(zinb, type = \"density\", at = 0)),\n  Hurdle = sum(predict(hnb, type = \"density\", at = 0)))\n\n    dmfs     ZINB   Hurdle \n176.0000 177.1231 176.0000 \n\n## Correlation of observations and fitted means\ncor(cbind(dmfs = OralHealthNL$dmfs,\n  ZINB = fitted(zinb), HNB = fitted(hnb)))\n\n          dmfs     ZINB       HNB\ndmfs 1.0000000 0.348679 0.3490063\nZINB 0.3486790 1.000000 0.9997870\nHNB  0.3490063 0.999787 1.0000000\n\n## Bias-reduced logistic regression (due to separation)\nif(require(\"brglm2\")) {\nbr &lt;- glm(\n  factor(dmfs == 0, levels = c(TRUE, FALSE), labels = c(\"= 0\", \"&gt; 0\")) ~ .,\n  data = OralHealthNL, family = binomial, method = \"brglmFit\")\nprint(coeftest(br), digits = 1)\n}\n\n\nz test of coefficients:\n\n                   Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)           -0.32       0.20    -1.6     0.10  \neducationlow           0.40       0.21     1.9     0.06 .\ngendermale            -0.04       0.21    -0.2     0.84  \nethnicityimmigrant     0.47       0.29     1.6     0.10  \nbrushing&lt; 2            0.26       0.27     1.0     0.34  \nbreakfast&lt; 7           1.19       0.46     2.6     0.01 *\nfooddrink&gt; 7           0.93       0.45     2.1     0.04 *\ncorah&gt;= 13             3.33       1.48     2.3     0.02 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Data sets",
      "OralHealthNL"
    ]
  },
  {
    "objectID": "man/ztpois.html",
    "href": "man/ztpois.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the zero-truncated Poisson distribution with parameter lambda (= mean of the untruncated distribution) or mean (= of the truncated distribution).\n\n\n\nsztpois(x, lambda, mean, parameter = \"lambda\", drop = TRUE)\nhztpois(x, lambda, mean, parameter = \"lambda\", drop = TRUE)\nmean_ztpois(lambda, mean, drop = TRUE)\nvar_ztpois(lambda, mean, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nlambda\n\n\nvector of (non-negative) means of the untruncated Poisson distribution. Only one of lambda or mean should be specified.\n\n\n\n\nmean\n\n\nvector of means (greater than 1) of the zero-truncated Poisson distribution. Only one of lambda or mean should be specified.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “lambda” or “mean” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe untruncated Poisson distribution has density\n\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\). The zero-truncated density is then simply obtained as\n\n\\(g(x) = \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\).\nThe zero-truncated distribution has expectation \\(E(X) = \\mu = \\lambda / (1 - \\exp(-\\lambda))\\) and variance \\(Var(X) = \\mu \\cdot (\\lambda + 1 - \\mu)\\), where \\(\\lambda\\) is the expectation of the untruncated Poisson distribution.\nDespite the simple form of the transformation \\(\\mu(\\lambda)\\) the inverse \\(\\lambda(\\mu)\\) has no closed-form solution and is computed numerically if needed.\n\n\n\nsztpois gives the score function (= derivative of the log-density with respect to lambda or mean). hztpois gives the hessian (= 2nd derivative of the log-density with respect to lambda or mean). mean_ztpois and var_ztpois give the mean and the variance, respectively.\n\n\n\ndztpois, ztpoisson, dpois, zerotrunc",
    "crumbs": [
      "Distribution extensions",
      "Zero-truncated Poisson"
    ]
  },
  {
    "objectID": "man/ztpois.html#extension-of-the-zero-truncated-poisson-distribution",
    "href": "man/ztpois.html#extension-of-the-zero-truncated-poisson-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the zero-truncated Poisson distribution with parameter lambda (= mean of the untruncated distribution) or mean (= of the truncated distribution).\n\n\n\nsztpois(x, lambda, mean, parameter = \"lambda\", drop = TRUE)\nhztpois(x, lambda, mean, parameter = \"lambda\", drop = TRUE)\nmean_ztpois(lambda, mean, drop = TRUE)\nvar_ztpois(lambda, mean, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nlambda\n\n\nvector of (non-negative) means of the untruncated Poisson distribution. Only one of lambda or mean should be specified.\n\n\n\n\nmean\n\n\nvector of means (greater than 1) of the zero-truncated Poisson distribution. Only one of lambda or mean should be specified.\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “lambda” or “mean” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe untruncated Poisson distribution has density\n\n\\(f(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\\)\nfor \\(x = 0, 1, 2, \\ldots\\). The zero-truncated density is then simply obtained as\n\n\\(g(x) = \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\).\nThe zero-truncated distribution has expectation \\(E(X) = \\mu = \\lambda / (1 - \\exp(-\\lambda))\\) and variance \\(Var(X) = \\mu \\cdot (\\lambda + 1 - \\mu)\\), where \\(\\lambda\\) is the expectation of the untruncated Poisson distribution.\nDespite the simple form of the transformation \\(\\mu(\\lambda)\\) the inverse \\(\\lambda(\\mu)\\) has no closed-form solution and is computed numerically if needed.\n\n\n\nsztpois gives the score function (= derivative of the log-density with respect to lambda or mean). hztpois gives the hessian (= 2nd derivative of the log-density with respect to lambda or mean). mean_ztpois and var_ztpois give the mean and the variance, respectively.\n\n\n\ndztpois, ztpoisson, dpois, zerotrunc",
    "crumbs": [
      "Distribution extensions",
      "Zero-truncated Poisson"
    ]
  },
  {
    "objectID": "man/zitest.html",
    "href": "man/zitest.html",
    "title": "countreg",
    "section": "",
    "text": "Tests the null hypothesis of a Poisson GLM against the alternative of a zero-inflated version.\n\nzitest(object, type = c(\"scoreZIP\"))\n\n\n\n\n\nobject\n\n\na fitted Poisson GLM of class “glm” as fitted by glm with family poisson.\n\n\n\n\ntype\n\n\ntype of test, currently only scoreZIP. See details.\n\n\n\nCurrently alternative contains only intercept term in binary part, as in van den Broek (1995).\nNote that under the null hypothesis the parameter is on the boundary of the parameter space, hence the p-value is non-standard.\n\nAn object of class “htest”.\n\nvan den Broek J (1995). “A Score Test for Zero Inflation in a Poisson Distribution”. Biometrics, 51, 738–743.\n\nglm, poisson, glm.nb\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nCrabSatellites &lt;- transform(CrabSatellites,\n  color = as.numeric(color),\n  spine = as.numeric(spine),\n  cwidth = cut(width, c(-Inf, seq(23.25, 29.25), Inf))\n)\n\ncs_p &lt;- glm(satellites ~ width + color, data = CrabSatellites, family = poisson)\nzitest(cs_p)\n\n\n    Zero inflation test\n\ndata:  cs_p\nS = 42.608, p-value = 3.345e-11",
    "crumbs": [
      "Diagnostic tests",
      "zitest"
    ]
  },
  {
    "objectID": "man/zitest.html#zero-inflation-tests",
    "href": "man/zitest.html#zero-inflation-tests",
    "title": "countreg",
    "section": "",
    "text": "Tests the null hypothesis of a Poisson GLM against the alternative of a zero-inflated version.\n\nzitest(object, type = c(\"scoreZIP\"))\n\n\n\n\n\nobject\n\n\na fitted Poisson GLM of class “glm” as fitted by glm with family poisson.\n\n\n\n\ntype\n\n\ntype of test, currently only scoreZIP. See details.\n\n\n\nCurrently alternative contains only intercept term in binary part, as in van den Broek (1995).\nNote that under the null hypothesis the parameter is on the boundary of the parameter space, hence the p-value is non-standard.\n\nAn object of class “htest”.\n\nvan den Broek J (1995). “A Score Test for Zero Inflation in a Poisson Distribution”. Biometrics, 51, 738–743.\n\nglm, poisson, glm.nb\n\n\nlibrary(\"countreg\")\n\ndata(\"CrabSatellites\", package = \"countreg\")\nCrabSatellites &lt;- transform(CrabSatellites,\n  color = as.numeric(color),\n  spine = as.numeric(spine),\n  cwidth = cut(width, c(-Inf, seq(23.25, 29.25), Inf))\n)\n\ncs_p &lt;- glm(satellites ~ width + color, data = CrabSatellites, family = poisson)\nzitest(cs_p)\n\n\n    Zero inflation test\n\ndata:  cs_p\nS = 42.608, p-value = 3.345e-11",
    "crumbs": [
      "Diagnostic tests",
      "zitest"
    ]
  },
  {
    "objectID": "man/binom.html",
    "href": "man/binom.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the binomial distribution with parameters prob and size.\n\nsbinom(x, prob, size, parameter = \"prob\", drop = TRUE)\nhbinom(x, prob, size, parameter = \"prob\", drop = TRUE)\nmean_binom(prob, size, drop = TRUE)\nvar_binom(prob, size, drop = TRUE)\n\n\n\n\n\nx\n\n\nvector of quantiles.\n\n\n\n\nprob\n\n\nprobability of success on each trial.\n\n\n\n\nsize\n\n\nnumber of trials (zero or more).\n\n\n\n\nparameter\n\n\ncharacter. Derivatives are computed wrt this paramter. Note: Only “prob” is implemented.\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\nThe binomial distribution with size \\(= n\\) and prob \\(= p\\) has density\n\n\n\\(p(x) = {n \\choose x} {p}^{x} {(1-p)}^{n-x}\\)\nfor \\(x = 0, \\ldots, n\\).\nThe score function is\n\n\n\\(s(p) = \\frac{x}{p} - \\frac{n-x}{1-p}\\)\nThe hessian is\n\n\n\\(h(p) = - \\frac{x}{p^2} - \\frac{n-x}{(1-p)^2}\\)\n\nsbinom gives the score function, i.e., the 1st derivative of the log-density wrt prob and hbinom gives the hessian, i.e., the 2nd derivative of the log-density wrt prob. mean and var give the mean and variance, respectively.\n\nBinomial encompassing dbinom, pbinom, qbinom and rbinom.\n\n\nlibrary(\"countreg\")\n\n## Simulate some data\nset.seed(123)\ny &lt;- rbinom(50, size = 1, prob = 0.3)\n\n## Plot log-likelihood function\npar(mfrow = c(1,3))\nll &lt;- function(x) {sum(dbinom(y, size = 1, prob = x, log = TRUE))}\ncurve(sapply(x, ll), xlab = expression(pi), ylab = \"\", main = \"Log-likelihood\")\nabline(v = 0.3, lty = 3)\n\n## Plot score function\ncurve(sapply(x, function(x) sum(sbinom(y, size = 1, x))),\n      xlab = expression(pi), ylab = \"\", main = \"Score\")\nabline(h = 0, lty = 3)\nabline(v = 0.3, lty = 3)\n\n## Plot hessian\ncurve(sapply(x, function(x) sum(hbinom(y, size = 1, x))),\n      xlab = expression(pi), ylab = \"\", main = \"Hessian\")\nabline(v = 0.3, lty = 3)",
    "crumbs": [
      "Distribution extensions",
      "Binomial"
    ]
  },
  {
    "objectID": "man/binom.html#extension-of-the-binomial-distribution",
    "href": "man/binom.html#extension-of-the-binomial-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and variance for the binomial distribution with parameters prob and size.\n\nsbinom(x, prob, size, parameter = \"prob\", drop = TRUE)\nhbinom(x, prob, size, parameter = \"prob\", drop = TRUE)\nmean_binom(prob, size, drop = TRUE)\nvar_binom(prob, size, drop = TRUE)\n\n\n\n\n\nx\n\n\nvector of quantiles.\n\n\n\n\nprob\n\n\nprobability of success on each trial.\n\n\n\n\nsize\n\n\nnumber of trials (zero or more).\n\n\n\n\nparameter\n\n\ncharacter. Derivatives are computed wrt this paramter. Note: Only “prob” is implemented.\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\nThe binomial distribution with size \\(= n\\) and prob \\(= p\\) has density\n\n\n\\(p(x) = {n \\choose x} {p}^{x} {(1-p)}^{n-x}\\)\nfor \\(x = 0, \\ldots, n\\).\nThe score function is\n\n\n\\(s(p) = \\frac{x}{p} - \\frac{n-x}{1-p}\\)\nThe hessian is\n\n\n\\(h(p) = - \\frac{x}{p^2} - \\frac{n-x}{(1-p)^2}\\)\n\nsbinom gives the score function, i.e., the 1st derivative of the log-density wrt prob and hbinom gives the hessian, i.e., the 2nd derivative of the log-density wrt prob. mean and var give the mean and variance, respectively.\n\nBinomial encompassing dbinom, pbinom, qbinom and rbinom.\n\n\nlibrary(\"countreg\")\n\n## Simulate some data\nset.seed(123)\ny &lt;- rbinom(50, size = 1, prob = 0.3)\n\n## Plot log-likelihood function\npar(mfrow = c(1,3))\nll &lt;- function(x) {sum(dbinom(y, size = 1, prob = x, log = TRUE))}\ncurve(sapply(x, ll), xlab = expression(pi), ylab = \"\", main = \"Log-likelihood\")\nabline(v = 0.3, lty = 3)\n\n## Plot score function\ncurve(sapply(x, function(x) sum(sbinom(y, size = 1, x))),\n      xlab = expression(pi), ylab = \"\", main = \"Score\")\nabline(h = 0, lty = 3)\nabline(v = 0.3, lty = 3)\n\n## Plot hessian\ncurve(sapply(x, function(x) sum(hbinom(y, size = 1, x))),\n      xlab = expression(pi), ylab = \"\", main = \"Hessian\")\nabline(v = 0.3, lty = 3)",
    "crumbs": [
      "Distribution extensions",
      "Binomial"
    ]
  },
  {
    "objectID": "man/hnbinom.html",
    "href": "man/hnbinom.html",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and, variance for the (zero-)hurdle negative binomial distribution with parameters mu (= mean of the underlying negative binomial distribution), dispersion parameter theta (or equivalently size), and hurdle crossing probability pi (i.e., 1 - pi is the probability for observed zeros).\n\n\n\nshnbinom(x, mu, theta, size, pi, parameter = c(\"mu\", \"theta\", \"pi\"), drop = TRUE)\nhhnbinom(x, mu, theta, size, pi, parameter = c(\"mu\", \"theta\", \"pi\"), drop = TRUE)\nmean_hnbinom(mu, theta, size, pi, drop = TRUE)\nvar_hnbinom(mu, theta, size, pi, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nmu\n\n\nvector of non-negative means of the underlying negative binomial distribution.\n\n\n\n\ntheta, size\n\n\nvector of strictly positive dispersion parameters (shape parameter of the gamma mixing distribution). Only one of theta or size must be specified.\n\n\n\n\npi\n\n\nvector of hurdle crossing probabilities (i.e., 1 - pi is the probability for observed zeros).\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “theta” and/or “pi” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe underlying negative binomial distribution has density\n\nf(x) = \nfor \\(x = 0, 1, 2, \\ldots\\). The hurdle density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\) and \\(g(0) = 1 - \\pi\\), respectively.\n\n\n\nshnbinom gives the score function (= derivative of the log-density with respect to mu and/or theta and/or pi). hhnbinom gives the hessian (= 2nd derivative of the log-density with respect to mu and/or theta and/or pi). mean_hnbinom and var_hnbinom give the mean and the variance, respectively.\n\n\n\ndhnbinom, dnbinom, hurdle",
    "crumbs": [
      "Distribution extensions",
      "Hurdle negative binomial"
    ]
  },
  {
    "objectID": "man/hnbinom.html#extension-of-the-hurdle-negative-binomial-distribution",
    "href": "man/hnbinom.html#extension-of-the-hurdle-negative-binomial-distribution",
    "title": "countreg",
    "section": "",
    "text": "Score function, hessian, mean, and, variance for the (zero-)hurdle negative binomial distribution with parameters mu (= mean of the underlying negative binomial distribution), dispersion parameter theta (or equivalently size), and hurdle crossing probability pi (i.e., 1 - pi is the probability for observed zeros).\n\n\n\nshnbinom(x, mu, theta, size, pi, parameter = c(\"mu\", \"theta\", \"pi\"), drop = TRUE)\nhhnbinom(x, mu, theta, size, pi, parameter = c(\"mu\", \"theta\", \"pi\"), drop = TRUE)\nmean_hnbinom(mu, theta, size, pi, drop = TRUE)\nvar_hnbinom(mu, theta, size, pi, drop = TRUE)\n\n\n\n\n\n\n\nx\n\n\nvector of (positive integer) quantiles.\n\n\n\n\nmu\n\n\nvector of non-negative means of the underlying negative binomial distribution.\n\n\n\n\ntheta, size\n\n\nvector of strictly positive dispersion parameters (shape parameter of the gamma mixing distribution). Only one of theta or size must be specified.\n\n\n\n\npi\n\n\nvector of hurdle crossing probabilities (i.e., 1 - pi is the probability for observed zeros).\n\n\n\n\nparameter\n\n\ncharacter. Should the derivative with respect to “mu” and/or “theta” and/or “pi” be computed?\n\n\n\n\ndrop\n\n\nlogical. Should the result be a matrix (drop = FALSE) or should the dimension be dropped (drop = TRUE, the default)?\n\n\n\n\n\n\nThe underlying negative binomial distribution has density\n\nf(x) = \nfor \\(x = 0, 1, 2, \\ldots\\). The hurdle density is then simply obtained as\n\n\\(g(x) = \\pi \\cdot \\frac{f(x)}{1 - f(0)}\\)\nfor \\(x = 1, 2, \\ldots\\) and \\(g(0) = 1 - \\pi\\), respectively.\n\n\n\nshnbinom gives the score function (= derivative of the log-density with respect to mu and/or theta and/or pi). hhnbinom gives the hessian (= 2nd derivative of the log-density with respect to mu and/or theta and/or pi). mean_hnbinom and var_hnbinom give the mean and the variance, respectively.\n\n\n\ndhnbinom, dnbinom, hurdle",
    "crumbs": [
      "Distribution extensions",
      "Hurdle negative binomial"
    ]
  }
]