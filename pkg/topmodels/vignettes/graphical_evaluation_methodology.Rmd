---
title: "Graphical Evaluation: Methodology"
author: "Moritz Lang, Achim Zeileis"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: ref.bib
vignette: >
  %\VignetteIndexEntry{Graphical Model Assessment under Different Model Misspecifications}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{topmodels,crch,MASS,rmutil,sn}
  %\VignetteKeywords{FIXME}
  %\VignettePackage{topmodels}
---

```{css zoom-lib-src, echo = FALSE}
script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"
```

```{js zoom-jquery, echo = FALSE}
 $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
```

```{r preliminaries, echo = FALSE, message = FALSE}
library("topmodels")
library("crch")
library("MASS")
library("rmutil")  # for laplace dist
library("sn") # for skewed N dist
options(digits = 4)
knitr::opts_chunk$set(fig.aling = "center")

make_residuals_plot <- function(object, breaks = "Sturges", main = "") {
  name <- deparse(substitute(object))

  set.seed(1)
  hist(qresiduals(object), breaks = breaks, freq = FALSE,
    main = main,
    xlab = "Q-Q residuals", xlim = c(-5, 5), ylim = c(0, 1)
  )
  curve(dnorm, from = -5, to = 5, col = 2, lwd = 2, add = TRUE)
  legend("topright", "std. norm", lty = 1, col = 2, lwd = 2, bty = "n")
  box()
}
```

```{r dgp, echo = FALSE}
set.seed(0)

## regressors
d <- data.frame(
  x = runif(500, -1, 1),
  z = runif(500, -1, 1)
)

d <- transform(d,
  y_norm = rnorm(500),
  y_rskew = sn::rsn(500, xi = 0, omega = 1, alpha = 5, tau = 0),
  y_lskew = sn::rsn(500, xi = 0, omega = 1, alpha = -5, tau = 0),

  y_htail = rt(500, df = 4),
  y_ltail = c(rnorm(100), runif(400, min = -1.5, max = 1.5)),

  y_odisp = rpois(500, lambda = exp(0 + 2 * x + rnorm(500, mean = 0, sd = 1.5))),
  y_udisp = round(rnorm(n = 500, mean = exp(0 + 2 * x), sd = 0.001))

)
```

```{r fit, echo = FALSE}
m_norm <- lm(y_norm ~ 1, data = d)
m_rskew <- lm(y_rskew ~ 1, data = d)
m_lskew <- lm(y_lskew ~ 1, data = d)
m_htail <- lm(y_htail ~ 1, data = d)
m_ltail <- lm(y_ltail ~ 1, data = d)

m_odisp <- glm(y_odisp ~ x, family = "poisson", data = d)
m_udisp <- glm(y_udisp ~ x, family = "poisson", data = d)
```

## Introduction

Before discussing the various graphical evaluation methods in more detail, we
try to outline the underlying motivation and identify similarities and
differences in the evaluation methods. According to the seminal work of
@Gneiting+Balabdaoui+Raftery:2007, probabilistic forecasts aim to *maximize the
sharpness of the predictive distributions subject to calibration*. Calibration
here refers to the statistical concordance between the forecast and the
observation, and is thus a joint property of the forecasts and observations.
Sharpness, on the other hand, is a property of the forecast only and indicates
how concentrated a predictive distribution is. In general, the more
concentrated the sharper the forecast.

We focus on several graphical diagnostic tools to assess the calibration of a
probabilistic forecast $F( \cdot | \boldsymbol{\theta}_{i})$, issued in form
of a predictive distribution $f( \cdot | \boldsymbol{\theta}_{i})$. Given
observations $y_i (1 = \ldots, n)$, we assume a set of observation-specific
fitted parameters $\hat{\boldsymbol{\theta}}_{i}$, where the estimation may
have been performed on the same observations $i = 1, \ldots, n$ (i.e.,
corresponding to an in-sample assessment) or on a different data set (i.e.,
corresponding to an out-of-sample evaluation). The estimation procedure itself
can be either fully parametric or semi-parametric, as long as fitted parameters
$\hat{\boldsymbol{\theta}}_{i}$ exist for all observations of interest.
However, since the uncertainty in the estimation of the parameters is not
accounted for, small deviations from asymptotic theoretical properties will be
apparent in all graphical displays due to some sampling variation.


### Probabilistic calibration: PIT residuals

According to @Gneiting+Balabdaoui+Raftery:2007, calibration can be further
distinguished between probabilistic calibration and marginal calibration.
Probabilistic calibration is usually assessed using probability integral
transform (PIT) values
[@Dawid:1984;@Diebold+Gunther+Tay:1998;@Gneiting+Balabdaoui+Raftery:2007] or
so-called PIT residuals [@Warton:2017]:

$$u_i = F(y_i | \, \hat{\boldsymbol{\theta}}_i).$$

Here $F( \cdot )$ denotes the cumulative distribution function (CDF) of the
modeled distribution $f( \cdot )$ with estimated parameters
$\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1}, \ldots,
\hat{\theta}_{iK})^\top$. 

PIT residuals have the desirable property, that if the model is a good
approximation to the true data-generating process, i.e., the observation is
drawn from the predictive distribution, the PIT residuals $u_i$ are
approximately uniformly distributed on $[0, 1]$ for continous predictive
distributions $F( \cdot )$.  PIT residuals or variants are therefore have been
used extensively for model diagnosis and depending on their implementation are
named various names, among them forecast distribution transformed residuals
[@Smith:1985], randomized quantile residuals [@Dunn+Smyth:1996], universal
residuals [@Brockwell:2007].

In case of discrete predictive distributions or distributions with a discrete
component, e.g., in case of censoring, $u_i$ can be generated as a random draw U
from the interval:

$$u_i = \text{U}[F(y_i - 1 | \, \hat{\boldsymbol{\theta}}_i), F(y_i | \,
\hat{\boldsymbol{\theta}}_i)].$$

Here, we follow the definition by @Dunn+Smyth:1996, but similar approaches have
also been proposed in, e.g., @Brockwell:2007 and @Smith:1985. Again $u_i$ is
uniformally distributed, apart from sampling variability.

Since the PIT residuals are an iid sample from the standard uniform
distribution, the PIT residuals can also be mapped to other distribution
scales, e.g. to the standard normal scale, and should follow a standard normal
distribution here. In the simplest case, the PIT residuals $u_i$ can be plotted
in so-called P-P plots against the probabilities of a uniform distribution
[@Wilk:1968;@Handcock+Morris:1999]. However, it is far more common to
transform the PIT residuals to the normal scale and compare them to the
standard normal quantiles in a normal Q-Q plot [@Hoaglin:2006].  Alternatively,
in a PIT histogram, the uniformally distributed PIT residuals are divided into
intervals by a certain number of breakpoints and plotted in a histogram-style
plot. Regardless of the graphical display, the PIT residuals are always on the
probability scale, which might be transformed to the normal scale or another
scale if preferred.


### Marginal calibration: Observed vs. expected frequencies

Marginal calibration is generally concerned with whether the oberseved
frequencies match the frequencies expected by the model. For discrete
observations, frequencies for the observations themselves can be considered;
for continuous observations or more generally, frequencies for intevals of
observations are being used. Here, the expected frequencies are computed by
differences between the predictive CDFs $F( \cdot )$, evaluated at the interval
breaks. Hence, mariginal calibration is always obtained on the observation
scale compared to the probabilistic calibration performed on the probability
scale.

Although there are some previous studies that display observation points rather
than intervals [e.g., @Gneiting+Balabdaoui+Raftery:2007], here we stick to the
former and discuss only the so-called rootograms which are histogram-style plots
[@Kleiber+Zeileis:2016].

### Similarities and differences

In the graphical displays for assessing the goodness of fit, several recurring
elements can be seen:

* PIT residuals are asymptotically uniformly distributed or transformed to
  another probability scale: There is the PIT histogram on the uniform
  probability scale and the Q-Q plot traditionally employing the standard normal
  quantiles. Whereas, the transformation to the normals scale spreads the values
  in the tails further apart and thus better highlights possible discrepancies in
  the distribuional tails.
  
* To check the marginal calibration, one usually stays on the observation scale
  and evaluates whether observed and expected frequencies match. The rootogram
  thus evaluates on the observation scale, which is especially useful for count
  data with values close to zero.
  
* Discretization: Instead of plotting the raw values, e.g. PIT residuals, often
  some discretization improves readability of the graphical displays. The
  disadvantage here is that the breakpoint are kind of arbitrary and certain
  misspecification might be masked by plotting intervals. For example,
  misscpecifications in the outer tails of the distribution are often not visible
  in PIT histograms, as the intervals are averaving over many data points. Here,
  Q-Q plots are clearly superior.
  
* The uncertainty due to the estimation of the parameters is not taken into
  account. Therefore, some sampling variation is seen in all graphical displays.

## Methodology

### Rootogram

The rootogram is a graphical tool for assessing the goodness of fit in terms of
mariginal calibration of a parametric univariate distributional model, with
estimated parameters $\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1},
\ldots, \hat{\theta}_{iK})^\top$ and $f( \cdot )$ desribing the density or
probability mass function.  Rootograms evaluate graphically whether observed
frequencies $\text{obs}_j$ match the expected frequencies $\text{exp}_j$ by
plotting histogram-like rectangles or bars for the observed frequencies and a
curve for the fitted frequencies, both on a square-root scale. In the form
presented here, it was implemented by @Kleiber+Zeileis:2016 building on work of
@Tukey:1977. 

In the most general form, given an observational vector of a random variable
$y_i (i = 1, \ldots, n)$ which is divided into subsets by a set of breakpoints $b_0,
b_1, b_2, \dots~$, the observed and expected frequencies are given by

$$\text{obs}_j = \sum_{i=1}^{n}w_{i} I(y_i \in (b_j, b_{j+1}]),$$
$$\text{exp}_j = \sum_{i=1}^{n}w\{F(b_{j+1} |
  \hat{\boldsymbol{\theta}}_{i}) - F(b_{j} | \hat{\boldsymbol{\theta}}_{i})\},$$

with $F( \cdot )$ being the CDF of the modeled
distributional model $f( \cdot )$ and $w_i$ being optional
observation-specific weights. Whereby, the weights are typically needed either
for survey data or for situations with model-based weights
[@Kleiber+Zeileis:2016].

For a discrete variable $y_i$, the observed and expected frequencies can be
simplified and are given for each integer $j$ by

$$\text{obs}_j = \sum_{i=1}^{n}I(y_i - j),$$
$$\text{exp}_j = \sum_{i=1}^{n}f(j | \hat{\boldsymbol{\theta}}_{i}),$$

with the indicator variable $I( \cdot )$ [@Kleiber+Zeileis:2016]. As rootograms are
best known for count data, the latter form is quite common. 

Different styles of rootograms  have been proposed and are extensively discussed in
@Kleiber+Zeileis:2016. As default, they propose a so called "hanging"
rootogram, which aligns all deviations along the horizontal axis, as the
rectangles are drawn from $\sqrt{exp_j}$ to $\sqrt{exp_j} - \sqrt{obs_j}$, so
that they "hang" from the curve with the expected frequencies $\sqrt{exp_j}$.

The concept of comparing observed and expected frequencies graphically was also
introduced in the seminal work on assessing calibration and sharpness for a
predictive probalilty model by @Gneiting+Balabdaoui+Raftery:2007 and, building
on this, applied to count data by @Czado+Gneiting+Held:2009.  However, since in
both cases either the deviations or the expected and observed frequencies are
presented only as lines connecting the respective frequencies, deviations are
more difficult to detect compared to the rootograms introduced by @Tukey:1977
and further enhanced by @Kleiber+Zeileis:2016.


### PIT Histogram

As described in the introduction, to check for probabilistic calibration of a
regression model, @Dawid:1984 proposed the use of the probability integral
transform (PIT) which is simply the predictive cumulative distribution function
(CDF) evaluated at the observations. PIT values have been used under various
names [e.g., @Smith:1985;@Dunn+Smyth:1996;@Brockwell:2007] , to emphasize their
similar properties to residuals we follow @Warton:2017 and refer to them as PIT
residuals from now on. For a continuous random variable $y_i (i = 1, \ldots,
n)$, PIT residuals are defined as 

$$u_i = F(y_i | \, \hat{\boldsymbol{\theta}}_i)$$

where $F( \cdot )$ denotes the CDF of the modeled distribution $f( \cdot )$
with estimated parameters $\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1},
\ldots, \hat{\theta}_{iK})^\top$. If the estimated model is a good
approximation to the true data generating process, the observation will be
drawn from the predictive distribution and the PIT residuals $u_i$ are
approximately uniformly distributed on $[0, 1]$. Plotting the histogram of the
PIT residuals and checking for uniformity is therefore a common empirical way
of checking for calibration [@Diebold+Gunther+Tay:1998;
@Gneiting+Balabdaoui+Raftery:2007].  Whereas, deviations from uniformity point
to underlying forecast errors and model deficiencies: U-shaped histograms refer
to underdispersed predictive distributions, inverted U-shaped histograms to
overdispersion, and skewed histograms suggest that central tendencies must be
biased [@Czado+Gneiting+Held:2009].

When considering discrete response distributions or distributions with a
discrete component, e.g., in case of censoring, for a random discrete variable
$y_i$ the PIT $u_i$ can be generated as a random draw from the interval $[F(y_i
- 1 | \, \hat{\boldsymbol{\theta}}_i), F(y_i | \,
  \hat{\boldsymbol{\theta}}_i)]$. Even if this leads to some randomness in the
graphical representation of PIT residuals, for cases with a high number of
observations the impact on the graphical evaluation when repeating the
calculations (i.e. drawing new values $u_i$) is typically rather small. For
small data sets, we recommend to increase the number of random draws which
significantly reduces the randomness in the graphical display.

Alternatively, a nonrandom PIT histogram was introduced by
@Czado+Gneiting+Held:2009, where rather than building on randomized pointwise
PIT resdiuals $u_i$ the expected fraction of the CDF along the interval $[F(y_i
- 1 | \, \hat{\boldsymbol{\theta}}_i), F(y_i | \,
  \hat{\boldsymbol{\theta}}_i)]$ is used. This is asympotically equivalent to
drawing an infinite number of random PIT residuals.

### Q-Q Residuals Plot

Quantile residuals are simply the inverse cumulative distribution function of a
standard normal distribution $\Phi^{-1}$ evaluated at the PIT residuals $u_i (i
= 1, \ldots, n)$, hence, they can be defined as 

$$\hat{r}_i = \Phi^{-1}(F(y_i | \, \hat{\boldsymbol{\theta}}_{i})) = 
  \Phi^{-1}(u_i),$$ 

where $F( \cdot )$ again denotes the cumulative distribution
function (CDF) of the modeled distribution $f( \cdot )$ with
estimated parameters $\hat{\boldsymbol{\theta}}_{i} = (\hat{\theta}_{i1},
\ldots, \hat{\theta}_{iK})^\top$ [@Dunn+Smyth:1996]. As before, for discrete
or partly discrete responses, the approach includes some randomization to
achieve continuous $u_i$ values; quantile residuals are therefore often
referred to as randomized quantile residuals in the literature
[@Dunn+Smyth:1996]. 

In case of a correct model fit, the values $u_i$ are uniformly distributed on
the unit interval and the Q-Q residuals should at least approximately be
standard normally distributed. Hence, to check for normality, quantile
residuals can be graphically compared to theoretical quantiles of the standard
normal distribution, where strong deviations from the bisecting line indicate a
misspecified model fit. 

Mathematically, Q-Q plot consists of the tuples

$$(z_{(1)},  \hat{r}_{(1)}), \ldots,  (z_{(n)},  \hat{r}_{(n)}),$$

where $\hat{r}_{(i)}$ denotes the $i$th order statistic of the quantile
residuals, so that $\hat{r}_{(1)} \leq \hat{r}_{(2)} \cdot \leq \hat{r}_{(n)}$,
and $z_{(i)}$ is the ordered statistics from the respective standard normal
quantiles $\Phi^{-1}( p_i)$, evaluated at the cumulative proportion $p_i = (i -
0.5) / n$ for $n$ greater $10$. This graphical evaluation is well known as normal
probability plot or normal Q-Q plot [@Hoaglin:2006]. Due to the transformation
of the PIT residuals $u_i$ to the normal scale, their extreme values are more
widely spread, so that normal Q-Q diagrams are better suited than, for example,
PIT histograms to detect violations of the distribution assumption within its
tails. An additional possible advantage of Q-Q plots is that they avoid the
necessity of defining breakpoints as typically needed for histogram style
evaluations [@Klein+Kneib+Lang+Sohn:2015].

But Q-Q plots can also be applied to check if residuals follow any other known
distribution, by employing any other inverse cumulative distribution function
of interest instead of $\Phi^{-1}$ in the computation and comparing the
quantile residuals $\hat{r}_i$ to the respective theoretical quantiles. This is
called than a theoretical quantile-quantile plot or Q-Q plot for short
[@Friendly:1991].


### Worm plot

As in Q-Q plots, small too medium deviations can be quite hard to detect,
untilting the plot by subtracting the theoretical quantiles, makes detecting
pattern of departure from a now horizontal line much easier. Mathematically,
therefore, the tuples in the plot are

$$(z_{(1)},  \hat{r}_{(1)} - z_{(1)}), \ldots,  (z_{(n)},  \hat{r}_{(n)} - z_{(n)}),$$

where as before, where $\hat{r}_{(i)}$ denotes the order statistic of the
empirical quantile residuals and $z_{(i)}$ the ordered statistics of the
respective standard normal quantiles.  This so-called de-trended Q-Q plot
[@Friendly:1991] is best known by the application of @Buuren+Fredriks:2001, and
is therefore usually referred to as worm plot according to their naming.

## Summary plot
```{r plot-qq, echo = FALSE, results = "hide"}
grDevices::svg(file = "figures/graph_eval_meth_qq.svg", width = 14, height = 14)
par(mfrow = c(6, 7), bg = "#f7f7f7")
make_residuals_plot(m_norm, breaks = 20, main = "Well calibrated")
make_residuals_plot(m_rskew, breaks = 20, main = "Too skew to the left")
make_residuals_plot(m_lskew, breaks = 20, main = "Too skew to the right")
make_residuals_plot(m_htail, breaks = 20, main = "Tails too light")
make_residuals_plot(m_ltail, breaks = 20, main = "Tails too heavy")
make_residuals_plot(m_odisp, main = "Underdispersed")
make_residuals_plot(m_udisp, main = "Overdispersed")
qqrplot(m_norm, main = "Q-Q residuals plot")
qqrplot(m_rskew, main = "Q-Q resiudals plot")
qqrplot(m_lskew, main = "Q-Q resiudals plot")
qqrplot(m_htail, main = "Q-Q resiudals plot")
qqrplot(m_ltail, main = "Q-Q resiudals plot")
qqrplot(m_odisp, main = "Q-Q resiudals plot")
qqrplot(m_udisp, main = "Q-Q resiudals plot")
#qqrplot(m_norm, trafo = identity, main = "Uniform Q-Q residuals plot")
#qqrplot(m_rskew, trafo = identity, main = "Uniform Q-Q resiudals plot")
#qqrplot(m_lskew, trafo = identity, main = "Uniform Q-Q resiudals plot")
#qqrplot(m_htail, trafo = identity, main = "Uniform Q-Q resiudals plot")
#qqrplot(m_ltail, trafo = identity, main = "Uniform Q-Q resiudals plot")
#qqrplot(m_odisp, trafo = identity, main = "Uniform Q-Q resiudals plot")
#qqrplot(m_udisp, trafo = identity, main = "Uniform Q-Q resiudals plot")
wormplot(m_norm, main = "Worm plot")
wormplot(m_rskew, main = "Worm plot")
wormplot(m_lskew, main = "Worm plot")
wormplot(m_htail, main = "Worm plot")
wormplot(m_ltail, main = "Worm plot")
wormplot(m_odisp, main = "Worm plot")
wormplot(m_udisp, main = "Worm plot")
pithist(m_norm, main = "PIT histogram")
pithist(m_rskew, main = "PIT histogram")
pithist(m_lskew, main = "PIT histogram")
pithist(m_htail, main = "PIT histogram") 
pithist(m_ltail, main = "PIT histogram") 
pithist(m_odisp, main = "PIT histogram")
pithist(m_udisp, main = "PIT histogram")
pithist(m_norm, main = "Customized PIT histogram",
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1)) 
pithist(m_rskew, main = "Customized PIT histogram",
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1)) 
pithist(m_lskew, main = "Customized PIT histogram",
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1)) 
pithist(m_htail, main = "Customized PIT histogram", 
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1)) 
pithist(m_ltail, main = "Customized PIT histogram", 
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1))
pithist(m_odisp, main = "Customized PIT histogram",
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1)) 
pithist(m_udisp, main = "Customized PIT histogram",
  breaks = c(0, 0.005, seq(0,1,length.out = 22)[-c(1,22)], 0.995, 1)) 
rootogram(m_norm, main = "Rootogram")
rootogram(m_rskew, main = "Rootogram")
rootogram(m_lskew, main = "Rootogram")
rootogram(m_htail, main = "Rootogram")
rootogram(m_ltail, main = "Rootogram")
rootogram(m_odisp, main = "Rootogram")
rootogram(m_udisp, main = "Rootogram")
par(mfrow = c(1, 1))
dev.off()
```

![](figures/graph_eval_meth_qq.svg){width=100%}

Predictive distribution | well calibrated | too skew to the left | too skew to the right | tails too light | tails too heavy | underdispersed (underestimated variance) | overdispersed (overestimated variance)
--- | --- | --- | --- | --- | --- | --- | ---
**Resdiuals** | normal | right skewed | left skewed | heavy tailed | light tailed | overdispersed | underdispersed
**Q-Q plot** | bisecting line | positive curvature (bends above the line at both ends) | negative curvature (bends down at both ends) | reverse S-shape (dips below the line at the low end and rises above it at the high end) | S-shape | crossing qqline from below (?) | crossing qqline from above ?
**Worm plot** | horizontal line | U-shape | inverse U-shape | S-shape on the left bent down | S-shape on the left bent up | positive slope | negative slope
**PIT histogram** | uniform | skewed | skewed | superimposed U-shape | superimposed inverse U-shape" | U-shape | inverse U-shape 
**Rootogram** | no deviations | ? | ? | wave-like (underfitting in the tails and the center) | ? | ? | ?
**Interpretation** | no misspecifications | ? | ? |  values more extrem as expected |  values less extrem as expected | ? | ?

## References

