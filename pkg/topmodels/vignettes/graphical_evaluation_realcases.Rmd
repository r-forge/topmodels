---
title: "Graphical Evaluation: Real Cases"
author: "Moritz Lang, Achim Zeileis"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: ref.bib
vignette: >
  %\VignetteIndexEntry{Graphical Evaluation: Simulations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{topmodels,crch,ggplot2,MASS,rmutil,sn}
  %\VignetteKeywords{FIXME}
  %\VignettePackage{topmodels}
---

```{r preliminaries, echo = FALSE, message = FALSE}
library("topmodels")
library("distributions3")
library("crch")
library("ggplot2")
knitr::opts_chunk$set(fig.aling = "center")
```

This vignette provides two real cases discussing the graphical evaluation of
the marginal and probabilistic calibration of probabilistic regression models. The
first example provides a short case study modeling the goals at the 2018 FIFA
World Cup employing a Poisson distribution. It is a direct extension of the
vignette given in [`distributions3`](https://github.com/alexpghayes/distributions3). The second example models daily
precipitation sums using a Gaussian normal distribution censored at zero accounting for non-negative precipitation sums. The use case builds heavily on the vignette given in [`crch`](https://cran.r-project.org/web/packages/crch/) by @Messner+Mayr+Zeileis:2016. 

# Goals in the 2018 FIFA World Cup

This use-case employs the Poisson distribution for modeling count data along
predicted probabilities for the number of goals in soccer matches from the 2018
FIFA World Cup. The full analysis with an illustrative introduction from basic
probability theory to regression models using the R package
[`distributions3`](https://github.com/alexpghayes/distributions3) is given
[here](https://alexpghayes.github.io/distributions3/dev/articles/poisson.html).

To investigate the number of goals scored per match in the 2018 FIFA World Cup,
the `FIFA2018` data set provides two rows, one for each team, for each of the 64 matches
during the tournament. In the following, we treat the goals scored by the two
teams in the same match as independent which is a realistic assumption for this
particular data set. We just remark briefly that there are also bivariate
generalizations of the Poisson distribution that would allow for correlated
observations but which are not considered here.

In addition to the goals, the data set provides some basic meta-information for
the matches (an ID, team name abbreviations, type of match, group vs. knockout stage)
as well as some further covariates that we will revisit later in this document.
The data looks like this:

```{r}
data("FIFA2018", package = "distributions3")
head(FIFA2018)
```

For now, we will focus on the `goals` variable only. A brief summary yields

```{r}
summary(FIFA2018$goals)
```

showing that the teams scored between $0$ and $6$ goals per match with
an average of $\bar y = `r round(mean(FIFA2018$goals), digits = 3)`$
from the observations $y_i$ ($i = 1, \dots, 128$). The corresponding
table of observed absolute and relative frequencies are:

```{r}
table(FIFA2018$goals)
prop.table(table(FIFA2018$goals))
```

(Note that in recent versions of R using `proportions()` rather than `prop.table()`
is recommended.)

This confirms that goals are relatively rare events in a soccer game with
each team scoring zero to two goals per match in almost 90 percent of the
matches. Below we show that this observed frequency distribution can be
approximated very well by a Poisson distribution which can subsequently
be used to obtain predicted probabilities for the goals scored in a match.


## From basic probability theory to regression models

In a first step, we simply assume that goals are scored with a constant mean
over all teams and fit a single Poisson distribution for the number of goals.
To do so, we obtain a point estimate of the Poisson parameter by using the
empirical mean $\hat \lambda = \bar y = `r round(mean(FIFA2018$goals), digits =
3)`$ and set up the corresponding distribution object:

```{r}
p_const <- distributions3::Poisson(lambda = mean(FIFA2018$goals))
p_const
```

This actually corresponds to the maximum likelihood estimator for this
distriubtion fitting a generalized linear model (GLM) to the data that links
the expected number of goals per team/match $\lambda_i$ to the linear predictor
$x_i^\top \beta$ with regressor vector $x_i^\top$ and corresponding coefficient
vector $\beta$ using a log-link: $\log(\lambda_i) = x_i^\top \beta$.

Here, in the simplest case fitting an intercept-only model without further
regressors, the regressor vector can be written as $x_i^\top = 1$ and the
maximum likelihood estimator $\hat \beta$ with corresponding inference,
predictions, residuals, etc. can be obtained using the `glm()` function from
base R with `family = poisson`: 

```{r}
m_ic <- glm(goals ~ 1, data = FIFA2018, family = poisson)
m_ic
```

The corresponding prediction for the number of goals can be obtained manually
from the extracted `coef()` by applying `exp()` (as the inverse of the
log-link).

```{r}
lambda_zero <- exp(coef(m_ic)[1])
lambda_zero
```

Or equivalently the `predict()` function can be used with `type = "response"`
in order to get the expected $\hat \lambda_i$ (rather than just the linear
predictor $x_i^\top \hat \beta$ that is predicted by default).

```{r}
predict(m_ic, newdata = data.frame(difference = 0), type = "response")
```

To account for different expected performances from the teams in the 2018 FIFA World Cup,
the `FIFA2018` data provides an estimated `logability` for each team. These
have been estimated by @Zeileis+Leitner+Hornik:2018 prior to the start of the
tournament (2018-05-20) based on quoted odds from 26 online bookmakers using
the bookmaker consensus model of @Leitner+Zeileis+Hornik:2010. The `difference` in
`logability` between a team and its opponent is a useful predictor for the
number of `goals` scored. 

Hence, the the intercept-only model `m_ic` can be extended by using the
regressor vector $x_i^\top = (1, \mathtt{difference}_i)$:

```{r}
m_prob <- glm(goals ~ difference, data = FIFA2018, family = poisson)
m_prob
```

Now, the slope of $`r round(coef(m_prob)[2], digits = 3)`$ can be interpreted
as an ability elasticity of the number of goals scored. This is because the
difference of the log-abilities can also be understood as the log of the ability
ratio. Thus, when the ability ratio increases by $1$ percent, the expected
number of goals increases approximately by $`r round(coef(m_prob)[2], digits = 3)`$
percent.

## Model evaluation: Marginal calibration

To evaluate the marignal fit, we can compare the 
the observed and expected absolute frequencies using a so-called _rootogram_
[@Kleiber+Zeileis:2016]. The gray bars represent the observed
frequencies overlayed by the expected frequencies as a red line.

```{r}
r1 <- rootogram(m_ic, fitted = TRUE, style = "standing", scale = "raw", ref = FALSE)
```


As before, we can evaluate the marginal fit by comparing the observed and expected frequencies
using a so-called _rootogram_:

```{r}
r2 <- rootogram(m_prob, fitted = TRUE, style = "standing", scale = "raw", ref = FALSE, plot = FALSE)
autoplot(c(r1, r2))
```

Further in a first step, we show the frequencies on a square root scale in order to stabilize
the variances of the discrepancies which puts a larger focus on discrepancies for small frequencies.

```{r}
autoplot(c(r1, r2), scale = "sqrt")  # FIXME: (ML) names got lost
```

In a second step, we employ a so-called _hanging rootogram_ where bars representing the square-root of the observed frequencies are "hanging" from the square-root of the expected frequencies in the red line.
The offset around the x-axis compared to a reference line now clearly shows the difference between the two frequencies for the intercept only model and the full probabilistic model. For both models the differences are reasonably close to zero indicating a rather good marginal fit:

```{r}
autoplot(c(r1, r2), scale = "sqrt", style = "hanging", ref = TRUE)  # FIXME: (ML) names got lost
```

So far we have only evaluated the marginal calibration comparing whether the
oberseved frequencies match the frequencies expected by estimated model on the observaional scale.
In the next step we will evaluate the probabilistic calibration performed on the probability scale.

## Model evaluation: Probabilistic calibration

Probabilistic calibration is usually assessed using probability
integral transform (PIT) values
[@Dawid:1984;@Diebold+Gunther+Tay:1998;@Gneiting+Balabdaoui+Raftery:2007] or
so-called PIT residuals [@Warton:2017]. These are simply the predictive
cumulative distribution function (CDF) evaluated at the observations

$$u_i = F(y_i | \, \hat{\boldsymbol{\theta}}_i),$$

where $F( \cdot )$ denotes the CDF of the modeled distribution $f( \cdot )$
with estimated parameters $\hat{\boldsymbol{\theta}}_{i}$. PIT residuals have
the desirable property, that if the model is a good approximation to the true
data-generating process, i.e., the observation is drawn from the predictive
distribution, the PIT residuals $u_i$ are approximately uniformly distributed
on $[0, 1]$ for continous predictive distributions $F( \cdot )$:

```{r}
p1 <- pithist(m_prob)
```

Alternatively PIT residuals can also be transformed to the normal scale, 
so that their extreme values are more widely spread putting a larger focus on 
violations of the distribution assumption within its tails.

```{r}
p2 <- pithist(m_prob, trafo = qnorm, type = "random", simint = FALSE) # `type = "expected"` does not work, check!
```

PIT residuals transformed to the normal scale can also be validated by well
known Q-Q Plots. By plotting the raw (transformed) PIT residuals without any
discretization, no rather arbitrary breakpoint must be chosen and certain
misspecification cannot be masked.  For example, misscpecifications in the
outer tails of the distribution are often not visible in PIT histograms, as the
intervals are averaving over many data points.

```{r}
q1 <- qqrplot(m_prob, confint = "line", simint = FALSE)
```

As in Q-Q plots, small too medium deviations can be quite hard to detect,
untilting the plot by subtracting the theoretical quantiles, makes detecting
pattern of departure from a now horizontal line much easier:

```{r}
w1 <- wormplot(m_prob, confint = "line", simint = FALSE)
```

# Precipitation forecasts

Censored or truncated response variables occur in a variety of applications.
Censored data arise if exact values are only reported in a restricted range.
Data may fall outside this range but are reported at the range limits. In
contrast, if data outside this range are omitted completely we call the dataset
truncated. E.g., consider wind measurements with an instrument that needs a
certain minimum wind speed to start working. If wind speeds below this minimum
are recorded as ≤ minimum the data are censored. If only wind speeds exceeding
this limit are reported and those below are omitted the data are truncated.
Even if the generating process is not as clear, censoring or truncation can be
useful to consider limited data such as precipitation observations [@Messner+Mayr+Zeileis:2016]. 

The following application shows numerical weather predictions for daily precipitation sums
for Innsbruck (Austria).

```{r crch data}
data("RainIbk", package = "crch")

RainIbk <- sqrt(RainIbk)
RainIbk$ensmean <- apply(RainIbk[,grep('^rainfc',names(RainIbk))], 1, mean)
RainIbk$enssd <- apply(RainIbk[,grep('^rainfc',names(RainIbk))], 1, sd)
RainIbk <- subset(RainIbk, enssd > 0)
```

## From linear regression to distributional regression models

```{r crch models}
## linear model
m_lm <- lm(rain ~ ensmean, data = RainIbk)

## heteroscedastic censored regression with a logistic distribution assumption
m_hclog <- crch(rain ~ ensmean | log(enssd), data = RainIbk, left = 0,
  dist = "logistic")
```

```{r}
plot(rain ~ ensmean, data = RainIbk, pch = 19, col = gray(0, alpha = 0.2))
abline(coef(m_lm)[1:2], col = 1, lwd = 2)
abline(coef(m_hclog)[1:2], col = 4, lwd = 2)

legend("topright", lwd = c(1, 1), lty = c(1, 1), col = c(1, 4), 
  c("m_lm", "m_hclog"), bty = "n")
```

## Model evaluation: Marginal calibration

```{r}
r1 <- rootogram(m_lm, plot = FALSE)
r2 <- rootogram(m_hclog, plot = FALSE)

autoplot(c(r1, r2))

```

## Model evaluation: Probabilistic calibration

```{r}
p1 <- pithist(m_lm, plot = FALSE)
p2 <- pithist(m_hclog, plot = FALSE)

autoplot(c(p1, p2))
autoplot(c(p1, p2), colour = c(1, 4), style = "line", single_graph = TRUE, confint_col = 1, confint = "line", ref = FALSE)
```

```{r}
q1 <- qqrplot(m_lm, plot = FALSE)
q2 <- qqrplot(m_hclog, plot = FALSE)
autoplot(c(q1, q2), colour = c(1, 4), alpha = 0.8, single_graph = TRUE, simint = FALSE, confint ="line")
```

## References


